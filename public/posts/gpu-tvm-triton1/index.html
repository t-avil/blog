<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NOTES: Network Virtualization | T|A</title><meta name=keywords content="notes,networking,cloud,virtualization,andromeda,snap"><meta name=description content="A backend engineer’s deep dive into how modern cloud networks work: from Koponen et al.’s classic SDN paper to Google’s Andromeda and SNAP systems."><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.3f7ba6a00d316a1658af1e52b60f5592bfd3f63e1683217d447958625c9fec2a.css integrity="sha256-P3umoA0xahZYrx5Stg9Vkr/T9j4WgyF9RHlYYlyf7Co=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/gpu-tvm-triton1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/gpu-tvm-triton1/"><meta property="og:site_name" content="T|A"><meta property="og:title" content="NOTES: Network Virtualization"><meta property="og:description" content="A backend engineer’s deep dive into how modern cloud networks work: from Koponen et al.’s classic SDN paper to Google’s Andromeda and SNAP systems."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-10T14:00:00-07:00"><meta property="article:modified_time" content="2025-10-10T14:00:00-07:00"><meta property="article:tag" content="Notes"><meta property="article:tag" content="Networking"><meta property="article:tag" content="Cloud"><meta property="article:tag" content="Virtualization"><meta property="article:tag" content="Andromeda"><meta property="article:tag" content="Snap"><meta property="og:image" content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:title content="NOTES: Network Virtualization"><meta name=twitter:description content="A backend engineer’s deep dive into how modern cloud networks work: from Koponen et al.’s classic SDN paper to Google’s Andromeda and SNAP systems."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"NOTES: Network Virtualization","item":"http://localhost:1313/posts/gpu-tvm-triton1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NOTES: Network Virtualization","name":"NOTES: Network Virtualization","description":"A backend engineer’s deep dive into how modern cloud networks work: from Koponen et al.’s classic SDN paper to Google’s Andromeda and SNAP systems.","keywords":["notes","networking","cloud","virtualization","andromeda","snap"],"articleBody":"If GPUs were about squeezing performance from silicon, cloud networking is about squeezing determinism from chaos - hundreds of thousands of tenants, each expecting a private, secure, and high-performance network that doesn’t even “feel” shared.\nThis post takes you from the ground up: we’ll start with what virtualization means in networking, explain overlay networks, fabric topologies, and control planes, and then we’ll walk through three real systems - (1) Koponen et al.’s Network Virtualization in Multi-Tenant Datacenters (NSDI 2014), (2) Google’s Andromeda, and (3) its successor, SNAP (Scalable Network Architecture Platform).\nIt’s the same story as the evolution of GPUs → TVM → Triton, just in the network world.\n1. Starting From Zero: What Is Network Virtualization? Let’s start with what problem we’re solving.\nIn a datacenter, thousands of virtual machines (VMs) from different tenants share the same physical network. Each tenant expects their own private network, with their own IPs, firewalls, and routers - even though physically, all of them are plugged into the same switches.\nThis illusion is achieved using network virtualization: abstracting one physical network into many logical ones. It’s like giving every tenant a fake Ethernet cable that “feels” dedicated but is actually software-defined.\nOverlay and Underlay The real, physical network (switches, routers, cables) is called the underlay. The virtual network that the VMs see - isolated, programmable, and logically routed - is called the overlay.\nTo connect the two, each packet from a VM is encapsulated: wrapped in another packet with special headers, then tunneled across the underlay. When it reaches the destination host, the outer header is stripped, and the original packet is delivered to the target VM.\nThis wrapping process uses protocols like VXLAN or GRE. Conceptually, it’s the same idea as a shipping box - you can ship arbitrary data safely, even if the delivery network is shared.\n2. The Fabric and the Planes In large datacenters, the physical topology looks like a Clos fabric - layers of leaf and spine switches that provide uniform bandwidth between any two racks.\nOver this hardware, networking is divided into three conceptual layers:\nData plane: the layer that moves packets (e.g., a vSwitch on a host). Control plane: decides how packets should be routed and installs those rules into the data plane. Management plane: orchestrates everything globally (for example, when a new tenant is created). This separation is the foundation of Software-Defined Networking (SDN). The control plane is centralized and “smart”; the data plane is distributed and “dumb but fast.”\n3. Koponen et al. (NSDI 2014): The OG SDN Design In 2014, Teemu Koponen and colleagues proposed one of the first scalable SDN architectures for multi-tenant datacenters. Their system was based on Open vSwitch (OVS) and tunneling.\nEach hypervisor host ran a vSwitch, a small virtual Ethernet switch that handled all packet forwarding for the VMs on that host. The vSwitch stored forwarding rules programmed by a central controller - a logically centralized service that understood all tenants’ topologies.\nHere’s how it worked:\nA VM sends a packet. The vSwitch checks its flow table (like a hash map of “if packet matches → do this”). If the flow rule is missing, the packet is sent to the controller. The controller computes what to do and pushes a new rule back to the vSwitch. Future packets of that flow now follow the cached rule. The control plane (the central brain) knows everything: which VMs exist, which tenants they belong to, and how they should be connected. The data plane (vSwitches) simply enforces those decisions.\nThis architecture solved isolation and mobility (VMs could move between servers without changing IPs). But it had a cost: every packet passed through multiple software layers, context switches, and kernel calls, limiting throughput to a few hundred thousand packets per second per host.\n4. Andromeda: Scaling the Dream (Google NSDI 2018) Fast-forward to Google Cloud Platform (GCP). By 2018, the scale problem had exploded. The same ideas as Koponen’s system still applied, but now the bar was higher:\nMillions of VMs. Microsecond-level latency budgets. Zero downtime feature rollout. Strong performance isolation between tenants. The Core Idea: Hierarchical Flow Processing Andromeda’s big innovation is the flow hierarchy - different processing “paths” depending on the flow’s requirements:\nFast Path: pure userspace, OS-bypass packet processing for high-throughput, low-latency traffic (like VM-to-VM in the same cluster). Coprocessor Path (Slow Path): CPU-heavy tasks like encryption, NAT, or firewalling. Hoverboard Path: a “catch-all” for tiny, short-lived flows, using shared gateways. Instead of processing every packet with full software stack overhead, Andromeda sends 99% of traffic down the fast path, which is basically a tight loop in userspace reading and writing directly to NIC queues via shared memory.\nOS Bypass and Shared Memory Queues Normally, packet I/O goes through the OS kernel: system calls, interrupts, scheduler wakeups - slow. Andromeda uses busy-polling and shared memory rings to directly read packets from VM NIC queues and write them to host NIC queues. It’s like cutting the operating system out of the hot path entirely.\nThis userspace networking stack evolved across versions:\nAndromeda 1.0: used modified OVS in kernel. Andromeda 2.0: moved to full userspace dataplane. Andromeda 2.1: bypassed the VMM entirely. Andromeda 2.2: added DMA offload using Intel QuickData Engines. DMA (Direct Memory Access) means copying packets directly from VM memory to NIC buffers without using the CPU, freeing cycles for computation.\nHoverboard: Scaling Control Plane In the original SDN model, the control plane installed one flow rule per connection. That breaks at Google scale. Hoverboard fixes this by sending small, short-lived flows through shared gateways that already know how to forward traffic, so no per-flow setup is needed.\nThis design drastically reduces control-plane churn and lets GCP spin up thousands of VMs in seconds.\nIsolation and Hardware Offload Because GCP is multi-tenant, Andromeda enforces performance isolation. Fast and slow paths are assigned different cores or NIC queues, so one tenant’s heavy traffic doesn’t interfere with another’s. Hardware offloads (encryption, checksums, segmentation) ensure that CPU time isn’t wasted on repetitive work.\nReference: Dalton et al., Andromeda: Performance, Isolation, and Velocity at Scale in Cloud Network Virtualization, USENIX NSDI 2018.\n5. SNAP: The Next Generation (Google, 2021) By 2021, Google had evolved Andromeda into SNAP (Scalable Network Architecture Platform) - a re-architecture for even larger scale and faster innovation.\nKey Difference: Decoupling Features From Fast Path In Andromeda, feature growth (like new firewall rules or telemetry) still touched the dataplane code. SNAP separates these concerns entirely:\nFast Path: minimal, stable, performance-critical core. Service Modules: plug-in features (load balancing, encryption, flow export) that can be updated independently. Remote Packet I/O (RPIO): moves NIC access to dedicated “network service hosts,” letting compute hosts focus purely on user VMs. This modularization allows GCP to roll out new network features without risking downtime or performance regressions in the dataplane.\nInfrastructure Design Each SNAP “host” runs multiple network service threads, each pinned to CPU cores, directly communicating with hardware queues on smart NICs. Packet metadata is cached in shared memory regions, and inter-core communication is done via lock-free ring buffers.\nFeature modules attach through a service graph - a chain of packet-processing functions like a microservice pipeline but running in-process. Each module declares its compute and latency requirements so the scheduler can pin it optimally.\nThis design makes SNAP both faster and safer to evolve - it’s a clean separation between network plumbing and value-added features.\nReference: Hock et al., SNAP: A Microkernel Approach to Cloud Network Virtualization, USENIX NSDI 2021.\n6. Applied Summary: From Research to Practice If you’re a backend engineer, here’s the intuitive way to think about the whole progression:\nLayer Koponen et al. (2014) Andromeda (2018) SNAP (2021) Concept SDN for VMs Datacenter-scale virtualization Modular network microkernel Data Plane vSwitch (kernel) Userspace OS-bypass Multi-core service graph Control Plane Centralized controller Hierarchical, distributed Modular API-based Performance Flexible, slow Hardware-offloaded, fast Hardware + modular isolation Scale Thousands of hosts Hundreds of thousands Millions of endpoints Feature Updates Global rollout Safe partial deployment Hot-swappable modules Think of Koponen as the “Linux kernel” moment for networking - the foundation. Andromeda is like DPDK meets Kubernetes - highly optimized, software-defined, yet dynamic. SNAP is the “microkernel” rearchitecture - breaking up the monolith to scale feature velocity.\n7. Final Thoughts Network virtualization isn’t just about packets - it’s about turning the network into an API.\nKoponen gave us the first blueprint. Andromeda made it real at hyperscale. SNAP turned it into a living, evolving platform.\nTogether, they form the story of how Google - and the industry at large - turned the network into software, just like TVM and Triton turned hardware acceleration into compilers.\nFurther Reading:\nKoponen et al., Network Virtualization in Multi-Tenant Datacenters, NSDI 2014 - PDF from USENIX: https://www.usenix.org/system/files/conference/nsdi14/nsdi14-paper-koponen.pdf Dalton et al., Andromeda: Performance, Isolation, and Velocity at Scale, NSDI 2018 - PDF from USENIX: https://www.usenix.org/system/files/conference/nsdi18/nsdi18-dalton.pdf SNAP: Snap: a Microkernel Approach to Host Networking, SOSP / Google - Official web page (includes PDF) at Google Research: https://research.google/pubs/snap-a-microkernel-approach-to-host-networking/ ","wordCount":"1492","inLanguage":"en","image":"http://localhost:1313/images/papermod-cover.png","datePublished":"2025-10-10T14:00:00-07:00","dateModified":"2025-10-10T14:00:00-07:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/gpu-tvm-triton1/"},"publisher":{"@type":"Organization","name":"T|A","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="T|A (Alt + H)">T|A</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about title=About><span>About</span></a></li><li><a href=http://localhost:1313/chronicles title=Chronicles><span>Chronicles</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://github.com/t-avil title=Github><span>Github</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">NOTES: Network Virtualization</h1><div class=post-description>A backend engineer’s deep dive into how modern cloud networks work: from Koponen et al.’s classic SDN paper to Google’s Andromeda and SNAP systems.</div><div class=post-meta><span title='2025-10-10 14:00:00 -0700 PDT'>October 10, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1492 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/t-avil/blog/tree/main/content/posts/gpu-tvm-triton1.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-starting-from-zero-what-is-network-virtualization>1. Starting From Zero: What Is Network Virtualization?</a><ul><li><a href=#overlay-and-underlay>Overlay and Underlay</a></li></ul></li><li><a href=#2-the-fabric-and-the-planes>2. The Fabric and the Planes</a></li><li><a href=#3-koponen-et-al-nsdi-2014-the-og-sdn-design>3. Koponen et al. (NSDI 2014): The OG SDN Design</a></li><li><a href=#4-andromeda-scaling-the-dream-google-nsdi-2018>4. Andromeda: Scaling the Dream (Google NSDI 2018)</a><ul><li><a href=#the-core-idea-hierarchical-flow-processing>The Core Idea: Hierarchical Flow Processing</a></li><li><a href=#os-bypass-and-shared-memory-queues>OS Bypass and Shared Memory Queues</a></li><li><a href=#hoverboard-scaling-control-plane>Hoverboard: Scaling Control Plane</a></li><li><a href=#isolation-and-hardware-offload>Isolation and Hardware Offload</a></li></ul></li><li><a href=#5-snap-the-next-generation-google-2021>5. SNAP: The Next Generation (Google, 2021)</a><ul><li><a href=#key-difference-decoupling-features-from-fast-path>Key Difference: Decoupling Features From Fast Path</a></li><li><a href=#infrastructure-design>Infrastructure Design</a></li></ul></li><li><a href=#6-applied-summary-from-research-to-practice>6. Applied Summary: From Research to Practice</a></li><li><a href=#7-final-thoughts>7. Final Thoughts</a></li></ul></nav></div></details></div><div class=post-content><p>If GPUs were about squeezing performance from silicon, cloud networking is about <strong>squeezing determinism from chaos</strong> - hundreds of thousands of tenants, each expecting a private, secure, and high-performance network that doesn’t even “feel” shared.</p><p>This post takes you from the ground up: we’ll start with what <em>virtualization</em> means in networking, explain <strong>overlay networks</strong>, <strong>fabric topologies</strong>, and <strong>control planes</strong>, and then we’ll walk through three real systems -
<strong>(1)</strong> Koponen et al.’s <em>Network Virtualization in Multi-Tenant Datacenters</em> (NSDI 2014),
<strong>(2)</strong> Google’s <em>Andromeda</em>, and
<strong>(3)</strong> its successor, <em>SNAP</em> (Scalable Network Architecture Platform).</p><p>It’s the same story as the evolution of GPUs → TVM → Triton, just in the network world.</p><hr><h2 id=1-starting-from-zero-what-is-network-virtualization>1. Starting From Zero: What Is Network Virtualization?<a hidden class=anchor aria-hidden=true href=#1-starting-from-zero-what-is-network-virtualization>#</a></h2><p>Let’s start with what problem we’re solving.</p><p>In a datacenter, thousands of virtual machines (VMs) from different tenants share the same physical network. Each tenant expects their own <strong>private network</strong>, with their own IPs, firewalls, and routers - even though physically, all of them are plugged into the same switches.</p><p>This illusion is achieved using <strong>network virtualization</strong>: abstracting one physical network into many <em>logical</em> ones. It’s like giving every tenant a fake Ethernet cable that “feels” dedicated but is actually software-defined.</p><h3 id=overlay-and-underlay>Overlay and Underlay<a hidden class=anchor aria-hidden=true href=#overlay-and-underlay>#</a></h3><p>The real, physical network (switches, routers, cables) is called the <strong>underlay</strong>.
The virtual network that the VMs see - isolated, programmable, and logically routed - is called the <strong>overlay</strong>.</p><p>To connect the two, each packet from a VM is <strong>encapsulated</strong>: wrapped in another packet with special headers, then tunneled across the underlay. When it reaches the destination host, the outer header is stripped, and the original packet is delivered to the target VM.</p><p>This wrapping process uses protocols like <strong>VXLAN</strong> or <strong>GRE</strong>. Conceptually, it’s the same idea as a shipping box - you can ship arbitrary data safely, even if the delivery network is shared.</p><hr><h2 id=2-the-fabric-and-the-planes>2. The Fabric and the Planes<a hidden class=anchor aria-hidden=true href=#2-the-fabric-and-the-planes>#</a></h2><p>In large datacenters, the physical topology looks like a <strong>Clos fabric</strong> - layers of leaf and spine switches that provide uniform bandwidth between any two racks.</p><p>Over this hardware, networking is divided into three conceptual layers:</p><ul><li><strong>Data plane:</strong> the layer that moves packets (e.g., a vSwitch on a host).</li><li><strong>Control plane:</strong> decides how packets should be routed and installs those rules into the data plane.</li><li><strong>Management plane:</strong> orchestrates everything globally (for example, when a new tenant is created).</li></ul><p>This separation is the foundation of <strong>Software-Defined Networking (SDN)</strong>. The control plane is centralized and “smart”; the data plane is distributed and “dumb but fast.”</p><hr><h2 id=3-koponen-et-al-nsdi-2014-the-og-sdn-design>3. Koponen et al. (NSDI 2014): The OG SDN Design<a hidden class=anchor aria-hidden=true href=#3-koponen-et-al-nsdi-2014-the-og-sdn-design>#</a></h2><p>In 2014, Teemu Koponen and colleagues proposed one of the first scalable SDN architectures for <strong>multi-tenant datacenters</strong>. Their system was based on <strong>Open vSwitch (OVS)</strong> and <strong>tunneling</strong>.</p><p>Each hypervisor host ran a <strong>vSwitch</strong>, a small virtual Ethernet switch that handled all packet forwarding for the VMs on that host. The vSwitch stored forwarding rules programmed by a <strong>central controller</strong> - a logically centralized service that understood all tenants’ topologies.</p><p>Here’s how it worked:</p><ol><li>A VM sends a packet.</li><li>The vSwitch checks its flow table (like a hash map of “if packet matches → do this”).</li><li>If the flow rule is missing, the packet is sent to the controller.</li><li>The controller computes what to do and pushes a new rule back to the vSwitch.</li><li>Future packets of that flow now follow the cached rule.</li></ol><p>The control plane (the central brain) knows everything: which VMs exist, which tenants they belong to, and how they should be connected. The data plane (vSwitches) simply enforces those decisions.</p><p>This architecture solved <em>isolation</em> and <em>mobility</em> (VMs could move between servers without changing IPs). But it had a cost: every packet passed through multiple software layers, context switches, and kernel calls, limiting throughput to a few hundred thousand packets per second per host.</p><hr><h2 id=4-andromeda-scaling-the-dream-google-nsdi-2018>4. Andromeda: Scaling the Dream (Google NSDI 2018)<a hidden class=anchor aria-hidden=true href=#4-andromeda-scaling-the-dream-google-nsdi-2018>#</a></h2><p>Fast-forward to Google Cloud Platform (GCP). By 2018, the scale problem had exploded. The same ideas as Koponen’s system still applied, but now the bar was higher:</p><ul><li>Millions of VMs.</li><li>Microsecond-level latency budgets.</li><li>Zero downtime feature rollout.</li><li>Strong performance isolation between tenants.</li></ul><h3 id=the-core-idea-hierarchical-flow-processing>The Core Idea: Hierarchical Flow Processing<a hidden class=anchor aria-hidden=true href=#the-core-idea-hierarchical-flow-processing>#</a></h3><p>Andromeda’s big innovation is the <strong>flow hierarchy</strong> - different processing “paths” depending on the flow’s requirements:</p><ul><li><strong>Fast Path:</strong> pure userspace, OS-bypass packet processing for high-throughput, low-latency traffic (like VM-to-VM in the same cluster).</li><li><strong>Coprocessor Path (Slow Path):</strong> CPU-heavy tasks like encryption, NAT, or firewalling.</li><li><strong>Hoverboard Path:</strong> a “catch-all” for tiny, short-lived flows, using shared gateways.</li></ul><p>Instead of processing every packet with full software stack overhead, Andromeda sends 99% of traffic down the fast path, which is basically a tight loop in userspace reading and writing directly to NIC queues via <strong>shared memory</strong>.</p><h3 id=os-bypass-and-shared-memory-queues>OS Bypass and Shared Memory Queues<a hidden class=anchor aria-hidden=true href=#os-bypass-and-shared-memory-queues>#</a></h3><p>Normally, packet I/O goes through the OS kernel: system calls, interrupts, scheduler wakeups - slow.
Andromeda uses <strong>busy-polling</strong> and <strong>shared memory rings</strong> to directly read packets from VM NIC queues and write them to host NIC queues. It’s like cutting the operating system out of the hot path entirely.</p><p>This userspace networking stack evolved across versions:</p><ul><li><em>Andromeda 1.0:</em> used modified OVS in kernel.</li><li><em>Andromeda 2.0:</em> moved to full userspace dataplane.</li><li><em>Andromeda 2.1:</em> bypassed the VMM entirely.</li><li><em>Andromeda 2.2:</em> added <strong>DMA offload</strong> using Intel QuickData Engines.</li></ul><p>DMA (Direct Memory Access) means copying packets directly from VM memory to NIC buffers without using the CPU, freeing cycles for computation.</p><h3 id=hoverboard-scaling-control-plane>Hoverboard: Scaling Control Plane<a hidden class=anchor aria-hidden=true href=#hoverboard-scaling-control-plane>#</a></h3><p>In the original SDN model, the control plane installed one flow rule per connection. That breaks at Google scale. Hoverboard fixes this by sending small, short-lived flows through shared <strong>gateways</strong> that already know how to forward traffic, so no per-flow setup is needed.</p><p>This design drastically reduces control-plane churn and lets GCP spin up thousands of VMs in seconds.</p><h3 id=isolation-and-hardware-offload>Isolation and Hardware Offload<a hidden class=anchor aria-hidden=true href=#isolation-and-hardware-offload>#</a></h3><p>Because GCP is multi-tenant, Andromeda enforces <strong>performance isolation</strong>. Fast and slow paths are assigned different cores or NIC queues, so one tenant’s heavy traffic doesn’t interfere with another’s. Hardware offloads (encryption, checksums, segmentation) ensure that CPU time isn’t wasted on repetitive work.</p><p><strong>Reference:</strong>
Dalton et al., <em>Andromeda: Performance, Isolation, and Velocity at Scale in Cloud Network Virtualization</em>, USENIX NSDI 2018.</p><hr><h2 id=5-snap-the-next-generation-google-2021>5. SNAP: The Next Generation (Google, 2021)<a hidden class=anchor aria-hidden=true href=#5-snap-the-next-generation-google-2021>#</a></h2><p>By 2021, Google had evolved Andromeda into <strong>SNAP (Scalable Network Architecture Platform)</strong> - a re-architecture for even larger scale and faster innovation.</p><h3 id=key-difference-decoupling-features-from-fast-path>Key Difference: Decoupling Features From Fast Path<a hidden class=anchor aria-hidden=true href=#key-difference-decoupling-features-from-fast-path>#</a></h3><p>In Andromeda, feature growth (like new firewall rules or telemetry) still touched the dataplane code. SNAP separates these concerns entirely:</p><ul><li><strong>Fast Path:</strong> minimal, stable, performance-critical core.</li><li><strong>Service Modules:</strong> plug-in features (load balancing, encryption, flow export) that can be updated independently.</li><li><strong>Remote Packet I/O (RPIO):</strong> moves NIC access to dedicated “network service hosts,” letting compute hosts focus purely on user VMs.</li></ul><p>This modularization allows GCP to roll out new network features without risking downtime or performance regressions in the dataplane.</p><h3 id=infrastructure-design>Infrastructure Design<a hidden class=anchor aria-hidden=true href=#infrastructure-design>#</a></h3><p>Each SNAP “host” runs multiple <strong>network service threads</strong>, each pinned to CPU cores, directly communicating with <strong>hardware queues</strong> on smart NICs.
Packet metadata is cached in <strong>shared memory regions</strong>, and inter-core communication is done via <strong>lock-free ring buffers</strong>.</p><p>Feature modules attach through a <strong>service graph</strong> - a chain of packet-processing functions like a microservice pipeline but running in-process. Each module declares its compute and latency requirements so the scheduler can pin it optimally.</p><p>This design makes SNAP both <strong>faster</strong> and <strong>safer to evolve</strong> - it’s a clean separation between network plumbing and value-added features.</p><p><strong>Reference:</strong>
Hock et al., <em>SNAP: A Microkernel Approach to Cloud Network Virtualization</em>, USENIX NSDI 2021.</p><hr><h2 id=6-applied-summary-from-research-to-practice>6. Applied Summary: From Research to Practice<a hidden class=anchor aria-hidden=true href=#6-applied-summary-from-research-to-practice>#</a></h2><p>If you’re a backend engineer, here’s the intuitive way to think about the whole progression:</p><table><thead><tr><th>Layer</th><th>Koponen et al. (2014)</th><th>Andromeda (2018)</th><th>SNAP (2021)</th></tr></thead><tbody><tr><td>Concept</td><td>SDN for VMs</td><td>Datacenter-scale virtualization</td><td>Modular network microkernel</td></tr><tr><td>Data Plane</td><td>vSwitch (kernel)</td><td>Userspace OS-bypass</td><td>Multi-core service graph</td></tr><tr><td>Control Plane</td><td>Centralized controller</td><td>Hierarchical, distributed</td><td>Modular API-based</td></tr><tr><td>Performance</td><td>Flexible, slow</td><td>Hardware-offloaded, fast</td><td>Hardware + modular isolation</td></tr><tr><td>Scale</td><td>Thousands of hosts</td><td>Hundreds of thousands</td><td>Millions of endpoints</td></tr><tr><td>Feature Updates</td><td>Global rollout</td><td>Safe partial deployment</td><td>Hot-swappable modules</td></tr></tbody></table><p>Think of <strong>Koponen</strong> as the “Linux kernel” moment for networking - the foundation.
<strong>Andromeda</strong> is like DPDK meets Kubernetes - highly optimized, software-defined, yet dynamic.
<strong>SNAP</strong> is the “microkernel” rearchitecture - breaking up the monolith to scale feature velocity.</p><hr><h2 id=7-final-thoughts>7. Final Thoughts<a hidden class=anchor aria-hidden=true href=#7-final-thoughts>#</a></h2><p>Network virtualization isn’t just about packets - it’s about <strong>turning the network into an API</strong>.</p><p>Koponen gave us the first blueprint.
Andromeda made it real at hyperscale.
SNAP turned it into a living, evolving platform.</p><p>Together, they form the story of how Google - and the industry at large - turned the network into software, just like TVM and Triton turned hardware acceleration into compilers.</p><p><strong>Further Reading:</strong></p><ul><li>Koponen et al., <em>Network Virtualization in Multi-Tenant Datacenters</em>, NSDI 2014 - PDF from USENIX: <a href=https://www.usenix.org/system/files/conference/nsdi14/nsdi14-paper-koponen.pdf>https://www.usenix.org/system/files/conference/nsdi14/nsdi14-paper-koponen.pdf</a></li><li>Dalton et al., <em>Andromeda: Performance, Isolation, and Velocity at Scale</em>, NSDI 2018 - PDF from USENIX: <a href=https://www.usenix.org/system/files/conference/nsdi18/nsdi18-dalton.pdf>https://www.usenix.org/system/files/conference/nsdi18/nsdi18-dalton.pdf</a></li><li>SNAP: <em>Snap: a Microkernel Approach to Host Networking</em>, SOSP / Google - Official web page (includes PDF) at Google Research: <a href=https://research.google/pubs/snap-a-microkernel-approach-to-host-networking/>https://research.google/pubs/snap-a-microkernel-approach-to-host-networking/</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/notes/>Notes</a></li><li><a href=http://localhost:1313/tags/networking/>Networking</a></li><li><a href=http://localhost:1313/tags/cloud/>Cloud</a></li><li><a href=http://localhost:1313/tags/virtualization/>Virtualization</a></li><li><a href=http://localhost:1313/tags/andromeda/>Andromeda</a></li><li><a href=http://localhost:1313/tags/snap/>Snap</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/gpu-tvm-triton/><span class=title>Next »</span><br><span>NOTES: GPU Architecture, TVM, and Triton</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>T|A</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>