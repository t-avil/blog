<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NOTES: Searching the unsearchable | T|A</title><meta name=keywords content="notes,elasticsearch,search,system-design,distributed-systems,vectors,fuzzy,ai"><meta name=description content="Levenshtein automata, inverted indexes, BM25, segment merges, ANN vectors, CDC pipelines, and why Elasticsearch is basically three PhDs stapled together with JSON."><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.3f7ba6a00d316a1658af1e52b60f5592bfd3f63e1683217d447958625c9fec2a.css integrity="sha256-P3umoA0xahZYrx5Stg9Vkr/T9j4WgyF9RHlYYlyf7Co=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/searching-the-unsearchable/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/searching-the-unsearchable/"><meta property="og:site_name" content="T|A"><meta property="og:title" content="NOTES: Searching the unsearchable"><meta property="og:description" content="Levenshtein automata, inverted indexes, BM25, segment merges, ANN vectors, CDC pipelines, and why Elasticsearch is basically three PhDs stapled together with JSON."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-01T04:52:54-07:00"><meta property="article:modified_time" content="2025-08-01T04:52:54-07:00"><meta property="article:tag" content="Notes"><meta property="article:tag" content="Elasticsearch"><meta property="article:tag" content="Search"><meta property="article:tag" content="System-Design"><meta property="article:tag" content="Distributed-Systems"><meta property="article:tag" content="Vectors"><meta property="og:image" content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:title content="NOTES: Searching the unsearchable"><meta name=twitter:description content="Levenshtein automata, inverted indexes, BM25, segment merges, ANN vectors, CDC pipelines, and why Elasticsearch is basically three PhDs stapled together with JSON."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"NOTES: Searching the unsearchable","item":"http://localhost:1313/posts/searching-the-unsearchable/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NOTES: Searching the unsearchable","name":"NOTES: Searching the unsearchable","description":"Levenshtein automata, inverted indexes, BM25, segment merges, ANN vectors, CDC pipelines, and why Elasticsearch is basically three PhDs stapled together with JSON.","keywords":["notes","elasticsearch","search","system-design","distributed-systems","vectors","fuzzy","ai"],"articleBody":" Fuzzy Search is More Than Just Levenshtein Distance Let’s start from the ground level. If you mistype “pizzq” but still want “pizza”, how does a search engine know? At the simplest level you could calculate Levenshtein distance between two strings: the minimum number of insertions, deletions, or substitutions required to turn one string into another.\nHere’s a simple dynamic programming implementation in TypeScript:\nfunction levenshtein(a: string, b: string): number { const dp: number[][] = Array.from({ length: a.length + 1 }, () =\u003e Array(b.length + 1).fill(0) ) for (let i = 0; i \u003c= a.length; i++) dp[i][0] = i for (let j = 0; j \u003c= b.length; j++) dp[0][j] = j for (let i = 1; i \u003c= a.length; i++) { for (let j = 1; j \u003c= b.length; j++) { if (a[i - 1] === b[j - 1]) { dp[i][j] = dp[i - 1][j - 1] } else { dp[i][j] = Math.min( dp[i - 1][j] + 1, // deletion dp[i][j - 1] + 1, // insertion dp[i - 1][j - 1] + 1 // substitution ) } } } return dp[a.length][b.length] } console.log(levenshtein(\"pizza\", \"pizzq\")) // 1 This works, but if you have millions of documents it’s way too slow to compare every string pairwise.\nEnter Levenshtein Automata Instead of comparing against every document, you build a deterministic finite automaton (DFA) that represents all words within edit distance k of your query. Then you run the DFA against the index.\nThe states of the automaton represent “how many edits we have left” and “where in the word we are”. A simplified DFA construction for edit distance 1 might look like:\ntype State = { index: number; edits: number } function nextStates(state: State, char: string): State[] { const { index, edits } = state const states: State[] = [] // Match without edit states.push({ index: index + 1, edits }) if (edits \u003e 0) { // Substitution states.push({ index: index + 1, edits: edits - 1 }) // Insertion states.push({ index, edits: edits - 1 }) // Deletion states.push({ index + 1, edits: edits - 1 }) } return states } In production Lucene builds these automata efficiently using special algorithms, but the concept is the same: instead of checking one string at a time, you check all possible close strings in parallel.\nInverted Indexes and Compression Tricks Search engines don’t store documents like databases do. They flip the structure into an inverted index:\npizza -\u003e [doc1, doc3, doc7] pizzeria -\u003e [doc2, doc5] pepperoni -\u003e [doc1, doc4, doc7] Now queries become intersections of lists. To make this efficient, the lists are compressed.\nExample: storing document IDs [100, 101, 103, 110]. Instead of storing raw numbers, store gaps:\n[100, 1, 2, 7] Then apply variable-length encoding:\nfunction encodeVarint(nums: number[]): number[] { const bytes: number[] = [] for (const n of nums) { let value = n while (value \u003e= 128) { bytes.push((value \u0026 0x7f) | 0x80) value \u003e\u003e= 7 } bytes.push(value) } return bytes } console.log(encodeVarint([100, 1, 2, 7])) // -\u003e [100, 1, 2, 7] but each as compressed bytes This allows Elasticsearch to store millions of postings in memory and still intersect them quickly.\nScoring: TF-IDF vs BM25 Both TF-IDF and BM25 are ways to rank documents based on how relevant they are to the query.\nTF-IDF:\nTerm Frequency (TF): how many times the word appears in the document. Inverse Document Frequency (IDF): how rare the word is across the collection. Score = TF * IDF. The problem: TF grows linearly. A word appearing 50 times is not 50x more relevant.\nBM25: BM25 adds two important ideas:\nSaturation: additional term frequency gives diminishing returns. Normalization: longer documents should not always get higher scores just because they have more words. The BM25 formula looks like this:\nscore = IDF * ( (tf * (k+1)) / (tf + k * (1 - b + b * (docLen / avgDocLen))) ) Where k controls saturation and b controls length normalization.\nQuery Execution Strategies When combining postings lists, there are two strategies:\nTerm-at-a-time (TAAT): Process one query term’s postings at a time, updating partial scores. Document-at-a-time (DAAT): Walk all postings lists in sync, scoring one document fully before moving on. Example: Query = “pizza OR pasta”.\nTAAT:\nScan postings for “pizza” → update scores. Scan postings for “pasta” → update scores. DAAT:\nLook at doc1 → check if it has pizza or pasta → assign final score. Move to doc2 → repeat. DAAT is usually better for top-k queries because you can use a heap and stop early.\nVector Search and HNSW Graphs Elasticsearch supports vector search with HNSW graphs (Hierarchical Navigable Small Worlds).\nImagine each document as a point in high-dimensional space. Searching means finding nearest neighbors. Brute-force is O(n), too slow.\nHNSW builds a multi-layer graph:\nUpper layers have long links (skip connections). Lower layers have dense local connections. Search walks top-down: start at the top layer, use long edges to get close, then drop down layers until you reach the closest nodes. Complexity ~log N.\nIn Elasticsearch you encode text into vectors using a model, then store them as dense_vector. At query time, you provide a query vector and ES traverses the HNSW graph to find nearest docs.\nIn our case all of this will be used for semantic search (ANN) where found information will be ranked by the distance between the query vector and found items within the HNSW graph space.\nHybrid Search: Lexical Recall, Semantic Precision Hybrid retrieval = combine two worlds:\nLexical search (BM25): high recall, because it matches exact tokens. Semantic search (ANN): high precision, because it captures meaning. Workflow:\nUse BM25 to retrieve top N candidates fast. Use embeddings + ANN search to re-rank them by semantic similarity. Example:\nQuery: “cheap Italian food” BM25 finds docs with tokens “cheap”, “Italian”, “food”. ANN embedding search reranks so “affordable pizza place” ranks above “expensive Italian furniture”. The Cluster is Alive (System Design Magic) People think Elasticsearch is “just a search box.” Nope. It’s a distributed system disguised as a JSON API.\nEvery index is sharded. Each shard is a Lucene index. Replicas exist not just for HA, but also to spread query load. Queries fan out from a coordinator node to the right shards, then results fan back in. Cluster state? That’s kept by the master node(s), who do all the boring leader-election / metadata wrangling.\nThen there’s segment merging. Every refresh creates new segments (like immutable SSTables). Too many segments = sad query latency. So ES merges them in the background. But merging is expensive, which means you’re always trading indexing throughput vs query latency.\nCaches help but lie to you:\nQuery cache → only for repeated identical queries. Shard request cache → helps aggregations, not term lookups. FST-based autocomplete → literally stores prefix/suffix tries as finite state transducers in RAM. If you ever wondered why “near real-time” in ES means ~1s, it’s because of the refresh interval. You can lower it, but then merges \u0026 memory pressure will ruin your day.\nAnd yes, you can wire ES to your OLTP database via CDC (Change Data Capture). Tools like Debezium stream binlogs → Kafka → ES, giving you low-latency snapshot updates. That’s how you keep your search index within a second or two of reality without rewriting your entire app.\nAutocomplete Strategies ElasticSearch autocomplete is not a single feature but a set of clever data structures and algorithms that map a user’s partial input to complete terms efficiently. The simplest approach, edge n-grams, works by splitting each indexed word into all its prefixes. For example, the word “pizza” becomes “p”, “pi”, “piz”, “pizz”, and “pizza”. Conceptually, this can be represented as a prefix tree, or trie, where each node corresponds to a prefix and leaves correspond to complete terms. In TypeScript, one might build it like this:\ntype TrieNode = { children: Map\u003cstring, TrieNode\u003e, isWord: boolean } function insert(root: TrieNode, word: string) { let node = root for (const char of word) { if (!node.children.has(char)) node.children.set(char, { children: new Map(), isWord: false }) node = node.children.get(char)! } node.isWord = true } function autocomplete(root: TrieNode, prefix: string): string[] { let node = root for (const char of prefix) { if (!node.children.has(char)) return [] node = node.children.get(char)! } const results: string[] = [] function dfs(n: TrieNode, path: string) { if (n.isWord) results.push(path) for (const [c, child] of n.children) dfs(child, path + c) } dfs(node, prefix) return results } Edge n-grams are simple and fast to query, but they consume a lot of index space. For a more memory-efficient solution, ElasticSearch uses completion suggesters backed by finite state transducers (FSTs). FSTs compress common prefixes and store outputs such as document IDs or weights on edges. Traversing the FST from root to leaf enumerates all completions efficiently, essentially providing O(k) lookup time for prefixes of length k. Context suggesters extend this idea by attaching metadata to completions, like location or category, allowing queries like “pizza near Seattle” to return filtered autocomplete results without rebuilding the index.\nElasticSearch also handles typos in autocomplete using fuzzy matching, which is built on Levenshtein automata. Crucially, fuzzy matching is applied on top of the prefix / edge n-gram structure or FST. Each indexed prefix node is effectively a candidate, and the automaton enumerates all paths in the prefix tree that are within the allowed edit distance. For example, if a user types “piza” with a maximum edit distance of 1, the Levenshtein automaton explores the trie paths, allowing one insertion, deletion, or substitution, and still finds “pizza” as a valid completion. This means that fuzzy matching does not ignore the prefix structure; rather, it traverses the prefix tree or FST while tolerating small deviations, combining the efficiency of prefix search with the flexibility of typo tolerance.\nHighlighting in ElasticSearch It is about extracting and presenting the portions of text that match a query. There are three main strategies: plain highlighter, unified highlighter, and fast vector highlighter. The plain highlighter is simple, it re-analyzes the document and locates matches. The unified highlighter uses offsets from the inverted index to locate terms precisely and efficiently. The fast vector highlighter leverages pre-stored term vectors to avoid re-analysis. Conceptually, each document stores term positions and offsets, and the highlighter simply retrieves the spans for matched terms instead of scanning the text. In TypeScript pseudo-code:\ntype TermVector = { term: string, positions: number[], offsets: [number, number][] } function fastVectorHighlight(termVectors: TermVector[], queryTerms: string[]): [number, number][] { const spans: [number, number][] = [] for (const tv of termVectors) { if (queryTerms.includes(tv.term)) { spans.push(...tv.offsets) } } return spans } // Example: highlights could then be mapped to document text const docTermVectors: TermVector[] = [ { term: \"pizza\", positions: [1], offsets: [[7, 12]] }, { term: \"pasta\", positions: [3], offsets: [[17, 22]] } ] console.log(fastVectorHighlight(docTermVectors, [\"pizza\"])) // [[7, 12]] Aggregates getting called out Aggregations in ElasticSearch are distributed map-reduce operations, but understanding the mechanics can help you avoid surprises in CPU and memory usage. Each shard computes aggregations per segment and returns partial results, which the coordinating node merges into the final output. Even a simple count can spike CPU if shards have many segments or high-cardinality fields. Key points to keep in mind:\nSegment-level work: Each shard has multiple Lucene segments; aggregation runs on each segment. More segments = more work. Memory usage: High-cardinality terms or large hash tables can blow up memory on shards. Use shard_size, composite aggregations, or pre-aggregated counts to reduce load. Merging results: Coordinating node combines shard results. Simple sums are cheap, but top-N terms require sorting across shards. Filters matter: Apply filters before aggregations to reduce the data processed. Use filter aggregations or query-time filters strategically. Regarding memory allocation: ElasticSearch does not let you assign memory to specific aggregations, but you can influence resource availability by configuring your cluster. For aggregation-heavy workloads, it is common to deploy dedicated data nodes or coordinating nodes with larger heap sizes so they can handle the shard-level computations and merging without hitting memory limits. In cloud-managed clusters, you can also provision spot or on-demand nodes with more RAM during peak hours to absorb temporary spikes in aggregation load. Additionally, splitting very large aggregations into smaller composite aggregations reduces peak memory per shard and keeps operations predictable. With these strategies, aggregations become a tunable and reliable part of your search infrastructure, rather than a black box that unexpectedly consumes CPU or memory.\nWhen Theory Meets Production This is where things get fun:\nHot-warm-cold tiers: keep fresh stuff on SSDs, dump old logs on slow spinning rust. Cross-cluster search: federated queries across data centers. Yes, global search is a thing. Multi-tenancy problems: one noisy client with wildcard queries can starve everyone. Security: never expose ES to the public internet unless you like ransomware. In the end, ES is basically a mashup of:\nsearch engine theory (inverted indexes, automata, scoring), distributed systems design (shards, replicas, coordination), and AI-modern glue (embeddings, re-ranking). That’s why it feels magical. You can start with “find me pizza near Seattle” and end up deploying a hybrid lexical-semantic search system with CDC updates and ANN vectors - all in one stack.\n","wordCount":"2178","inLanguage":"en","image":"http://localhost:1313/images/papermod-cover.png","datePublished":"2025-08-01T04:52:54-07:00","dateModified":"2025-08-01T04:52:54-07:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/searching-the-unsearchable/"},"publisher":{"@type":"Organization","name":"T|A","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="T|A (Alt + H)">T|A</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about title=About><span>About</span></a></li><li><a href=http://localhost:1313/chronicles title=Chronicles><span>Chronicles</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://github.com/t-avil title=Github><span>Github</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">NOTES: Searching the unsearchable</h1><div class=post-description>Levenshtein automata, inverted indexes, BM25, segment merges, ANN vectors, CDC pipelines, and why Elasticsearch is basically three PhDs stapled together with JSON.</div><div class=post-meta><span title='2025-08-01 04:52:54 -0700 PDT'>August 1, 2025</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;2178 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/t-avil/blog/tree/main/content/posts/searching-the-unsearchable.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#fuzzy-search-is-more-than-just-levenshtein-distance>Fuzzy Search is More Than Just Levenshtein Distance</a><ul><li><a href=#enter-levenshtein-automata>Enter Levenshtein Automata</a></li></ul></li><li><a href=#inverted-indexes-and-compression-tricks>Inverted Indexes and Compression Tricks</a></li><li><a href=#scoring-tf-idf-vs-bm25>Scoring: TF-IDF vs BM25</a></li><li><a href=#query-execution-strategies>Query Execution Strategies</a></li><li><a href=#vector-search-and-hnsw-graphs>Vector Search and HNSW Graphs</a></li><li><a href=#hybrid-search-lexical-recall-semantic-precision>Hybrid Search: Lexical Recall, Semantic Precision</a></li><li><a href=#the-cluster-is-alive-system-design-magic>The Cluster is Alive (System Design Magic)</a></li><li><a href=#autocomplete-strategies>Autocomplete Strategies</a></li><li><a href=#highlighting-in-elasticsearch>Highlighting in ElasticSearch</a></li><li><a href=#aggregates-getting-called-out>Aggregates getting called out</a></li><li><a href=#when-theory-meets-production>When Theory Meets Production</a></li></ul></nav></div></details></div><div class=post-content><hr><h2 id=fuzzy-search-is-more-than-just-levenshtein-distance>Fuzzy Search is More Than Just Levenshtein Distance<a hidden class=anchor aria-hidden=true href=#fuzzy-search-is-more-than-just-levenshtein-distance>#</a></h2><p>Let’s start from the ground level. If you mistype &ldquo;pizzq&rdquo; but still want &ldquo;pizza&rdquo;, how does a search engine know? At the simplest level you could calculate <strong>Levenshtein distance</strong> between two strings: the minimum number of insertions, deletions, or substitutions required to turn one string into another.</p><p>Here’s a simple dynamic programming implementation in TypeScript:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ts data-lang=ts><span class=line><span class=cl><span class=kd>function</span> <span class=nx>levenshtein</span><span class=p>(</span><span class=nx>a</span>: <span class=kt>string</span><span class=p>,</span> <span class=nx>b</span>: <span class=kt>string</span><span class=p>)</span><span class=o>:</span> <span class=kt>number</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>dp</span>: <span class=kt>number</span><span class=p>[][]</span> <span class=o>=</span> <span class=nb>Array</span><span class=p>.</span><span class=kr>from</span><span class=p>({</span> <span class=nx>length</span>: <span class=kt>a.length</span> <span class=o>+</span> <span class=mi>1</span> <span class=p>},</span> <span class=p>()</span> <span class=o>=&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nb>Array</span><span class=p>(</span><span class=nx>b</span><span class=p>.</span><span class=nx>length</span> <span class=o>+</span> <span class=mi>1</span><span class=p>).</span><span class=nx>fill</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kd>let</span> <span class=nx>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=nx>i</span> <span class=o>&lt;=</span> <span class=nx>a</span><span class=p>.</span><span class=nx>length</span><span class=p>;</span> <span class=nx>i</span><span class=o>++</span><span class=p>)</span> <span class=nx>dp</span><span class=p>[</span><span class=nx>i</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=nx>i</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kd>let</span> <span class=nx>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=nx>j</span> <span class=o>&lt;=</span> <span class=nx>b</span><span class=p>.</span><span class=nx>length</span><span class=p>;</span> <span class=nx>j</span><span class=o>++</span><span class=p>)</span> <span class=nx>dp</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=nx>j</span><span class=p>]</span> <span class=o>=</span> <span class=nx>j</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kd>let</span> <span class=nx>i</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=nx>i</span> <span class=o>&lt;=</span> <span class=nx>a</span><span class=p>.</span><span class=nx>length</span><span class=p>;</span> <span class=nx>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kd>let</span> <span class=nx>j</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=nx>j</span> <span class=o>&lt;=</span> <span class=nx>b</span><span class=p>.</span><span class=nx>length</span><span class=p>;</span> <span class=nx>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=k>if</span> <span class=p>(</span><span class=nx>a</span><span class=p>[</span><span class=nx>i</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>===</span> <span class=nx>b</span><span class=p>[</span><span class=nx>j</span> <span class=o>-</span> <span class=mi>1</span><span class=p>])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nx>dp</span><span class=p>[</span><span class=nx>i</span><span class=p>][</span><span class=nx>j</span><span class=p>]</span> <span class=o>=</span> <span class=nx>dp</span><span class=p>[</span><span class=nx>i</span> <span class=o>-</span> <span class=mi>1</span><span class=p>][</span><span class=nx>j</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nx>dp</span><span class=p>[</span><span class=nx>i</span><span class=p>][</span><span class=nx>j</span><span class=p>]</span> <span class=o>=</span> <span class=nb>Math</span><span class=p>.</span><span class=nx>min</span><span class=p>(</span>
</span></span><span class=line><span class=cl>          <span class=nx>dp</span><span class=p>[</span><span class=nx>i</span> <span class=o>-</span> <span class=mi>1</span><span class=p>][</span><span class=nx>j</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span>    <span class=c1>// deletion
</span></span></span><span class=line><span class=cl><span class=c1></span>          <span class=nx>dp</span><span class=p>[</span><span class=nx>i</span><span class=p>][</span><span class=nx>j</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span>    <span class=c1>// insertion
</span></span></span><span class=line><span class=cl><span class=c1></span>          <span class=nx>dp</span><span class=p>[</span><span class=nx>i</span> <span class=o>-</span> <span class=mi>1</span><span class=p>][</span><span class=nx>j</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span> <span class=c1>// substitution
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nx>dp</span><span class=p>[</span><span class=nx>a</span><span class=p>.</span><span class=nx>length</span><span class=p>][</span><span class=nx>b</span><span class=p>.</span><span class=nx>length</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=nx>levenshtein</span><span class=p>(</span><span class=s2>&#34;pizza&#34;</span><span class=p>,</span> <span class=s2>&#34;pizzq&#34;</span><span class=p>))</span> <span class=c1>// 1
</span></span></span></code></pre></div><p>This works, but if you have millions of documents it’s way too slow to compare every string pairwise.</p><h3 id=enter-levenshtein-automata>Enter Levenshtein Automata<a hidden class=anchor aria-hidden=true href=#enter-levenshtein-automata>#</a></h3><p>Instead of comparing against every document, you build a <strong>deterministic finite automaton (DFA)</strong> that represents all words within edit distance k of your query. Then you run the DFA against the index.</p><p>The states of the automaton represent &ldquo;how many edits we have left&rdquo; and &ldquo;where in the word we are&rdquo;. A simplified DFA construction for edit distance 1 might look like:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ts data-lang=ts><span class=line><span class=cl><span class=kr>type</span> <span class=nx>State</span> <span class=o>=</span> <span class=p>{</span> <span class=nx>index</span>: <span class=kt>number</span><span class=p>;</span> <span class=nx>edits</span>: <span class=kt>number</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kd>function</span> <span class=nx>nextStates</span><span class=p>(</span><span class=nx>state</span>: <span class=kt>State</span><span class=p>,</span> <span class=nx>char</span>: <span class=kt>string</span><span class=p>)</span><span class=o>:</span> <span class=nx>State</span><span class=p>[]</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=p>{</span> <span class=nx>index</span><span class=p>,</span> <span class=nx>edits</span> <span class=p>}</span> <span class=o>=</span> <span class=nx>state</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>states</span>: <span class=kt>State</span><span class=p>[]</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// Match without edit
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=nx>states</span><span class=p>.</span><span class=nx>push</span><span class=p>({</span> <span class=nx>index</span>: <span class=kt>index</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=nx>edits</span> <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=nx>edits</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Substitution
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nx>states</span><span class=p>.</span><span class=nx>push</span><span class=p>({</span> <span class=nx>index</span>: <span class=kt>index</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=nx>edits</span>: <span class=kt>edits</span> <span class=o>-</span> <span class=mi>1</span> <span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Insertion
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nx>states</span><span class=p>.</span><span class=nx>push</span><span class=p>({</span> <span class=nx>index</span><span class=p>,</span> <span class=nx>edits</span>: <span class=kt>edits</span> <span class=o>-</span> <span class=mi>1</span> <span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Deletion
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nx>states</span><span class=p>.</span><span class=nx>push</span><span class=p>({</span> <span class=nx>index</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=nx>edits</span>: <span class=kt>edits</span> <span class=o>-</span> <span class=mi>1</span> <span class=p>})</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nx>states</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>In production Lucene builds these automata efficiently using special algorithms, but the concept is the same: instead of checking one string at a time, you check all possible close strings in parallel.</p><hr><h2 id=inverted-indexes-and-compression-tricks>Inverted Indexes and Compression Tricks<a hidden class=anchor aria-hidden=true href=#inverted-indexes-and-compression-tricks>#</a></h2><p>Search engines don’t store documents like databases do. They flip the structure into an <strong>inverted index</strong>:</p><pre tabindex=0><code>pizza -&gt; [doc1, doc3, doc7]
pizzeria -&gt; [doc2, doc5]
pepperoni -&gt; [doc1, doc4, doc7]
</code></pre><p>Now queries become intersections of lists. To make this efficient, the lists are compressed.</p><p>Example: storing document IDs <code>[100, 101, 103, 110]</code>. Instead of storing raw numbers, store <strong>gaps</strong>:</p><pre tabindex=0><code>[100, 1, 2, 7]
</code></pre><p>Then apply variable-length encoding:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ts data-lang=ts><span class=line><span class=cl><span class=kd>function</span> <span class=nx>encodeVarint</span><span class=p>(</span><span class=nx>nums</span>: <span class=kt>number</span><span class=p>[])</span><span class=o>:</span> <span class=kt>number</span><span class=p>[]</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>bytes</span>: <span class=kt>number</span><span class=p>[]</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kr>const</span> <span class=nx>n</span> <span class=k>of</span> <span class=nx>nums</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kd>let</span> <span class=nx>value</span> <span class=o>=</span> <span class=nx>n</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=nx>value</span> <span class=o>&gt;=</span> <span class=mi>128</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nx>bytes</span><span class=p>.</span><span class=nx>push</span><span class=p>((</span><span class=nx>value</span> <span class=o>&amp;</span> <span class=mh>0x7f</span><span class=p>)</span> <span class=o>|</span> <span class=mh>0x80</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=nx>value</span> <span class=o>&gt;&gt;=</span> <span class=mi>7</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=nx>bytes</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=nx>value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nx>bytes</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=nx>encodeVarint</span><span class=p>([</span><span class=mi>100</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>7</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=c1>// -&gt; [100, 1, 2, 7] but each as compressed bytes
</span></span></span></code></pre></div><p>This allows Elasticsearch to store millions of postings in memory and still intersect them quickly.</p><hr><h2 id=scoring-tf-idf-vs-bm25>Scoring: TF-IDF vs BM25<a hidden class=anchor aria-hidden=true href=#scoring-tf-idf-vs-bm25>#</a></h2><p>Both TF-IDF and BM25 are ways to rank documents based on how relevant they are to the query.</p><ul><li><p><strong>TF-IDF</strong>:</p><ul><li>Term Frequency (TF): how many times the word appears in the document.</li><li>Inverse Document Frequency (IDF): how rare the word is across the collection.</li><li>Score = TF * IDF.</li></ul></li></ul><p>The problem: TF grows linearly. A word appearing 50 times is not 50x more relevant.</p><ul><li><p><strong>BM25</strong>:
BM25 adds two important ideas:</p><ul><li><strong>Saturation</strong>: additional term frequency gives diminishing returns.</li><li><strong>Normalization</strong>: longer documents should not always get higher scores just because they have more words.</li></ul></li></ul><p>The BM25 formula looks like this:</p><pre tabindex=0><code>score = IDF * ( (tf * (k+1)) / (tf + k * (1 - b + b * (docLen / avgDocLen))) )
</code></pre><p>Where k controls saturation and b controls length normalization.</p><hr><h2 id=query-execution-strategies>Query Execution Strategies<a hidden class=anchor aria-hidden=true href=#query-execution-strategies>#</a></h2><p>When combining postings lists, there are two strategies:</p><ul><li><strong>Term-at-a-time (TAAT)</strong>: Process one query term’s postings at a time, updating partial scores.</li><li><strong>Document-at-a-time (DAAT)</strong>: Walk all postings lists in sync, scoring one document fully before moving on.</li></ul><p>Example: Query = &ldquo;pizza OR pasta&rdquo;.</p><ul><li><p>TAAT:</p><ul><li>Scan postings for &ldquo;pizza&rdquo; → update scores.</li><li>Scan postings for &ldquo;pasta&rdquo; → update scores.</li></ul></li><li><p>DAAT:</p><ul><li>Look at doc1 → check if it has pizza or pasta → assign final score.</li><li>Move to doc2 → repeat.</li></ul></li></ul><p>DAAT is usually better for top-k queries because you can use a heap and stop early.</p><hr><h2 id=vector-search-and-hnsw-graphs>Vector Search and HNSW Graphs<a hidden class=anchor aria-hidden=true href=#vector-search-and-hnsw-graphs>#</a></h2><p>Elasticsearch supports vector search with <strong>HNSW graphs (Hierarchical Navigable Small Worlds)</strong>.</p><p>Imagine each document as a point in high-dimensional space. Searching means finding nearest neighbors. Brute-force is O(n), too slow.</p><p>HNSW builds a multi-layer graph:</p><ul><li>Upper layers have long links (skip connections).</li><li>Lower layers have dense local connections.</li></ul><p>Search walks top-down: start at the top layer, use long edges to get close, then drop down layers until you reach the closest nodes. Complexity ~log N.</p><p>In Elasticsearch you encode text into vectors using a model, then store them as <code>dense_vector</code>. At query time, you provide a query vector and ES traverses the HNSW graph to find nearest docs.</p><p>In our case all of this will be used for semantic search (ANN) where found information will be ranked by the distance between the query vector and found items within the HNSW graph space.</p><hr><h2 id=hybrid-search-lexical-recall-semantic-precision>Hybrid Search: Lexical Recall, Semantic Precision<a hidden class=anchor aria-hidden=true href=#hybrid-search-lexical-recall-semantic-precision>#</a></h2><p>Hybrid retrieval = combine two worlds:</p><ul><li><strong>Lexical search (BM25)</strong>: high recall, because it matches exact tokens.</li><li><strong>Semantic search (ANN)</strong>: high precision, because it captures meaning.</li></ul><p>Workflow:</p><ol><li>Use BM25 to retrieve top N candidates fast.</li><li>Use embeddings + ANN search to re-rank them by semantic similarity.</li></ol><p>Example:</p><ul><li>Query: &ldquo;cheap Italian food&rdquo;</li><li>BM25 finds docs with tokens &ldquo;cheap&rdquo;, &ldquo;Italian&rdquo;, &ldquo;food&rdquo;.</li><li>ANN embedding search reranks so &ldquo;affordable pizza place&rdquo; ranks above &ldquo;expensive Italian furniture&rdquo;.</li></ul><h2 id=the-cluster-is-alive-system-design-magic>The Cluster is Alive (System Design Magic)<a hidden class=anchor aria-hidden=true href=#the-cluster-is-alive-system-design-magic>#</a></h2><p>People think Elasticsearch is “just a search box.” Nope. It’s a distributed system disguised as a JSON API.</p><p>Every index is sharded. Each shard is a Lucene index. Replicas exist not just for HA, but also to spread query load. Queries fan out from a <strong>coordinator node</strong> to the right shards, then results fan back in. Cluster state? That’s kept by the <strong>master node(s)</strong>, who do all the boring leader-election / metadata wrangling.</p><p>Then there’s <strong>segment merging.</strong> Every refresh creates new segments (like immutable SSTables). Too many segments = sad query latency. So ES merges them in the background. But merging is expensive, which means you’re always trading <strong>indexing throughput vs query latency.</strong></p><p>Caches help but lie to you:</p><ul><li>Query cache → only for repeated identical queries.</li><li>Shard request cache → helps aggregations, not term lookups.</li><li>FST-based autocomplete → literally stores prefix/suffix tries as finite state transducers in RAM.</li></ul><p>If you ever wondered why “near real-time” in ES means ~1s, it’s because of the <strong>refresh interval.</strong> You can lower it, but then merges & memory pressure will ruin your day.</p><p>And yes, you can wire ES to your OLTP database via <strong>CDC (Change Data Capture)</strong>. Tools like Debezium stream binlogs → Kafka → ES, giving you low-latency snapshot updates. That’s how you keep your search index within a second or two of reality without rewriting your entire app.</p><hr><h2 id=autocomplete-strategies>Autocomplete Strategies<a hidden class=anchor aria-hidden=true href=#autocomplete-strategies>#</a></h2><p>ElasticSearch autocomplete is not a single feature but a set of clever data structures and algorithms that map a user’s partial input to complete terms efficiently. The simplest approach, edge n-grams, works by splitting each indexed word into all its prefixes. For example, the word “pizza” becomes “p”, “pi”, “piz”, “pizz”, and “pizza”. Conceptually, this can be represented as a prefix tree, or trie, where each node corresponds to a prefix and leaves correspond to complete terms. In TypeScript, one might build it like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ts data-lang=ts><span class=line><span class=cl><span class=kr>type</span> <span class=nx>TrieNode</span> <span class=o>=</span> <span class=p>{</span> <span class=nx>children</span>: <span class=kt>Map</span><span class=p>&lt;</span><span class=nt>string</span><span class=p>,</span> <span class=na>TrieNode</span><span class=p>&gt;,</span> <span class=nx>isWord</span>: <span class=kt>boolean</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kd>function</span> <span class=nx>insert</span><span class=p>(</span><span class=nx>root</span>: <span class=kt>TrieNode</span><span class=p>,</span> <span class=nx>word</span>: <span class=kt>string</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kd>let</span> <span class=nx>node</span> <span class=o>=</span> <span class=nx>root</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kr>const</span> <span class=nx>char</span> <span class=k>of</span> <span class=nx>word</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=nx>node</span><span class=p>.</span><span class=nx>children</span><span class=p>.</span><span class=nx>has</span><span class=p>(</span><span class=nx>char</span><span class=p>))</span> <span class=nx>node</span><span class=p>.</span><span class=nx>children</span><span class=p>.</span><span class=kr>set</span><span class=p>(</span><span class=nx>char</span><span class=p>,</span> <span class=p>{</span> <span class=nx>children</span>: <span class=kt>new</span> <span class=nx>Map</span><span class=p>(),</span> <span class=nx>isWord</span>: <span class=kt>false</span> <span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=nx>node</span> <span class=o>=</span> <span class=nx>node</span><span class=p>.</span><span class=nx>children</span><span class=p>.</span><span class=kr>get</span><span class=p>(</span><span class=nx>char</span><span class=p>)</span><span class=o>!</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=nx>node</span><span class=p>.</span><span class=nx>isWord</span> <span class=o>=</span> <span class=kc>true</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kd>function</span> <span class=nx>autocomplete</span><span class=p>(</span><span class=nx>root</span>: <span class=kt>TrieNode</span><span class=p>,</span> <span class=nx>prefix</span>: <span class=kt>string</span><span class=p>)</span><span class=o>:</span> <span class=kt>string</span><span class=p>[]</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kd>let</span> <span class=nx>node</span> <span class=o>=</span> <span class=nx>root</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kr>const</span> <span class=nx>char</span> <span class=k>of</span> <span class=nx>prefix</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=nx>node</span><span class=p>.</span><span class=nx>children</span><span class=p>.</span><span class=nx>has</span><span class=p>(</span><span class=nx>char</span><span class=p>))</span> <span class=k>return</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=nx>node</span> <span class=o>=</span> <span class=nx>node</span><span class=p>.</span><span class=nx>children</span><span class=p>.</span><span class=kr>get</span><span class=p>(</span><span class=nx>char</span><span class=p>)</span><span class=o>!</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>results</span>: <span class=kt>string</span><span class=p>[]</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>  <span class=kd>function</span> <span class=nx>dfs</span><span class=p>(</span><span class=nx>n</span>: <span class=kt>TrieNode</span><span class=p>,</span> <span class=nx>path</span>: <span class=kt>string</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>n</span><span class=p>.</span><span class=nx>isWord</span><span class=p>)</span> <span class=nx>results</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=nx>path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kr>const</span> <span class=p>[</span><span class=nx>c</span><span class=p>,</span> <span class=nx>child</span><span class=p>]</span> <span class=k>of</span> <span class=nx>n</span><span class=p>.</span><span class=nx>children</span><span class=p>)</span> <span class=nx>dfs</span><span class=p>(</span><span class=nx>child</span><span class=p>,</span> <span class=nx>path</span> <span class=o>+</span> <span class=nx>c</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=nx>dfs</span><span class=p>(</span><span class=nx>node</span><span class=p>,</span> <span class=nx>prefix</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nx>results</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Edge n-grams are simple and fast to query, but they consume a lot of index space. For a more memory-efficient solution, ElasticSearch uses <strong>completion suggesters backed by finite state transducers (FSTs)</strong>. FSTs compress common prefixes and store outputs such as document IDs or weights on edges. Traversing the FST from root to leaf enumerates all completions efficiently, essentially providing O(k) lookup time for prefixes of length k. Context suggesters extend this idea by attaching metadata to completions, like location or category, allowing queries like “pizza near Seattle” to return filtered autocomplete results without rebuilding the index.</p><p>ElasticSearch also handles <strong>typos in autocomplete using fuzzy matching</strong>, which is built on <strong>Levenshtein automata</strong>. Crucially, fuzzy matching is <strong>applied on top of the prefix / edge n-gram structure or FST</strong>. Each indexed prefix node is effectively a candidate, and the automaton enumerates all paths in the prefix tree that are within the allowed edit distance. For example, if a user types “piza” with a maximum edit distance of 1, the Levenshtein automaton explores the trie paths, allowing one insertion, deletion, or substitution, and still finds “pizza” as a valid completion. This means that fuzzy matching does not ignore the prefix structure; rather, it <strong>traverses the prefix tree or FST while tolerating small deviations</strong>, combining the efficiency of prefix search with the flexibility of typo tolerance.</p><hr><h2 id=highlighting-in-elasticsearch>Highlighting in ElasticSearch<a hidden class=anchor aria-hidden=true href=#highlighting-in-elasticsearch>#</a></h2><p>It is about extracting and presenting the portions of text that match a query. There are three main strategies: plain highlighter, unified highlighter, and fast vector highlighter. The plain highlighter is simple, it re-analyzes the document and locates matches. The unified highlighter uses offsets from the inverted index to locate terms precisely and efficiently. The fast vector highlighter leverages <strong>pre-stored term vectors</strong> to avoid re-analysis. Conceptually, each document stores term positions and offsets, and the highlighter simply retrieves the spans for matched terms instead of scanning the text. In TypeScript pseudo-code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ts data-lang=ts><span class=line><span class=cl><span class=kr>type</span> <span class=nx>TermVector</span> <span class=o>=</span> <span class=p>{</span> <span class=nx>term</span>: <span class=kt>string</span><span class=p>,</span> <span class=nx>positions</span>: <span class=kt>number</span><span class=p>[],</span> <span class=nx>offsets</span><span class=o>:</span> <span class=p>[</span><span class=kt>number</span><span class=p>,</span> <span class=kt>number</span><span class=p>][]</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kd>function</span> <span class=nx>fastVectorHighlight</span><span class=p>(</span><span class=nx>termVectors</span>: <span class=kt>TermVector</span><span class=p>[],</span> <span class=nx>queryTerms</span>: <span class=kt>string</span><span class=p>[])</span><span class=o>:</span> <span class=p>[</span><span class=kt>number</span><span class=p>,</span> <span class=kt>number</span><span class=p>][]</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kr>const</span> <span class=nx>spans</span><span class=o>:</span> <span class=p>[</span><span class=kt>number</span><span class=p>,</span> <span class=kt>number</span><span class=p>][]</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kr>const</span> <span class=nx>tv</span> <span class=k>of</span> <span class=nx>termVectors</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>queryTerms</span><span class=p>.</span><span class=nx>includes</span><span class=p>(</span><span class=nx>tv</span><span class=p>.</span><span class=nx>term</span><span class=p>))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nx>spans</span><span class=p>.</span><span class=nx>push</span><span class=p>(...</span><span class=nx>tv</span><span class=p>.</span><span class=nx>offsets</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nx>spans</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// Example: highlights could then be mapped to document text
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kr>const</span> <span class=nx>docTermVectors</span>: <span class=kt>TermVector</span><span class=p>[]</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span> <span class=nx>term</span><span class=o>:</span> <span class=s2>&#34;pizza&#34;</span><span class=p>,</span> <span class=nx>positions</span><span class=o>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=nx>offsets</span><span class=o>:</span> <span class=p>[[</span><span class=mi>7</span><span class=p>,</span> <span class=mi>12</span><span class=p>]]</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span> <span class=nx>term</span><span class=o>:</span> <span class=s2>&#34;pasta&#34;</span><span class=p>,</span> <span class=nx>positions</span><span class=o>:</span> <span class=p>[</span><span class=mi>3</span><span class=p>],</span> <span class=nx>offsets</span><span class=o>:</span> <span class=p>[[</span><span class=mi>17</span><span class=p>,</span> <span class=mi>22</span><span class=p>]]</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=nx>fastVectorHighlight</span><span class=p>(</span><span class=nx>docTermVectors</span><span class=p>,</span> <span class=p>[</span><span class=s2>&#34;pizza&#34;</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=c1>// [[7, 12]]
</span></span></span></code></pre></div><hr><h2 id=aggregates-getting-called-out>Aggregates getting called out<a hidden class=anchor aria-hidden=true href=#aggregates-getting-called-out>#</a></h2><p>Aggregations in ElasticSearch are <strong>distributed map-reduce operations</strong>, but understanding the mechanics can help you avoid surprises in CPU and memory usage. Each shard computes aggregations <strong>per segment</strong> and returns partial results, which the coordinating node merges into the final output. Even a simple count can spike CPU if shards have many segments or high-cardinality fields. Key points to keep in mind:</p><ul><li><strong>Segment-level work:</strong> Each shard has multiple Lucene segments; aggregation runs on each segment. More segments = more work.</li><li><strong>Memory usage:</strong> High-cardinality terms or large hash tables can blow up memory on shards. Use <code>shard_size</code>, <code>composite aggregations</code>, or pre-aggregated counts to reduce load.</li><li><strong>Merging results:</strong> Coordinating node combines shard results. Simple sums are cheap, but top-N terms require sorting across shards.</li><li><strong>Filters matter:</strong> Apply filters before aggregations to reduce the data processed. Use <code>filter</code> aggregations or query-time filters strategically.</li></ul><p>Regarding memory allocation: ElasticSearch does not let you assign memory to specific aggregations, but you can influence resource availability by configuring your cluster. For aggregation-heavy workloads, it is common to deploy <strong>dedicated data nodes or coordinating nodes</strong> with larger heap sizes so they can handle the shard-level computations and merging without hitting memory limits. In cloud-managed clusters, you can also provision <strong>spot or on-demand nodes with more RAM during peak hours</strong> to absorb temporary spikes in aggregation load. Additionally, splitting very large aggregations into <strong>smaller composite aggregations</strong> reduces peak memory per shard and keeps operations predictable. With these strategies, aggregations become a <strong>tunable and reliable part of your search infrastructure</strong>, rather than a black box that unexpectedly consumes CPU or memory.</p><hr><h2 id=when-theory-meets-production>When Theory Meets Production<a hidden class=anchor aria-hidden=true href=#when-theory-meets-production>#</a></h2><p>This is where things get fun:</p><ul><li><strong>Hot-warm-cold tiers</strong>: keep fresh stuff on SSDs, dump old logs on slow spinning rust.</li><li><strong>Cross-cluster search</strong>: federated queries across data centers. Yes, global search is a thing.</li><li><strong>Multi-tenancy problems</strong>: one noisy client with wildcard queries can starve everyone.</li><li><strong>Security</strong>: never expose ES to the public internet unless you like ransomware.</li></ul><p>In the end, ES is basically a mashup of:</p><ul><li>search engine theory (inverted indexes, automata, scoring),</li><li>distributed systems design (shards, replicas, coordination),</li><li>and AI-modern glue (embeddings, re-ranking).</li></ul><p>That’s why it feels magical. You can start with “find me pizza near Seattle” and end up deploying a <strong>hybrid lexical-semantic search system with CDC updates and ANN vectors</strong> - all in one stack.</p><hr></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/notes/>Notes</a></li><li><a href=http://localhost:1313/tags/elasticsearch/>Elasticsearch</a></li><li><a href=http://localhost:1313/tags/search/>Search</a></li><li><a href=http://localhost:1313/tags/system-design/>System-Design</a></li><li><a href=http://localhost:1313/tags/distributed-systems/>Distributed-Systems</a></li><li><a href=http://localhost:1313/tags/vectors/>Vectors</a></li><li><a href=http://localhost:1313/tags/fuzzy/>Fuzzy</a></li><li><a href=http://localhost:1313/tags/ai/>Ai</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/model-training-partitioning/><span class=title>« Prev</span><br><span>NOTES: Exploring large-scale distributed training systems</span>
</a><a class=next href=http://localhost:1313/posts/galaxy-zooing-my-first-cnn/><span class=title>Next »</span><br><span>PROJECT: Galaxy Zooing my first CNN</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>T|A</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>