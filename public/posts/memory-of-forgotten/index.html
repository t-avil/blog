<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NOTES: Memory of Forgotten | T|A</title><meta name=keywords content="notes,memory,architecture,numa,storage,systems"><meta name=description content="From registers to cloud-scale storage: deep notes on memory, latency, hierarchy, and systems research."><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.3f7ba6a00d316a1658af1e52b60f5592bfd3f63e1683217d447958625c9fec2a.css integrity="sha256-P3umoA0xahZYrx5Stg9Vkr/T9j4WgyF9RHlYYlyf7Co=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/memory-of-forgotten/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/memory-of-forgotten/"><meta property="og:site_name" content="T|A"><meta property="og:title" content="NOTES: Memory of Forgotten"><meta property="og:description" content="From registers to cloud-scale storage: deep notes on memory, latency, hierarchy, and systems research."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-21T23:20:02-08:00"><meta property="article:modified_time" content="2026-01-21T23:20:02-08:00"><meta property="article:tag" content="Notes"><meta property="article:tag" content="Memory"><meta property="article:tag" content="Architecture"><meta property="article:tag" content="Numa"><meta property="article:tag" content="Storage"><meta property="article:tag" content="Systems"><meta property="og:image" content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:title content="NOTES: Memory of Forgotten"><meta name=twitter:description content="From registers to cloud-scale storage: deep notes on memory, latency, hierarchy, and systems research."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"NOTES: Memory of Forgotten","item":"http://localhost:1313/posts/memory-of-forgotten/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NOTES: Memory of Forgotten","name":"NOTES: Memory of Forgotten","description":"From registers to cloud-scale storage: deep notes on memory, latency, hierarchy, and systems research.","keywords":["notes","memory","architecture","numa","storage","systems"],"articleBody":" Love, CPU Memory Access, FreeBSD Project - https://people.freebsd.org/~lstewart/articles/cpumemory.pdf\nHeMem: Scalable Tiered Memory Management for Big Data Applications and Real NVM, SOSP 2021 - https://doi.org/10.1145/3477132.3483550 (Technion)\nAssise: Performance and Availability via Client-local NVM in a Distributed File System, OSDI 2020 - https://www.usenix.org/system/files/osdi20-anderson.pdf (USENIX)\nDon’t Be a Blockhead: Zoned Namespaces Make Work on Conventional SSDs Obsolete, HotOS 2021 - https://doi.org/10.1145/3458336.3465300 (ACM SIGOPS)\nMemory systems in computing are a study of distance and control more than bits. Every level-from CPU registers to the cluster fabric-exists because cost (latency, energy) rises with distance and abstraction. What follows is a deep, chronological and conceptual journey from the smallest unit of state to systems that span racks, guided by both physics and software research.\n1. Registers and the First Illusion: Flat Memory At the very core of a CPU pipeline lie registers-architectural state local to the execution units. These are not “memory” in the conventional sense; they are bindings for values in-flight, allocated by the instruction scheduler and renamed to avoid false dependencies. Access to registers happens in a fraction of a nanosecond (≈0.25 ns, roughly one cycle on modern CPUs), and there is no latency to hide: data is present or it is not.\nRegisters are ephemeral. Once another instruction needs a slot, values spill to cache. This spill is the first time memory begins to matter.\n2. Caches: SRAM, Separate Instruction/Data Paths, and Coherence Modern microarchitectures layer caches between registers and main memory, constructed from Static RAM (SRAM). Unlike DRAM, SRAM doesn’t need refresh, making it fast but expensive and lower density.\nThe hierarchy is subtle:\nL1 Instruction (I-cache) and L1 Data (D-cache) are segregated. They serve different domains: fetched instructions and data operands. This separation is why self-modifying code and instruction fetch pipelines require explicit synchronization; they operate in different physical units. Love’s analysis shows that these distinctions carry performance implications that software rarely sees but hardware formalizes. (Awesome Papers)\nL2 cache often consolidates data and instructions for each core, trading size for latency.\nL3 cache is a shared, inclusive or non-inclusive substrate that enforces coherence across cores.\nCoherence protocols (MESI/MOESI) are distributed state machines: a load miss in L1 can trigger remote probes, turning what looked like a microsecond-scale access into a micro-protocol across cores. The FreeBSD CPU memory paper emphasizes that cache misses are topology events, not simple timing numbers. (Awesome Papers)\n3. NUMA: Latency with Geography In multisocket servers, memory has an accent. Each socket has its own memory controller and channels. An access to local DRAM may be ≈100 ns, but going across a socket can double that, and fetching a cache line from another core on a remote socket can take hundreds of nanoseconds.\nNUMA (Non-Uniform Memory Access) emerges not as an optimization but as an architectural necessity: we physically cannot centralize all memory without paying severe latency and bandwidth penalties. To manage this, OSes and runtime systems must be NUMA-aware-pinning pages and threads to local memory, migrating hot pages, and orchestrating data locality. Failure to do so makes “shared memory” feel like remote I/O.\n4. DRAM: The Silent Middle Dynamic RAM (DRAM) is the workhorse of main memory. It stores bits in capacitive cells that leak charge and need periodic refresh. DRAM’s latency (~80-120 ns) comes from row activation, precharge, and sense amplification: operations invisible to software but fundamental to performance.\nCrucially, DRAM is volatile. It forgets everything the moment power is removed. This fact influenced decades of OS design: everything important must be safely stored on stable media and brought into DRAM for processing.\n5. Hard Disk Drives: The Tyranny of Mechanics Hard Disk Drives (HDDs) predate all of the above. Their performance is dictated by mechanical physics:\nSeek time to position the head over the track (≈5-10 ms). Rotational latency waiting for the platter to spin (≈8.5 ms at 7200 RPM). Transfer time once the head is on target. These orders-of-magnitude latencies made sequential access the holy grail: once the head was placed, the cost of reading many bytes was amortized. fsck, FFS, NTFS, ext*, and their spatial allocation policies arise from this tyranny: co-locate related blocks to exploit hardware characteristics.\nShingled Magnetic Recording (SMR) took this further by overlapping tracks to increase density. SMR’s tracks behave like zones; writing to one track overwrites neighbors, forcing sequential, host-managed writes. It reshapes the classical block model and foreshadows zoned SSDs. (Wikipedia)\n6. SSDs and Operational Asymmetry SSD’s flash memory removed moving parts but introduced a new constraint: erase-before-write. Flash is composed of:\nSmall read/write pages (≈4 KB) Large erase blocks (≈1-8 MB) Write operations that must be to empty pages Erase operations that clear whole blocks The result is operational asymmetry: reads and writes are cheap, erases are expensive and erode lifetime. To present a conventional block interface, SSDs employ a Flash Translation Layer (FTL), which dynamically remaps logical blocks to physical pages and hides physical realities.\nThe problem is Garbage Collection (GC): when the drive runs out of clean blocks, the FTL must:\nIdentify a victim erase block Copy its live pages elsewhere Erase it before reuse This hidden GC work causes unpredictable tail latency-a nemesis of storage performance. The OS cannot see inside the FTL, so it can’t plan around these spikes.\n7. Zoned SSDs: Reclaiming Control Stavrinos et al.’s Don’t Be a Blockhead: Zoned Namespaces Make Work on Conventional SSDs Obsolete argues for exposing flash characteristics to the host instead of hiding them behind a block layer. ZNS SSDs divide storage into zones that must be written sequentially and explicitly reset by the host, removing device-side GC and giving control to software. (ACM SIGOPS)\nWith ZNS:\nThe host knows about physical zone boundaries. Garbage collection becomes an explicit action. Software can place hot and cold data to minimize erasure. Write amplification can be reduced because the system can group writes by temperature. This is a major shift: from opaque devices that manage everything internally to devices that require host-managed placement, analogous to how SMR forced host-managed zones on HDDs.\n8. Tiered Memory: PCM and NVM Emerging Non-Volatile Memory (NVM) technologies like Phase Change Memory (PCM) and Intel Optane DC blur the line between memory and storage. They are:\nByte-addressable Persistent Slower than DRAM (≈175-200 ns) but faster than SSDs Higher density and cheaper per bit This disrupts the long-standing assumption that memory must be volatile and homogeneous. NVM invites us to rethink memory hierarchies: perhaps memory is not a flat space but a tiered continuum from registers to persistent storage.\n9. HeMem: Software Tiered Memory Management HeMem: Scalable Tiered Memory Management for Big Data Applications and Real NVM tackles this directly. The paper identifies that conventional hardware and OS approaches to tiered memory (e.g., Intel Optane DRAM cache or naive OS-level NUMA-like placement) fail when real NVM is part of the hierarchy. (Technion)\nInstead of piggybacking on page tables, page faults, or simplistic heuristics, HeMem:\nUses CPU performance counters to sample memory access patterns asynchronously. Offloads page migration with DMA, which avoids CPU intervention and cache pollution. Accounts for asymmetric bandwidth between DRAM and NVM. Places policy in user space, letting applications customize heat-aware placement. HeMem’s insights are deep: hotness is a signal, not a static stat, and migration costs must be amortized and overlapped with application execution. The result is improved runtime and reduced tail latency for big data workloads on real tiered hardware.\n10. CXL and Disaggregated Memory Compute Express Link (CXL) decouples memory from the CPU socket, ushering in memory pooling and disaggregation. CXL memory can be coherent across devices and processors yet sits off the traditional DRAM bus. This enables:\nElastic memory expansion Shared memory pools Coherent access over an interconnect However, latency costs rise: CXL-attached memory is slower than slot-attached DIMMs but faster than networked block storage-creating another tier in the latency hierarchy.\nManaging this continuum demands systems like HeMem or more advanced OS policies that treat memory as elastic fabric, not static banks.\n11. Assise: Distributed File Systems With Client-local NVM Assise: Performance and Availability via Client-local NVM in a Distributed File System applies tiered memory thinking to distributed storage. Instead of a central server managing storage IO, Assise uses client-local persistent memory to cache and serve file system state directly. It builds the first persistent, replicated cache coherence layer (CC-NVM) that provides:\nCrash consistency via write-logging Linearizability across nodes Low tail latency by maximizing local access Fast failover using hot replicas (USENIX) Assise shows that distributed memory and storage should not be separate: locality and consistency are cheaper to maintain if we treat memory as a first-class, persistent tier at every node.\n12. From Memory to Storage: A Conceptual Synthesis The evolution of data management is a move from static geometry to dynamic placement:\nHDDs taught us physical locality matters SSDs taught us operational asymmetry dictates latency ZNS SSDs and PCM/NVM taught us host-managed placement is inevitable CXL taught us memory is fabric HeMem and Assise taught us software must manage tiered heat and coherence Traditional abstractions (flat memory, block interfaces, opaque devices) worked when hardware was slow and simple. As hardware becomes more complex-with multiple performance surfaces and asymmetries-software must regain control.\n13. Conclusion Memory is not a flat space. Every layer imposes cost, and every abstraction hides complexity. To build predictable systems, we must make the invisible visible to software-the hidden GC, coherence traffic, remote latencies, and tier boundaries-and manage them with informed policies, not hopes.\n","wordCount":"1562","inLanguage":"en","image":"http://localhost:1313/images/papermod-cover.png","datePublished":"2026-01-21T23:20:02-08:00","dateModified":"2026-01-21T23:20:02-08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/memory-of-forgotten/"},"publisher":{"@type":"Organization","name":"T|A","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="T|A (Alt + H)">T|A</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about title=About><span>About</span></a></li><li><a href=http://localhost:1313/chronicles title=Chronicles><span>Chronicles</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://github.com/t-avil title=Github><span>Github</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">NOTES: Memory of Forgotten</h1><div class=post-description>From registers to cloud-scale storage: deep notes on memory, latency, hierarchy, and systems research.</div><div class=post-meta><span title='2026-01-21 23:20:02 -0800 PST'>January 21, 2026</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1562 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/t-avil/blog/tree/main/content/posts/memory-of-forgotten.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-registers-and-the-first-illusion-flat-memory>1. Registers and the First Illusion: Flat Memory</a></li><li><a href=#2-caches-sram-separate-instructiondata-paths-and-coherence>2. Caches: SRAM, Separate Instruction/Data Paths, and Coherence</a></li><li><a href=#3-numa-latency-with-geography>3. NUMA: Latency with Geography</a></li><li><a href=#4-dram-the-silent-middle>4. DRAM: The Silent Middle</a></li><li><a href=#5-hard-disk-drives-the-tyranny-of-mechanics>5. Hard Disk Drives: The Tyranny of Mechanics</a></li><li><a href=#6-ssds-and-operational-asymmetry>6. SSDs and Operational Asymmetry</a></li><li><a href=#7-zoned-ssds-reclaiming-control>7. Zoned SSDs: Reclaiming Control</a></li><li><a href=#8-tiered-memory-pcm-and-nvm>8. Tiered Memory: PCM and NVM</a></li><li><a href=#9-hemem-software-tiered-memory-management>9. HeMem: Software Tiered Memory Management</a></li><li><a href=#10-cxl-and-disaggregated-memory>10. CXL and Disaggregated Memory</a></li><li><a href=#11-assise-distributed-file-systems-with-client-local-nvm>11. Assise: Distributed File Systems With Client-local NVM</a></li><li><a href=#12-from-memory-to-storage-a-conceptual-synthesis>12. From Memory to Storage: A Conceptual Synthesis</a></li><li><a href=#13-conclusion>13. Conclusion</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Love, <em>CPU Memory Access</em>, FreeBSD Project - <a href=https://people.freebsd.org/~lstewart/articles/cpumemory.pdf>https://people.freebsd.org/~lstewart/articles/cpumemory.pdf</a></p></blockquote><blockquote><p><em>HeMem: Scalable Tiered Memory Management for Big Data Applications and Real NVM</em>, SOSP 2021 - <a href=https://doi.org/10.1145/3477132.3483550>https://doi.org/10.1145/3477132.3483550</a> (<a href="https://cris.technion.ac.il/en/publications/hemem-scalable-tiered-memory-management-for-big-data-applications/?utm_source=chatgpt.com" title="HeMem: Scalable Tiered Memory Management for Big Data Applications and Real NVM - Technion - Israel Institute of Technology">Technion</a>)</p></blockquote><blockquote><p><em>Assise: Performance and Availability via Client-local NVM in a Distributed File System</em>, OSDI 2020 - <a href=https://www.usenix.org/system/files/osdi20-anderson.pdf>https://www.usenix.org/system/files/osdi20-anderson.pdf</a> (<a href="https://www.usenix.org/system/files/osdi20-anderson.pdf?utm_source=chatgpt.com" title="[PDF] Assise: Performance and Availability via Client-local NVM in a ...">USENIX</a>)</p></blockquote><blockquote><p><em>Don’t Be a Blockhead: Zoned Namespaces Make Work on Conventional SSDs Obsolete</em>, HotOS 2021 - <a href=https://doi.org/10.1145/3458336.3465300>https://doi.org/10.1145/3458336.3465300</a> (<a href="https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s07-stavrinos.pdf?utm_source=chatgpt.com" title="[PDF] Zoned Namespaces Make Work on Conventional SSDs Obsolete">ACM SIGOPS</a>)</p></blockquote><hr><p>Memory systems in computing are a <strong>study of distance and control</strong> more than bits. Every level-from CPU registers to the cluster fabric-exists because <strong>cost (latency, energy) rises with distance and abstraction</strong>. What follows is a deep, chronological and conceptual journey from the smallest unit of state to systems that span racks, guided by both physics and software research.</p><hr><h2 id=1-registers-and-the-first-illusion-flat-memory>1. Registers and the First Illusion: Flat Memory<a hidden class=anchor aria-hidden=true href=#1-registers-and-the-first-illusion-flat-memory>#</a></h2><p>At the very core of a CPU pipeline lie <strong>registers</strong>-architectural state local to the execution units. These are not “memory” in the conventional sense; they are <strong>bindings for values in-flight</strong>, allocated by the instruction scheduler and renamed to avoid false dependencies. Access to registers happens in <strong>a fraction of a nanosecond</strong> (≈0.25 ns, roughly one cycle on modern CPUs), and there is no latency to hide: data is present or it is not.</p><p>Registers are ephemeral. Once another instruction needs a slot, values spill to cache. This spill is the first time memory begins to matter.</p><hr><h2 id=2-caches-sram-separate-instructiondata-paths-and-coherence>2. Caches: SRAM, Separate Instruction/Data Paths, and Coherence<a hidden class=anchor aria-hidden=true href=#2-caches-sram-separate-instructiondata-paths-and-coherence>#</a></h2><p>Modern microarchitectures layer <strong>caches</strong> between registers and main memory, constructed from <strong>Static RAM (SRAM)</strong>. Unlike DRAM, SRAM doesn’t need refresh, making it fast but expensive and lower density.</p><p>The hierarchy is subtle:</p><ul><li><p><strong>L1 Instruction (I-cache)</strong> and <strong>L1 Data (D-cache)</strong> are <em>segregated</em>. They serve different domains: fetched instructions and data operands. This separation is why self-modifying code and instruction fetch pipelines require explicit synchronization; they operate in <strong>different physical units</strong>. Love’s analysis shows that these distinctions carry performance implications that software rarely sees but hardware formalizes. (<a href="https://paper.lingyunyang.com/reading-notes/conference/sosp-2021/hemem?utm_source=chatgpt.com" title="HeMem: Scalable Tiered Memory Management for Big Data Applications and Real NVM | Awesome Papers">Awesome Papers</a>)</p></li><li><p><strong>L2 cache</strong> often consolidates data and instructions for each core, trading size for latency.</p></li><li><p><strong>L3 cache</strong> is a shared, inclusive or non-inclusive substrate that enforces coherence across cores.</p></li></ul><p>Coherence protocols (MESI/MOESI) are distributed state machines: a load miss in L1 can trigger remote probes, turning what looked like a microsecond-scale access into a <strong>micro-protocol across cores</strong>. The FreeBSD CPU memory paper emphasizes that <strong>cache misses are topology events</strong>, not simple timing numbers. (<a href="https://paper.lingyunyang.com/reading-notes/conference/sosp-2021/hemem?utm_source=chatgpt.com" title="HeMem: Scalable Tiered Memory Management for Big Data Applications and Real NVM | Awesome Papers">Awesome Papers</a>)</p><hr><h2 id=3-numa-latency-with-geography>3. NUMA: Latency with Geography<a hidden class=anchor aria-hidden=true href=#3-numa-latency-with-geography>#</a></h2><p>In multisocket servers, memory has <strong>an accent</strong>. Each socket has its own memory controller and channels. An access to local DRAM may be ≈100 ns, but going across a socket can double that, and fetching a cache line from another core on a remote socket can take hundreds of nanoseconds.</p><p>NUMA (Non-Uniform Memory Access) emerges not as an optimization but as an architectural necessity: we physically cannot centralize all memory without paying severe latency and bandwidth penalties. To manage this, OSes and runtime systems must be <strong>NUMA-aware</strong>-pinning pages and threads to local memory, migrating hot pages, and orchestrating data locality. Failure to do so makes “shared memory” feel like remote I/O.</p><hr><h2 id=4-dram-the-silent-middle>4. DRAM: The Silent Middle<a hidden class=anchor aria-hidden=true href=#4-dram-the-silent-middle>#</a></h2><p>Dynamic RAM (DRAM) is the workhorse of main memory. It stores bits in capacitive cells that leak charge and need periodic refresh. DRAM’s latency (~80-120 ns) comes from <em>row activation, precharge, and sense amplification</em>: operations invisible to software but fundamental to performance.</p><p>Crucially, DRAM is <strong>volatile</strong>. It forgets everything the moment power is removed. This fact influenced decades of OS design: everything important must be safely stored on stable media and brought into DRAM for processing.</p><hr><h2 id=5-hard-disk-drives-the-tyranny-of-mechanics>5. Hard Disk Drives: The Tyranny of Mechanics<a hidden class=anchor aria-hidden=true href=#5-hard-disk-drives-the-tyranny-of-mechanics>#</a></h2><p>Hard Disk Drives (HDDs) predate all of the above. Their performance is dictated by <strong>mechanical physics</strong>:</p><ol><li><strong>Seek time</strong> to position the head over the track (≈5-10 ms).</li><li><strong>Rotational latency</strong> waiting for the platter to spin (≈8.5 ms at 7200 RPM).</li><li><strong>Transfer time</strong> once the head is on target.</li></ol><p>These orders-of-magnitude latencies made <strong>sequential access the holy grail</strong>: once the head was placed, the cost of reading many bytes was amortized. <code>fsck</code>, FFS, NTFS, ext*, and their spatial allocation policies arise from this tyranny: co-locate related blocks to exploit hardware characteristics.</p><p><strong>Shingled Magnetic Recording (SMR)</strong> took this further by overlapping tracks to increase density. SMR’s tracks behave like <strong>zones</strong>; writing to one track overwrites neighbors, forcing <strong>sequential, host-managed writes</strong>. It reshapes the classical block model and foreshadows zoned SSDs. (<a href="https://en.wikipedia.org/wiki/Shingled_magnetic_recording?utm_source=chatgpt.com" title="Shingled magnetic recording">Wikipedia</a>)</p><hr><h2 id=6-ssds-and-operational-asymmetry>6. SSDs and Operational Asymmetry<a hidden class=anchor aria-hidden=true href=#6-ssds-and-operational-asymmetry>#</a></h2><p>SSD’s flash memory removed moving parts but introduced a new constraint: <strong>erase-before-write</strong>. Flash is composed of:</p><ul><li>Small <strong>read/write pages</strong> (≈4 KB)</li><li>Large <strong>erase blocks</strong> (≈1-8 MB)</li><li>Write operations that <strong>must be to empty pages</strong></li><li>Erase operations that clear whole blocks</li></ul><p>The result is <strong>operational asymmetry</strong>: reads and writes are cheap, erases are expensive and erode lifetime. To present a conventional block interface, SSDs employ a <strong>Flash Translation Layer (FTL)</strong>, which dynamically remaps logical blocks to physical pages and hides physical realities.</p><p>The problem is <strong>Garbage Collection (GC)</strong>: when the drive runs out of clean blocks, the FTL must:</p><ul><li>Identify a victim erase block</li><li>Copy its live pages elsewhere</li><li>Erase it before reuse</li></ul><p>This <strong>hidden GC work</strong> causes unpredictable tail latency-a nemesis of storage performance. The OS cannot see inside the FTL, so it can’t plan around these spikes.</p><hr><h2 id=7-zoned-ssds-reclaiming-control>7. Zoned SSDs: Reclaiming Control<a hidden class=anchor aria-hidden=true href=#7-zoned-ssds-reclaiming-control>#</a></h2><p>Stavrinos et al.’s <em>Don’t Be a Blockhead: Zoned Namespaces Make Work on Conventional SSDs Obsolete</em> argues for <strong>exposing flash characteristics to the host</strong> instead of hiding them behind a block layer. ZNS SSDs divide storage into zones that <strong>must be written sequentially and explicitly reset</strong> by the host, removing device-side GC and giving control to software. (<a href="https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s07-stavrinos.pdf?utm_source=chatgpt.com" title="[PDF] Zoned Namespaces Make Work on Conventional SSDs Obsolete">ACM SIGOPS</a>)</p><p>With ZNS:</p><ul><li>The host knows about physical zone boundaries.</li><li>Garbage collection becomes an explicit action.</li><li>Software can place hot and cold data to minimize erasure.</li><li>Write amplification can be reduced because the system can group writes by temperature.</li></ul><p>This is a <strong>major shift</strong>: from opaque devices that manage everything internally to devices that require <strong>host-managed placement</strong>, analogous to how SMR forced host-managed zones on HDDs.</p><hr><h2 id=8-tiered-memory-pcm-and-nvm>8. Tiered Memory: PCM and NVM<a hidden class=anchor aria-hidden=true href=#8-tiered-memory-pcm-and-nvm>#</a></h2><p>Emerging Non-Volatile Memory (NVM) technologies like <strong>Phase Change Memory (PCM)</strong> and Intel Optane DC blur the line between memory and storage. They are:</p><ul><li><strong>Byte-addressable</strong></li><li><strong>Persistent</strong></li><li>Slower than DRAM (≈175-200 ns) but faster than SSDs</li><li>Higher density and cheaper per bit</li></ul><p>This disrupts the long-standing assumption that memory must be <strong>volatile and homogeneous</strong>. NVM invites us to rethink memory hierarchies: perhaps memory is not a flat space but a <strong>tiered continuum</strong> from registers to persistent storage.</p><hr><h2 id=9-hemem-software-tiered-memory-management>9. HeMem: Software Tiered Memory Management<a hidden class=anchor aria-hidden=true href=#9-hemem-software-tiered-memory-management>#</a></h2><p><em>HeMem: Scalable Tiered Memory Management for Big Data Applications and Real NVM</em> tackles this directly. The paper identifies that conventional hardware and OS approaches to tiered memory (e.g., Intel Optane DRAM cache or naive OS-level NUMA-like placement) fail when real NVM is part of the hierarchy. (<a href="https://cris.technion.ac.il/en/publications/hemem-scalable-tiered-memory-management-for-big-data-applications/?utm_source=chatgpt.com" title="HeMem: Scalable Tiered Memory Management for Big Data Applications and Real NVM - Technion - Israel Institute of Technology">Technion</a>)</p><p>Instead of piggybacking on page tables, page faults, or simplistic heuristics, HeMem:</p><ul><li>Uses <strong>CPU performance counters</strong> to sample memory access patterns asynchronously.</li><li>Offloads <strong>page migration with DMA</strong>, which avoids CPU intervention and cache pollution.</li><li>Accounts for <strong>asymmetric bandwidth</strong> between DRAM and NVM.</li><li>Places <strong>policy in user space</strong>, letting applications customize heat-aware placement.</li></ul><p>HeMem’s insights are deep: <strong>hotness is a signal, not a static stat</strong>, and migration costs must be amortized and overlapped with application execution. The result is improved runtime and reduced tail latency for big data workloads on real tiered hardware.</p><hr><h2 id=10-cxl-and-disaggregated-memory>10. CXL and Disaggregated Memory<a hidden class=anchor aria-hidden=true href=#10-cxl-and-disaggregated-memory>#</a></h2><p>Compute Express Link (CXL) decouples memory from the CPU socket, ushering in <strong>memory pooling and disaggregation</strong>. CXL memory can be coherent across devices and processors yet sits off the traditional DRAM bus. This enables:</p><ul><li>Elastic memory expansion</li><li>Shared memory pools</li><li>Coherent access over an interconnect</li></ul><p>However, latency costs rise: CXL-attached memory is slower than slot-attached DIMMs but faster than networked block storage-creating <strong>another tier</strong> in the latency hierarchy.</p><p>Managing this continuum demands systems like HeMem or more advanced OS policies that treat memory as <strong>elastic fabric</strong>, not static banks.</p><hr><h2 id=11-assise-distributed-file-systems-with-client-local-nvm>11. Assise: Distributed File Systems With Client-local NVM<a hidden class=anchor aria-hidden=true href=#11-assise-distributed-file-systems-with-client-local-nvm>#</a></h2><p><em>Assise: Performance and Availability via Client-local NVM in a Distributed File System</em> applies tiered memory thinking to distributed storage. Instead of a central server managing storage IO, Assise uses <strong>client-local persistent memory</strong> to cache and serve file system state directly. It builds the first persistent, replicated cache coherence layer (CC-NVM) that provides:</p><ul><li><strong>Crash consistency</strong> via write-logging</li><li><strong>Linearizability</strong> across nodes</li><li><strong>Low tail latency</strong> by maximizing local access</li><li><strong>Fast failover</strong> using hot replicas (<a href="https://www.usenix.org/system/files/osdi20-anderson.pdf?utm_source=chatgpt.com" title="[PDF] Assise: Performance and Availability via Client-local NVM in a ...">USENIX</a>)</li></ul><p>Assise shows that <strong>distributed memory and storage should not be separate</strong>: locality and consistency are cheaper to maintain if we treat memory as a first-class, persistent tier at every node.</p><hr><h2 id=12-from-memory-to-storage-a-conceptual-synthesis>12. From Memory to Storage: A Conceptual Synthesis<a hidden class=anchor aria-hidden=true href=#12-from-memory-to-storage-a-conceptual-synthesis>#</a></h2><p>The evolution of data management is a <strong>move from static geometry to dynamic placement</strong>:</p><ul><li>HDDs taught us <strong>physical locality matters</strong></li><li>SSDs taught us <strong>operational asymmetry dictates latency</strong></li><li>ZNS SSDs and PCM/NVM taught us <strong>host-managed placement is inevitable</strong></li><li>CXL taught us <strong>memory is fabric</strong></li><li>HeMem and Assise taught us <strong>software must manage tiered heat and coherence</strong></li></ul><p>Traditional abstractions (flat memory, block interfaces, opaque devices) worked when hardware was slow and simple. As hardware becomes more complex-with multiple performance surfaces and asymmetries-<strong>software must regain control</strong>.</p><hr><h2 id=13-conclusion>13. Conclusion<a hidden class=anchor aria-hidden=true href=#13-conclusion>#</a></h2><p>Memory is not a flat space. Every layer imposes cost, and every abstraction hides complexity. To build predictable systems, we must make the invisible <strong>visible to software</strong>-the hidden GC, coherence traffic, remote latencies, and tier boundaries-and manage them with informed policies, not hopes.</p><hr></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/notes/>Notes</a></li><li><a href=http://localhost:1313/tags/memory/>Memory</a></li><li><a href=http://localhost:1313/tags/architecture/>Architecture</a></li><li><a href=http://localhost:1313/tags/numa/>Numa</a></li><li><a href=http://localhost:1313/tags/storage/>Storage</a></li><li><a href=http://localhost:1313/tags/systems/>Systems</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/model-training-partitioning/><span class=title>Next »</span><br><span>NOTES: training the distributed scale?</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>T|A</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>