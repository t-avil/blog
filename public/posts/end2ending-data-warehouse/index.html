<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NOTES: End to ending Data Warehouses as a concept | T|A</title><meta name=keywords content="notes,data-engineering,CDC,data-warehouse,streaming,batch,dbt,system-design"><meta name=description content="A slightly practical walkthrough of designing a CDC-powered data pipeline: ingestor, bronze lake, silver warehouse, and dbt-powered golden marts with batch DAGs, streaming joins, state management, and chunked reducers."><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.3f7ba6a00d316a1658af1e52b60f5592bfd3f63e1683217d447958625c9fec2a.css integrity="sha256-P3umoA0xahZYrx5Stg9Vkr/T9j4WgyF9RHlYYlyf7Co=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/end2ending-data-warehouse/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/end2ending-data-warehouse/"><meta property="og:site_name" content="T|A"><meta property="og:title" content="NOTES: End to ending Data Warehouses as a concept"><meta property="og:description" content="A slightly practical walkthrough of designing a CDC-powered data pipeline: ingestor, bronze lake, silver warehouse, and dbt-powered golden marts with batch DAGs, streaming joins, state management, and chunked reducers."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-01T23:23:37-07:00"><meta property="article:modified_time" content="2024-10-01T23:23:37-07:00"><meta property="article:tag" content="Notes"><meta property="article:tag" content="Data-Engineering"><meta property="article:tag" content="CDC"><meta property="article:tag" content="Data-Warehouse"><meta property="article:tag" content="Streaming"><meta property="article:tag" content="Batch"><meta property="og:image" content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:title content="NOTES: End to ending Data Warehouses as a concept"><meta name=twitter:description content="A slightly practical walkthrough of designing a CDC-powered data pipeline: ingestor, bronze lake, silver warehouse, and dbt-powered golden marts with batch DAGs, streaming joins, state management, and chunked reducers."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"NOTES: End to ending Data Warehouses as a concept","item":"http://localhost:1313/posts/end2ending-data-warehouse/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NOTES: End to ending Data Warehouses as a concept","name":"NOTES: End to ending Data Warehouses as a concept","description":"A slightly practical walkthrough of designing a CDC-powered data pipeline: ingestor, bronze lake, silver warehouse, and dbt-powered golden marts with batch DAGs, streaming joins, state management, and chunked reducers.","keywords":["notes","data-engineering","CDC","data-warehouse","streaming","batch","dbt","system-design"],"articleBody":" If you care about analytics at scale, the day will come when you need to build a data pipeline that remembers everything and still gives you answers fast. This post walks the whole path: ingesting CDC, landing in a bronze data lake, cleaning \u0026 denormalizing into a silver warehouse, then producing golden data marts with dbt. I’ll show the nitty-gritty: exactly how CDC needs to be treated, how a control plane composes map-shuffle-reduce DAGs for SQL, how to make reducers scale (chunking and streaming), and how streaming joins keep marts fresh without recomputing the world.\nArchitecture overview - the five parts Ingestor (CDC) - capture change events (inserts/updates/deletes) reliably and in order. Bronze tier (data lake) - raw immutable events, append-only, schema + metadata (offsets, tx id). Persistent volumes with replication (RF=3). Silver tier (warehouse) - cleaned, denormalized, query-friendly state (often columnar tables). Golden tier (data marts / dbt) - curated models, tests, docs; materialized incrementally or full-refresh via dbt. Control plane - tracks chunks, metadata, computes DAGs, schedules map/shuffle/reduce tasks, reconciliation/lineage. CDC: not just “row snapshots” - treat it like transactional logs CDC comes in flavors. Two important distinctions:\nState-capture snapshots: occasional full table snapshots. Transaction log (logical decoding): ordered events exactly as they happened - this is what you want for correctness. Why the difference matters: if your CDC is transactional (Postgres WAL, MySQL binlog), events arrive as ordered, atomic units (tx boundaries, commit/abort). If you treat them as out-of-order or as naive key-value overwrites, you can compute wrong aggregates, miss tombstones (deletes), or break foreign-key replays.\nPractical CDC best practices:\nUse tools like Debezium / logical decoding to emit change events including: table, op (c/u/d), before, after, tx_id, lsn/offset, timestamp. Include commit markers or transaction IDs so you can apply events in commit order when reconstructing state. Emit tombstones for deletes to allow downstream compaction/merge logic to drop rows. Keep schema evolution under a registry (Avro/Protobuf/JSON Schema) and version events - never assume the payload shape is stable. Example CDC event (pseudo-JSON):\n{ \"tx_id\": \"0xabc123\", \"lsn\": 123456, \"op\": \"U\", \"table\": \"orders\", \"before\": { \"status\": \"pending\" }, \"after\": { \"status\": \"paid\" }, \"ts\": \"2025-08-25T09:00:00Z\", \"schema_version\": 4 } Bronze: raw immutable events \u0026 durable storage Bronze is the single source of truth. Requirements:\nAppend-only storage (PV with RF=3, or S3 for object stores). RF=3 gives resilience: 2 node failures tolerated. Each event writes with metadata: topic/partition, offset, tx_id, arrival_ts, schema_version. Compaction policy: keep raw events for N days/years, but also allow compacted “state snapshots” for faster rehydration. Implementation tips:\nPut CDC into a message broker (Kafka) or append-store. Consumers (batch/stream) read from durable offsets. Persist raw event files in a consistent layout (e.g., /bronze/{table}/{yyyy}/{mm}/{dd}/part-{shard}.avro) with checksums for integrity. Control plane: SQL → DAG → tasks When a SQL query arrives (ad-hoc report or scheduled job), the control plane must:\nParse SQL into a logical plan (scan, filter, join, aggregate). Rewrite into a physical plan: break into map/shuffle/reduce stages optimized for partitioning keys and data locality. Emit tasks: map tasks that read chunks/partitions, shuffle to reducers by key, and reduce tasks that do joins/aggregations. Track lineage \u0026 retries: store per-task metadata, checkpointing tokens, and ability to restart failed tasks deterministically. Very high-level pseudocode for DAG generation:\nlogical = parse(sql) physical = plan(logical) // choose join orders, hash vs sort merge stages = partition_into_stages(physical) for stage in stages: tasks = create_tasks(stage, based_on_chunks) schedule(tasks) Map → Shuffle → Reduce: details \u0026 guarantees Map: read bronze/silver chunk files, extract per-record keys, emit (key, payload) to shuffler. Map workers must implement:\nRetries \u0026 idempotency (process-by-offset or use idempotent output paths). Checkpoints (on success write completion marker). Backpressure for overload. Shuffle: network or disk transfer that groups records by key. Make sure partitioning function is stable (hash(key) % R). Shuffler should support spilling to disk when the in-memory buffer exceeds limits.\nReduce: receives all records for a partition (key range). Reducer workloads:\nJoin multiple tables’ records on the key. Preferred algorithms:\nHash join: build hash table for smaller relation, probe with larger-a good default. Sort-merge: for large sorted inputs; better when inputs are pre-sorted or when memory is limited. Block nested-loop with chunking: when one side is too large; stream chunks and iterate. Chunking strategy: if incoming partition is too big to fit memory, load the first chunk of table A into memory, stream chunks from table B and iterate; when exhausted, evict and next chunk from A. This is essentially external join with bounded memory.\nChunked reducer pseudo:\nfor chunkA in stream_chunks(tableA): build_hash(chunkA) for chunkB in stream_chunks(tableB): for r in chunkB: if hash_lookup(r.key): emit(joined_row) free(chunkA) Streaming vs Batch: when to pick what Batch (map-shuffle-reduce) is great for full recompute, complex multi-way joins, and backfills. Streaming is best for incremental updates, near-real-time dashboards, and when you want to avoid full recomputes.\nStreaming patterns to implement:\nIncremental aggregations: maintain keyed state (counts, sums) and update per event. Stateful joins: keep one stream as keyed state (persisted in a state backend like RocksDB); incoming events from the other stream are joined against that state - this produces incremental joins and avoids recomputing past windows. n−1 state: when joining multiple tables (n inputs), streaming engines must maintain persisted state for n−1 tables and apply incoming updates from the nth. Important streaming considerations:\nState backend \u0026 TTL: RocksDB (Flink) or managed state, with TTL/compaction to bound storage. Watermarks \u0026 late events: define watermarks to bound lateness; implement logic for retractions if late updates arrive. Exactly-once: use checkpointing + two-phase commit sinks where possible to get end-to-end exactly-once semantics (Flink, Kafka transactions). If you can’t, make consumers idempotent. Example streaming join behavior (simplified):\nStream A (left) persisted keyed state stateA[key]. New event from Stream B with key k arrives: lookup stateA[k], emit joined records for current stateA. This is incremental - it does not recompute when stateA later expands (unless you implement replays/retractions). Silver: cleaned, denormalized, query-optimized Silver is where messy events become usable rows:\nMaterialized snapshots or tables with denormalized joins (user + latest_address + order_summary). Columnar formats (Parquet/ORC) with partitioning (date, customer_id % N) for fast scans. Schema evolution: reconcile schema versions from bronze; fold fields with default/null rules. Design for efficient incremental runs:\nUse upserts by primary key (using MERGE) when applying cleaned events. Maintain change tracking so dbt or ETL jobs can run incrementals by modified ranges. Golden: dbt, tests, and analytics engineering dbt sits on top:\nBuilds models (SQL), materializes tables/views, supports incremental models, docs, and tests. dbt DAG maps dependencies - upstream silver tables trigger downstream recalculations. Use dbt for tests (unique, not_null, relationships) to catch data drift early. CI/CD: run dbt tests on PRs; schedule nightly full-refreshes where necessary. Operational notes \u0026 real-world gotchas Monitoring: instrument the pipeline. Track lag, task failure rates, shuffle spill rates, state sizes, and restore time. Backfills: design a deterministic replay mechanism from bronze with tx_id ordering. Schema drift: enforce contracts via a schema registry and pre-deploy migrations. Cost \u0026 storage: RF=3 persistent volumes are expensive; consider object storage with periodic state snapshots if you can tolerate eventual consistency. Testing: use synthetic workloads to test worst-case partition skew and reducer memory/IO patterns. Final, practical checklist (TL;DR) Capture CDC with tx boundaries; keep tombstones and schema versioning. Store raw events in bronze (RF=3) with consistent layouts. Build a control plane to generate map-shuffle-reduce DAGs and track chunk metadata. Implement map retries, checkpoints, and idempotency. Make shuffler/partitioning stable; spill to disk if needed. Reducers: prefer hash or sort-merge; use chunked external joins when memory limits hit. For streaming: use keyed state, watermarks, TTLs, and checkpointing for correctness. dbt for golden: tests, DAGs, docs, incremental models. Automate backfills \u0026 verify restore regularly. now you know how motherduck.com ’s of this world are build! ","wordCount":"1282","inLanguage":"en","image":"http://localhost:1313/images/papermod-cover.png","datePublished":"2024-10-01T23:23:37-07:00","dateModified":"2024-10-01T23:23:37-07:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/end2ending-data-warehouse/"},"publisher":{"@type":"Organization","name":"T|A","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="T|A (Alt + H)">T|A</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about title=About><span>About</span></a></li><li><a href=http://localhost:1313/chronicles title=Chronicles><span>Chronicles</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://github.com/t-avil title=Github><span>Github</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">NOTES: End to ending Data Warehouses as a concept</h1><div class=post-description>A slightly practical walkthrough of designing a CDC-powered data pipeline: ingestor, bronze lake, silver warehouse, and dbt-powered golden marts with batch DAGs, streaming joins, state management, and chunked reducers.</div><div class=post-meta><span title='2024-10-01 23:23:37 -0700 PDT'>October 1, 2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1282 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/t-avil/blog/tree/main/content/posts/end2ending-data-warehouse.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#architecture-overview---the-five-parts>Architecture overview - the five parts</a></li><li><a href=#cdc-not-just-row-snapshots---treat-it-like-transactional-logs>CDC: not just &ldquo;row snapshots&rdquo; - treat it like transactional logs</a></li><li><a href=#bronze-raw-immutable-events--durable-storage>Bronze: raw immutable events & durable storage</a></li><li><a href=#control-plane-sql--dag--tasks>Control plane: SQL → DAG → tasks</a></li><li><a href=#map--shuffle--reduce-details--guarantees>Map → Shuffle → Reduce: details & guarantees</a></li><li><a href=#streaming-vs-batch-when-to-pick-what>Streaming vs Batch: when to pick what</a></li><li><a href=#silver-cleaned-denormalized-query-optimized>Silver: cleaned, denormalized, query-optimized</a></li><li><a href=#golden-dbt-tests-and-analytics-engineering>Golden: dbt, tests, and analytics engineering</a></li><li><a href=#operational-notes--real-world-gotchas>Operational notes & real-world gotchas</a></li><li><a href=#final-practical-checklist-tldr>Final, practical checklist (TL;DR)</a><ul><li></li></ul></li></ul></nav></div></details></div><div class=post-content><hr><p>If you care about analytics at scale, the day will come when you need to build a data pipeline that <em>remembers</em> everything and still gives you answers fast. This post walks the whole path: ingesting CDC, landing in a bronze data lake, cleaning & denormalizing into a silver warehouse, then producing golden data marts with dbt. I’ll show the nitty-gritty: exactly how CDC needs to be treated, how a control plane composes map-shuffle-reduce DAGs for SQL, how to make reducers scale (chunking and streaming), and how streaming joins keep marts fresh without recomputing the world.</p><hr><h2 id=architecture-overview---the-five-parts>Architecture overview - the five parts<a hidden class=anchor aria-hidden=true href=#architecture-overview---the-five-parts>#</a></h2><ol><li><strong>Ingestor (CDC)</strong> - capture change events (inserts/updates/deletes) reliably and in order.</li><li><strong>Bronze tier (data lake)</strong> - raw immutable events, append-only, schema + metadata (offsets, tx id). Persistent volumes with replication (RF=3).</li><li><strong>Silver tier (warehouse)</strong> - cleaned, denormalized, query-friendly state (often columnar tables).</li><li><strong>Golden tier (data marts / dbt)</strong> - curated models, tests, docs; materialized incrementally or full-refresh via dbt.</li><li><strong>Control plane</strong> - tracks chunks, metadata, computes DAGs, schedules map/shuffle/reduce tasks, reconciliation/lineage.</li></ol><hr><h2 id=cdc-not-just-row-snapshots---treat-it-like-transactional-logs>CDC: not just &ldquo;row snapshots&rdquo; - treat it like transactional logs<a hidden class=anchor aria-hidden=true href=#cdc-not-just-row-snapshots---treat-it-like-transactional-logs>#</a></h2><p>CDC comes in flavors. Two important distinctions:</p><ul><li><strong>State-capture snapshots</strong>: occasional full table snapshots.</li><li><strong>Transaction log (logical decoding)</strong>: ordered events exactly as they happened - <em>this is what you want for correctness</em>.</li></ul><p>Why the difference matters: if your CDC is transactional (Postgres WAL, MySQL binlog), events arrive as ordered, atomic units (tx boundaries, commit/abort). If you treat them as out-of-order or as naive key-value overwrites, you can compute wrong aggregates, miss tombstones (deletes), or break foreign-key replays.</p><p>Practical CDC best practices:</p><ul><li>Use tools like <strong>Debezium</strong> / logical decoding to emit change events including: <code>table</code>, <code>op</code> (c/u/d), <code>before</code>, <code>after</code>, <code>tx_id</code>, <code>lsn</code>/<code>offset</code>, <code>timestamp</code>.</li><li>Include <strong>commit markers</strong> or transaction IDs so you can apply events in commit order when reconstructing state.</li><li>Emit <strong>tombstones</strong> for deletes to allow downstream compaction/merge logic to drop rows.</li><li>Keep schema evolution under a registry (Avro/Protobuf/JSON Schema) and version events - never assume the payload shape is stable.</li></ul><p>Example CDC event (pseudo-JSON):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;tx_id&#34;</span><span class=p>:</span> <span class=s2>&#34;0xabc123&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;lsn&#34;</span><span class=p>:</span> <span class=mi>123456</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;op&#34;</span><span class=p>:</span> <span class=s2>&#34;U&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;table&#34;</span><span class=p>:</span> <span class=s2>&#34;orders&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;before&#34;</span><span class=p>:</span> <span class=p>{</span> <span class=nt>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;pending&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;after&#34;</span><span class=p>:</span>  <span class=p>{</span> <span class=nt>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;paid&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;ts&#34;</span><span class=p>:</span> <span class=s2>&#34;2025-08-25T09:00:00Z&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;schema_version&#34;</span><span class=p>:</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><hr><h2 id=bronze-raw-immutable-events--durable-storage>Bronze: raw immutable events & durable storage<a hidden class=anchor aria-hidden=true href=#bronze-raw-immutable-events--durable-storage>#</a></h2><p>Bronze is the single source of truth. Requirements:</p><ul><li><strong>Append-only</strong> storage (PV with RF=3, or S3 for object stores). RF=3 gives resilience: 2 node failures tolerated.</li><li>Each event writes with metadata: topic/partition, offset, tx_id, arrival_ts, schema_version.</li><li><strong>Compaction policy</strong>: keep raw events for N days/years, but also allow compacted &ldquo;state snapshots&rdquo; for faster rehydration.</li></ul><p>Implementation tips:</p><ul><li>Put CDC into a message broker (Kafka) or append-store. Consumers (batch/stream) read from durable offsets.</li><li>Persist raw event files in a consistent layout (e.g., <code>/bronze/{table}/{yyyy}/{mm}/{dd}/part-{shard}.avro</code>) with checksums for integrity.</li></ul><hr><h2 id=control-plane-sql--dag--tasks>Control plane: SQL → DAG → tasks<a hidden class=anchor aria-hidden=true href=#control-plane-sql--dag--tasks>#</a></h2><p>When a SQL query arrives (ad-hoc report or scheduled job), the control plane must:</p><ol><li><strong>Parse SQL</strong> into a logical plan (scan, filter, join, aggregate).</li><li><strong>Rewrite</strong> into a physical plan: break into map/shuffle/reduce stages optimized for partitioning keys and data locality.</li><li><strong>Emit tasks</strong>: map tasks that read chunks/partitions, shuffle to reducers by key, and reduce tasks that do joins/aggregations.</li><li><strong>Track lineage & retries</strong>: store per-task metadata, checkpointing tokens, and ability to restart failed tasks deterministically.</li></ol><p>Very high-level pseudocode for DAG generation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>logical = parse(sql)
</span></span><span class=line><span class=cl>physical = plan(logical)   // choose join orders, hash vs sort merge
</span></span><span class=line><span class=cl>stages = partition_into_stages(physical)
</span></span><span class=line><span class=cl>for stage in stages:
</span></span><span class=line><span class=cl>  tasks = create_tasks(stage, based_on_chunks)
</span></span><span class=line><span class=cl>  schedule(tasks)
</span></span></code></pre></div><hr><h2 id=map--shuffle--reduce-details--guarantees>Map → Shuffle → Reduce: details & guarantees<a hidden class=anchor aria-hidden=true href=#map--shuffle--reduce-details--guarantees>#</a></h2><p><strong>Map</strong>: read bronze/silver chunk files, extract per-record keys, emit <code>(key, payload)</code> to shuffler. Map workers must implement:</p><ul><li><strong>Retries</strong> & idempotency (process-by-offset or use idempotent output paths).</li><li><strong>Checkpoints</strong> (on success write completion marker).</li><li><strong>Backpressure</strong> for overload.</li></ul><p><strong>Shuffle</strong>: network or disk transfer that groups records by key. Make sure partitioning function is stable (hash(key) % R). Shuffler should support spilling to disk when the in-memory buffer exceeds limits.</p><p><strong>Reduce</strong>: receives all records for a partition (key range). Reducer workloads:</p><ul><li><p><strong>Join</strong> multiple tables’ records on the key. Preferred algorithms:</p><ul><li><strong>Hash join</strong>: build hash table for smaller relation, probe with larger-a good default.</li><li><strong>Sort-merge</strong>: for large sorted inputs; better when inputs are pre-sorted or when memory is limited.</li><li><strong>Block nested-loop with chunking</strong>: when one side is too large; stream chunks and iterate.</li></ul></li><li><p><strong>Chunking strategy</strong>: if incoming partition is too big to fit memory, load the first chunk of table A into memory, stream chunks from table B and iterate; when exhausted, evict and next chunk from A. This is essentially <em>external</em> join with bounded memory.</p></li></ul><p>Chunked reducer pseudo:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>for chunkA in stream_chunks(tableA):
</span></span><span class=line><span class=cl>  build_hash(chunkA)
</span></span><span class=line><span class=cl>  for chunkB in stream_chunks(tableB):
</span></span><span class=line><span class=cl>    for r in chunkB:
</span></span><span class=line><span class=cl>      if hash_lookup(r.key):
</span></span><span class=line><span class=cl>         emit(joined_row)
</span></span><span class=line><span class=cl>  free(chunkA)
</span></span></code></pre></div><hr><h2 id=streaming-vs-batch-when-to-pick-what>Streaming vs Batch: when to pick what<a hidden class=anchor aria-hidden=true href=#streaming-vs-batch-when-to-pick-what>#</a></h2><p><strong>Batch</strong> (map-shuffle-reduce) is great for full recompute, complex multi-way joins, and backfills. <strong>Streaming</strong> is best for incremental updates, near-real-time dashboards, and when you want to avoid full recomputes.</p><p>Streaming patterns to implement:</p><ul><li><strong>Incremental aggregations</strong>: maintain keyed state (counts, sums) and update per event.</li><li><strong>Stateful joins</strong>: keep one stream as keyed state (persisted in a state backend like RocksDB); incoming events from the other stream are joined against that state - this produces incremental joins and avoids recomputing past windows.</li><li><strong>n−1 state</strong>: when joining multiple tables (n inputs), streaming engines must maintain persisted state for n−1 tables and apply incoming updates from the nth.</li></ul><p>Important streaming considerations:</p><ul><li><strong>State backend & TTL</strong>: RocksDB (Flink) or managed state, with TTL/compaction to bound storage.</li><li><strong>Watermarks & late events</strong>: define watermarks to bound lateness; implement logic for retractions if late updates arrive.</li><li><strong>Exactly-once</strong>: use checkpointing + two-phase commit sinks where possible to get end-to-end exactly-once semantics (Flink, Kafka transactions). If you can&rsquo;t, make consumers idempotent.</li></ul><p>Example streaming join behavior (simplified):</p><ol><li>Stream A (left) persisted keyed state <code>stateA[key]</code>.</li><li>New event from Stream B with key <code>k</code> arrives: lookup <code>stateA[k]</code>, emit joined records for current stateA. This is <em>incremental</em> - it does not recompute when <code>stateA</code> later expands (unless you implement replays/retractions).</li></ol><hr><h2 id=silver-cleaned-denormalized-query-optimized>Silver: cleaned, denormalized, query-optimized<a hidden class=anchor aria-hidden=true href=#silver-cleaned-denormalized-query-optimized>#</a></h2><p>Silver is where messy events become usable rows:</p><ul><li><strong>Materialized snapshots</strong> or tables with denormalized joins (user + latest_address + order_summary).</li><li><strong>Columnar formats</strong> (Parquet/ORC) with partitioning (date, customer_id % N) for fast scans.</li><li><strong>Schema evolution</strong>: reconcile schema versions from bronze; fold fields with default/null rules.</li></ul><p>Design for efficient incremental runs:</p><ul><li>Use <strong>upserts</strong> by primary key (using MERGE) when applying cleaned events.</li><li>Maintain <strong>change tracking</strong> so dbt or ETL jobs can run incrementals by modified ranges.</li></ul><hr><h2 id=golden-dbt-tests-and-analytics-engineering>Golden: dbt, tests, and analytics engineering<a hidden class=anchor aria-hidden=true href=#golden-dbt-tests-and-analytics-engineering>#</a></h2><p>dbt sits on top:</p><ul><li>Builds models (SQL), materializes tables/views, supports incremental models, docs, and tests.</li><li>dbt DAG maps dependencies - upstream silver tables trigger downstream recalculations.</li><li>Use dbt for tests (<code>unique</code>, <code>not_null</code>, <code>relationships</code>) to catch data drift early.</li><li>CI/CD: run dbt tests on PRs; schedule nightly full-refreshes where necessary.</li></ul><hr><h2 id=operational-notes--real-world-gotchas>Operational notes & real-world gotchas<a hidden class=anchor aria-hidden=true href=#operational-notes--real-world-gotchas>#</a></h2><ul><li><strong>Monitoring</strong>: instrument the pipeline. Track lag, task failure rates, shuffle spill rates, state sizes, and restore time.</li><li><strong>Backfills</strong>: design a deterministic replay mechanism from bronze with tx_id ordering.</li><li><strong>Schema drift</strong>: enforce contracts via a schema registry and pre-deploy migrations.</li><li><strong>Cost & storage</strong>: RF=3 persistent volumes are expensive; consider object storage with periodic state snapshots if you can tolerate eventual consistency.</li><li><strong>Testing</strong>: use synthetic workloads to test worst-case partition skew and reducer memory/IO patterns.</li></ul><hr><h2 id=final-practical-checklist-tldr>Final, practical checklist (TL;DR)<a hidden class=anchor aria-hidden=true href=#final-practical-checklist-tldr>#</a></h2><ul><li>Capture <strong>CDC with tx boundaries</strong>; keep tombstones and schema versioning.</li><li>Store raw events in bronze (RF=3) with consistent layouts.</li><li>Build a <strong>control plane</strong> to generate map-shuffle-reduce DAGs and track chunk metadata.</li><li>Implement <strong>map retries, checkpoints, and idempotency</strong>.</li><li>Make shuffler/partitioning stable; spill to disk if needed.</li><li>Reducers: prefer hash or sort-merge; use chunked external joins when memory limits hit.</li><li>For streaming: use keyed state, watermarks, TTLs, and checkpointing for correctness.</li><li>dbt for golden: tests, DAGs, docs, incremental models.</li><li>Automate backfills & verify restore regularly.</li></ul><hr><h5 id=now-you-know-how-motherduckcom-s-of-this-world-are-build>now you know how motherduck.com &rsquo;s of this world are build!<a hidden class=anchor aria-hidden=true href=#now-you-know-how-motherduckcom-s-of-this-world-are-build>#</a></h5></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/notes/>Notes</a></li><li><a href=http://localhost:1313/tags/data-engineering/>Data-Engineering</a></li><li><a href=http://localhost:1313/tags/cdc/>CDC</a></li><li><a href=http://localhost:1313/tags/data-warehouse/>Data-Warehouse</a></li><li><a href=http://localhost:1313/tags/streaming/>Streaming</a></li><li><a href=http://localhost:1313/tags/batch/>Batch</a></li><li><a href=http://localhost:1313/tags/dbt/>Dbt</a></li><li><a href=http://localhost:1313/tags/system-design/>System-Design</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/lgtm-stack-or-die/><span class=title>« Prev</span><br><span>NOTES: Lgtm Stack or Die</span>
</a><a class=next href=http://localhost:1313/posts/tail-of-two-regulation-and-compliance/><span class=title>Next »</span><br><span>NOTES: Tail of Two Regulation and Compliance</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>T|A</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>