<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NOTES: GPU Architecture, TVM, and Triton | T|A</title><meta name=keywords content="notes,machine-learning,gpu,compilers,tvm,triton"><meta name=description content="My reading notes on GPU hardware, TVM compilation, and Triton JIT for efficient tensor computation."><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.3f7ba6a00d316a1658af1e52b60f5592bfd3f63e1683217d447958625c9fec2a.css integrity="sha256-P3umoA0xahZYrx5Stg9Vkr/T9j4WgyF9RHlYYlyf7Co=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/gpu-tvm-triton/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/gpu-tvm-triton/"><meta property="og:site_name" content="T|A"><meta property="og:title" content="NOTES: GPU Architecture, TVM, and Triton"><meta property="og:description" content="My reading notes on GPU hardware, TVM compilation, and Triton JIT for efficient tensor computation."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-06T14:00:00-07:00"><meta property="article:modified_time" content="2025-10-06T14:00:00-07:00"><meta property="article:tag" content="Notes"><meta property="article:tag" content="Machine-Learning"><meta property="article:tag" content="Gpu"><meta property="article:tag" content="Compilers"><meta property="article:tag" content="Tvm"><meta property="article:tag" content="Triton"><meta property="og:image" content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:title content="NOTES: GPU Architecture, TVM, and Triton"><meta name=twitter:description content="My reading notes on GPU hardware, TVM compilation, and Triton JIT for efficient tensor computation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"NOTES: GPU Architecture, TVM, and Triton","item":"http://localhost:1313/posts/gpu-tvm-triton/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NOTES: GPU Architecture, TVM, and Triton","name":"NOTES: GPU Architecture, TVM, and Triton","description":"My reading notes on GPU hardware, TVM compilation, and Triton JIT for efficient tensor computation.","keywords":["notes","machine-learning","gpu","compilers","tvm","triton"],"articleBody":" If TensorFlow showed us how to scale ML across entire data centers, and PyTorch 2.0 showed us how to compile dynamic Python code, GPUs and compiler stacks like TVM and Triton show us how to squeeze the absolute last drop of performance out of hardware. This post is my attempt to tie together GPU architecture, TVM’s tensor-level compilation, and Triton’s clever tiling JIT strategy, all in one nerdy, slightly dorky package.\nGPU Architecture: Warps, SMs, and Parallel Execution Hardware instructions on modern GPUs are parallelized into multiples of 32 threads. These chunks of 32 threads are called warps, but the instructions inside a warp can differ - though it’s more efficient if they’re all the same. Each worker, or Streaming Multiprocessor (SM), can execute about 4 warps simultaneously.\nTasks, also called blocks, are assigned to SMs by the GPU scheduler. The kernel function determines how many tasks to launch and how many instructions each contains. Let’s run an example to see this in action:\nSuppose we have 2 different kernels, each with 51 blocks, and 100 SMs. Most blocks (say 50 from each kernel) contain 8 warps. Each SM executes 4 warps per cycle, so these blocks take 2 cycles each. The last two blocks (7 and 9 warps) are queued, requiring Max(ceil(7/4), ceil(9/4)) = 3 cycles. Total execution: 5 cycles.\nWarp Instruction \u0026 Memory Table Type Example Warp-level execution Memory hierarchy / latency 1-cycle / fast add, sub, fadd, fmul, fma, bitwise ops (and, or, xor), compare (cmp) 1 cycle per warp Registers; fastest, no memory dependency Slow / multi-cycle idiv, fdiv, sqrt, rsqrt Multiple cycles per warp, pipelined Registers; some compute-bound Global memory ld.global, st.global High latency (~300-600 cycles) per warp; scheduler switches warps DRAM Shared memory ld.shared, st.shared Low latency (~1-10 cycles) per warp; can bank conflict SM-local shared memory Texture / special units ld.texture, atomic Depends on unit; pipelined Read-only caches / special memory Matrix multiply / tensor ops wmma, dp4a, fma 1 cycle throughput per warp on tensor cores Registers or shared memory for operands Key notes:\nGlobal memory: shared across kernels; multiple kernels can read/write the same chunk simultaneously. Latency is high (~300-600 cycles). Shared memory: private to each block/SM; fast (~1-10 cycles). Registers: private to each thread; fastest (\u003c1 cycle). So, global memory is like a shared kitchen, while shared memory and registers are your private meal prep stations.\nTVM: From Computation Graphs to Hardware Execution TVM starts by taking in an IR, basically a computation graph that’s detached from any specific execution schedule. It runs optimizations over this graph, then compiles it for any target hardware.\nBefore actual code generation, the model is converted into Tensor Expressions (TE) - the highest level of abstraction where even scalar operations are represented as tensor computations. From there, TVM progressively lowers the code down to hardware-native operations:\nTPUs: tensor instructions CPUs: scalar code GPUs: vectorized code Vectorization is essentially mapping your data elements to threads so that each warp executes the same operation simultaneously.\nEven at the tensor level, TVM applies several techniques within the TE IR:\nTiling: breaks large loops into smaller blocks that fit better into caches or GPU thread blocks. Unrolling: expands short loops into straight-line code to reduce overhead and expose more instruction-level parallelism. Parallelism: maps independent loop iterations to multiple CPU threads or GPU blocks so they can run at the same time. This transforms nested loops into full grid-like execution on GPUs, where each tile or loop block maps to a thread block or warp expected to run in parallel on an SM.\nFinding the most optimal execution schedule isn’t straightforward - the search space is massive, and likely not even a P problem. There are many tunable knobs:\ntiling sizes unroll factors thread and block mappings vectorization width memory layouts Because of this, TVM uses machine-learning-based autotuning to predict which optimization combinations will give the best runtime, constantly refining its schedule through iterative search and real hardware feedback.\nConceptually: TVM receives a graph of computation, optimizes it, converts it into imperative tensor operations with a default schedule (how to unwrap and execute the loops), and fuses those two together. Then it compiles and runs the generated code. A background ML tuner observes runtime performance, tweaks the schedule parameters, re-fuses and recompiles, and repeats - gradually learning about the underlying hardware (treated as a black box) and version-controlling the best-found schedule.\nTriton: Tiles, JIT, and Memory Locality Triton takes a slightly different, complementary approach. Instead of fully automated schedule search like TVM, Triton introduces tiles - small sub-tensors that divide computations into manageable chunks.\nHow it works:\nTiles map naturally to thread blocks (groups of warps on an SM). Compiler analyzes tile iteration spaces, liveness, and memory dependencies. Each tile can be loaded into shared memory or cache, improving memory locality. Triton JIT automatically generates GPU code: partitioning tiles, mapping threads/warps, vectorization, and memory reuse. Interesting quirk: Triton leaves tile sizing to the programmer but handles everything else automatically (which is impressive taking into account that the paper demonstrates remarkable efficiency for large tensor operations without manually writing CUDA), which seems like a small jump conceptually but actually simplifies codegen and improves performance drastically.\nClosing Thoughts Reading GPU hardware, TVM, and Triton together is like zooming from transistor-level execution to high-level tensor graphs. GPUs define the rules of parallelism, TVM automates how loops and tensor ops map to hardware, and Triton shows how tiles + JIT + memory locality unlock blazing speed.\nTogether, they tell the story: hardware is complicated, but smart compilation and scheduling can make Python code run like native C++. For anyone who loves digging under the hood of ML performance, these papers are an absolute goldmine.\nTVM: An Automated End-to-End Optimizing Compiler for Deep Learning Triton: An Intermediate Representation and Compiler for Writing Efficient GPU Code ","wordCount":"971","inLanguage":"en","image":"http://localhost:1313/images/papermod-cover.png","datePublished":"2025-10-06T14:00:00-07:00","dateModified":"2025-10-06T14:00:00-07:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/gpu-tvm-triton/"},"publisher":{"@type":"Organization","name":"T|A","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="T|A (Alt + H)">T|A</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about title=About><span>About</span></a></li><li><a href=http://localhost:1313/chronicles title=Chronicles><span>Chronicles</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://github.com/t-avil title=Github><span>Github</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">NOTES: GPU Architecture, TVM, and Triton</h1><div class=post-description>My reading notes on GPU hardware, TVM compilation, and Triton JIT for efficient tensor computation.</div><div class=post-meta><span title='2025-10-06 14:00:00 -0700 PDT'>October 6, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;971 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/t-avil/blog/tree/main/content/posts/gpu-tvm-triton.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#gpu-architecture-warps-sms-and-parallel-execution>GPU Architecture: Warps, SMs, and Parallel Execution</a><ul><li><a href=#warp-instruction--memory-table>Warp Instruction & Memory Table</a></li></ul></li><li><a href=#tvm-from-computation-graphs-to-hardware-execution>TVM: From Computation Graphs to Hardware Execution</a></li><li><a href=#triton-tiles-jit-and-memory-locality>Triton: Tiles, JIT, and Memory Locality</a></li><li><a href=#closing-thoughts>Closing Thoughts</a></li></ul></nav></div></details></div><div class=post-content><hr><p>If TensorFlow showed us how to scale ML across entire data centers, and PyTorch 2.0 showed us how to compile dynamic Python code, GPUs and compiler stacks like TVM and Triton show us how to squeeze the absolute last drop of performance out of hardware. This post is my attempt to tie together GPU architecture, TVM’s tensor-level compilation, and Triton’s clever tiling JIT strategy, all in one nerdy, slightly dorky package.</p><hr><h2 id=gpu-architecture-warps-sms-and-parallel-execution>GPU Architecture: Warps, SMs, and Parallel Execution<a hidden class=anchor aria-hidden=true href=#gpu-architecture-warps-sms-and-parallel-execution>#</a></h2><p>Hardware instructions on modern GPUs are parallelized into multiples of <strong>32 threads</strong>. These chunks of 32 threads are called <strong>warps</strong>, but the instructions inside a warp can differ - though it’s more efficient if they’re all the same. Each worker, or <strong>Streaming Multiprocessor (SM)</strong>, can execute about <strong>4 warps simultaneously</strong>.</p><p>Tasks, also called <strong>blocks</strong>, are assigned to SMs by the GPU scheduler. The kernel function determines how many tasks to launch and how many instructions each contains. Let’s run an example to see this in action:</p><p>Suppose we have <strong>2 different kernels</strong>, each with 51 blocks, and <strong>100 SMs</strong>. Most blocks (say 50 from each kernel) contain 8 warps. Each SM executes 4 warps per cycle, so these blocks take <strong>2 cycles</strong> each. The last two blocks (7 and 9 warps) are queued, requiring <strong>Max(ceil(7/4), ceil(9/4)) = 3 cycles</strong>. Total execution: <strong>5 cycles</strong>.</p><hr><h3 id=warp-instruction--memory-table>Warp Instruction & Memory Table<a hidden class=anchor aria-hidden=true href=#warp-instruction--memory-table>#</a></h3><table><thead><tr><th>Type</th><th>Example</th><th>Warp-level execution</th><th>Memory hierarchy / latency</th></tr></thead><tbody><tr><td>1-cycle / fast</td><td>add, sub, fadd, fmul, fma, bitwise ops (and, or, xor), compare (cmp)</td><td>1 cycle per warp</td><td>Registers; fastest, no memory dependency</td></tr><tr><td>Slow / multi-cycle</td><td>idiv, fdiv, sqrt, rsqrt</td><td>Multiple cycles per warp, pipelined</td><td>Registers; some compute-bound</td></tr><tr><td>Global memory</td><td>ld.global, st.global</td><td>High latency (~300-600 cycles) per warp; scheduler switches warps</td><td>DRAM</td></tr><tr><td>Shared memory</td><td>ld.shared, st.shared</td><td>Low latency (~1-10 cycles) per warp; can bank conflict</td><td>SM-local shared memory</td></tr><tr><td>Texture / special units</td><td>ld.texture, atomic</td><td>Depends on unit; pipelined</td><td>Read-only caches / special memory</td></tr><tr><td>Matrix multiply / tensor ops</td><td>wmma, dp4a, fma</td><td>1 cycle throughput per warp on tensor cores</td><td>Registers or shared memory for operands</td></tr></tbody></table><p><strong>Key notes:</strong></p><ul><li><strong>Global memory:</strong> shared across kernels; multiple kernels can read/write the same chunk simultaneously. Latency is high (~300-600 cycles).</li><li><strong>Shared memory:</strong> private to each block/SM; fast (~1-10 cycles).</li><li><strong>Registers:</strong> private to each thread; fastest (&lt;1 cycle).</li></ul><p>So, global memory is like a shared kitchen, while shared memory and registers are your private meal prep stations.</p><hr><h2 id=tvm-from-computation-graphs-to-hardware-execution>TVM: From Computation Graphs to Hardware Execution<a hidden class=anchor aria-hidden=true href=#tvm-from-computation-graphs-to-hardware-execution>#</a></h2><p><a href=https://tvm.apache.org>TVM</a> starts by taking in an <strong>IR</strong>, basically a computation graph that’s detached from any specific execution schedule. It runs optimizations over this graph, then compiles it for any target hardware.</p><p>Before actual code generation, the model is converted into <strong>Tensor Expressions (TE)</strong> - the highest level of abstraction where even scalar operations are represented as tensor computations. From there, TVM progressively lowers the code down to hardware-native operations:</p><ul><li><strong>TPUs:</strong> tensor instructions</li><li><strong>CPUs:</strong> scalar code</li><li><strong>GPUs:</strong> vectorized code</li></ul><p><strong>Vectorization</strong> is essentially mapping your data elements to threads so that each warp executes the same operation simultaneously.</p><p>Even at the tensor level, TVM applies several techniques within the TE IR:</p><ul><li><strong>Tiling:</strong> breaks large loops into smaller blocks that fit better into caches or GPU thread blocks.</li><li><strong>Unrolling:</strong> expands short loops into straight-line code to reduce overhead and expose more instruction-level parallelism.</li><li><strong>Parallelism:</strong> maps independent loop iterations to multiple CPU threads or GPU blocks so they can run at the same time.</li></ul><p>This transforms nested loops into <strong>full grid-like execution on GPUs</strong>, where each tile or loop block maps to a thread block or warp expected to run in parallel on an SM.</p><p>Finding the most optimal execution schedule isn’t straightforward - the search space is massive, and likely <strong>not even a P problem</strong>. There are many tunable knobs:</p><ul><li>tiling sizes</li><li>unroll factors</li><li>thread and block mappings</li><li>vectorization width</li><li>memory layouts</li></ul><p>Because of this, TVM uses <strong>machine-learning-based autotuning</strong> to predict which optimization combinations will give the best runtime, constantly refining its schedule through iterative search and real hardware feedback.</p><p><strong>Conceptually:</strong> TVM receives a graph of computation, optimizes it, converts it into imperative tensor operations with a default schedule (how to unwrap and execute the loops), and fuses those two together.
Then it compiles and runs the generated code.
A background ML tuner observes runtime performance, tweaks the schedule parameters, re-fuses and recompiles, and repeats - gradually learning about the underlying hardware (treated as a black box) and version-controlling the best-found schedule.</p><hr><h2 id=triton-tiles-jit-and-memory-locality>Triton: Tiles, JIT, and Memory Locality<a hidden class=anchor aria-hidden=true href=#triton-tiles-jit-and-memory-locality>#</a></h2><p><a href=https://arxiv.org/abs/2003.06106>Triton</a> takes a slightly different, complementary approach. Instead of fully automated schedule search like TVM, Triton introduces <strong>tiles</strong> - small sub-tensors that divide computations into manageable chunks.</p><p><strong>How it works:</strong></p><ul><li>Tiles map naturally to <strong>thread blocks</strong> (groups of warps on an SM).</li><li>Compiler analyzes tile <strong>iteration spaces, liveness, and memory dependencies</strong>.</li><li>Each tile can be loaded into <strong>shared memory or cache</strong>, improving memory locality.</li><li>Triton JIT automatically generates GPU code: partitioning tiles, mapping threads/warps, vectorization, and memory reuse.</li></ul><p><strong>Interesting quirk:</strong> Triton leaves <strong>tile sizing</strong> to the programmer but handles everything else automatically (which is impressive taking into account that the paper demonstrates remarkable efficiency for large tensor operations without manually writing CUDA), which seems like a small jump conceptually but actually simplifies codegen and improves performance drastically.</p><hr><h2 id=closing-thoughts>Closing Thoughts<a hidden class=anchor aria-hidden=true href=#closing-thoughts>#</a></h2><p>Reading GPU hardware, TVM, and Triton together is like zooming from transistor-level execution to high-level tensor graphs. GPUs define the <strong>rules of parallelism</strong>, TVM automates <strong>how loops and tensor ops map to hardware</strong>, and Triton shows how <strong>tiles + JIT + memory locality</strong> unlock blazing speed.</p><p>Together, they tell the story: <em>hardware is complicated, but smart compilation and scheduling can make Python code run like native C++</em>. For anyone who loves digging under the hood of ML performance, these papers are an absolute goldmine.</p><ul><li><a href=https://arxiv.org/abs/1802.04799>TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</a></li><li><a href=https://arxiv.org/abs/2003.06106>Triton: An Intermediate Representation and Compiler for Writing Efficient GPU Code</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/notes/>Notes</a></li><li><a href=http://localhost:1313/tags/machine-learning/>Machine-Learning</a></li><li><a href=http://localhost:1313/tags/gpu/>Gpu</a></li><li><a href=http://localhost:1313/tags/compilers/>Compilers</a></li><li><a href=http://localhost:1313/tags/tvm/>Tvm</a></li><li><a href=http://localhost:1313/tags/triton/>Triton</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/virtual-spiders/><span class=title>« Prev</span><br><span>NOTES: vSpiders or how to virtualize a network</span>
</a><a class=next href=http://localhost:1313/posts/vming-containers/><span class=title>Next »</span><br><span>NOTES: VMing the Containers - The Latency-Availability Tradeoff</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>T|A</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>