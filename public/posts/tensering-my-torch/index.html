<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NOTES: TensorFlow, TorchDynamo, and TorchInductor | T|A</title><meta name=keywords content="notes,machine-learning,distributed-systems,compilers,tensorflow,pytorch"><meta name=description content="My reading notes on two generations of ML system papers: TensorFlow’s distributed execution model and PyTorch 2.0’s compiler stack (TorchDynamo + TorchInductor)."><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.3f7ba6a00d316a1658af1e52b60f5592bfd3f63e1683217d447958625c9fec2a.css integrity="sha256-P3umoA0xahZYrx5Stg9Vkr/T9j4WgyF9RHlYYlyf7Co=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/tensering-my-torch/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/tensering-my-torch/"><meta property="og:site_name" content="T|A"><meta property="og:title" content="NOTES: TensorFlow, TorchDynamo, and TorchInductor"><meta property="og:description" content="My reading notes on two generations of ML system papers: TensorFlow’s distributed execution model and PyTorch 2.0’s compiler stack (TorchDynamo + TorchInductor)."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-30T23:23:37-07:00"><meta property="article:modified_time" content="2025-09-30T23:23:37-07:00"><meta property="article:tag" content="Notes"><meta property="article:tag" content="Machine-Learning"><meta property="article:tag" content="Distributed-Systems"><meta property="article:tag" content="Compilers"><meta property="article:tag" content="Tensorflow"><meta property="article:tag" content="Pytorch"><meta property="og:image" content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:title content="NOTES: TensorFlow, TorchDynamo, and TorchInductor"><meta name=twitter:description content="My reading notes on two generations of ML system papers: TensorFlow’s distributed execution model and PyTorch 2.0’s compiler stack (TorchDynamo + TorchInductor)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"NOTES: TensorFlow, TorchDynamo, and TorchInductor","item":"http://localhost:1313/posts/tensering-my-torch/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NOTES: TensorFlow, TorchDynamo, and TorchInductor","name":"NOTES: TensorFlow, TorchDynamo, and TorchInductor","description":"My reading notes on two generations of ML system papers: TensorFlow’s distributed execution model and PyTorch 2.0’s compiler stack (TorchDynamo + TorchInductor).","keywords":["notes","machine-learning","distributed-systems","compilers","tensorflow","pytorch"],"articleBody":"If TensorFlow was about making large-scale distributed ML programmable, PyTorch 2.0 is about making dynamic Python code compilable without losing flexibility. The TensorFlow paper walked through distributed dataflow graphs, parameter servers, checkpointing, and execution placement. In contrast, the PyTorch 2.0 paper focused on TorchDynamo and TorchInductor, showing how dynamic graphs can be captured and compiled into fused kernels that run nearly as fast as handwritten CUDA. Below are my notes on the history, design, and impact of these systems.\nTensorFlow: Distributed ML via Dataflow Graphs History:\nReleased in 2015 (paper at OSDI 2016), TensorFlow was Google’s successor to DistBelief, but with the key difference of being open source and user-facing.\nKey ideas:\nDataflow graphs: nodes are ops, edges are tensors. Easy to parallelize and reason about. Device placement: runtime decides whether ops run on CPU, GPU, or TPU, with flexibility for heterogeneous clusters. Parameter servers: split model state management from computation; workers compute gradients, parameter servers update global state. Checkpointing and recovery: periodic backups of model state for fault tolerance and restart. Mutations: even small ops like += were highlighted for their efficiency in a distributed setting. Impact:\nBrought distributed ML into mainstream practice. Made concepts like parameter servers widely known. Inspired entire ecosystems of frameworks (MXNet, Horovod, PyTorch Distributed). Also exposed challenges like stale gradients, a problem still not fully solved. TorchDynamo: Graph Capture for Dynamic PyTorch History:\nIntroduced with PyTorch 2.0 (2023), TorchDynamo is the front-end of the new compiler stack. It uses CPython hooks to intercept execution and turn PyTorch programs into graphs.\nKey ideas:\nFrame evaluation hooks: intercept Python bytecode at runtime. FX graph IR: TorchDynamo lowers programs into FX graphs, a PyTorch-native intermediate representation. Guards: attach conditions (tensor shapes, dtypes, control flow) so graphs are only reused when valid. Minimal optimization: Dynamo itself does light simplifications but mainly produces a clean graph for backends. Impact:\nFirst system to make dynamic Python code reliably compilable in PyTorch. FX graphs became the “lingua franca” of PyTorch compilers. Established the bridge between eager-mode flexibility and compiled execution. TorchInductor: Fused Kernels and Code Generation History:\nAlso part of PyTorch 2.0, TorchInductor is the default backend compiler that takes FX graphs and lowers them to optimized code.\nKey ideas:\nOperator fusion: combine sequences of ops into a single kernel to cut down launch overhead and improve locality. Portable kernel generation: emits C++/OpenMP for CPUs, Triton for GPUs. Dynamic shapes support: designs execution plans that adapt without speculative recompilation. Seamless use: users just call torch.compile, no rewriting needed. Impact:\nTurned PyTorch 2.0 into a “compiled PyTorch” without losing dynamic semantics. Gave researchers nearly hand-tuned performance with no CUDA required. Strongly boosted distributed training frameworks (DDP, FSDP, DeepSpeed) by improving single-node throughput. Made Triton a hot topic in ML systems by showing it could serve as a production GPU kernel generator. Closing Thoughts Reading these two papers back to back felt like looking at two eras of ML systems design. TensorFlow focused on distributed execution and system scalability, while PyTorch 2.0 doubled down on compilation and single-node performance. TensorFlow brought parameter servers, checkpoints, and the dataflow model into the vocabulary of ML practitioners. PyTorch 2.0 brought graph capture, FX IR, and kernel fusion into the mainstream. Together they map the shift from “how do we scale models across data centers?” to “how do we make dynamic Python code run like C++?”\n","wordCount":"559","inLanguage":"en","image":"http://localhost:1313/images/papermod-cover.png","datePublished":"2025-09-30T23:23:37-07:00","dateModified":"2025-09-30T23:23:37-07:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/tensering-my-torch/"},"publisher":{"@type":"Organization","name":"T|A","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="T|A (Alt + H)">T|A</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about title=About><span>About</span></a></li><li><a href=http://localhost:1313/chronicles title=Chronicles><span>Chronicles</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://github.com/t-avil title=Github><span>Github</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">NOTES: TensorFlow, TorchDynamo, and TorchInductor</h1><div class=post-description>My reading notes on two generations of ML system papers: TensorFlow’s distributed execution model and PyTorch 2.0’s compiler stack (TorchDynamo + TorchInductor).</div><div class=post-meta><span title='2025-09-30 23:23:37 -0700 PDT'>September 30, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;559 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/t-avil/blog/tree/main/content/posts/tensering-my-torch.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#tensorflow-distributed-ml-via-dataflow-graphs>TensorFlow: Distributed ML via Dataflow Graphs</a></li><li><a href=#torchdynamo-graph-capture-for-dynamic-pytorch>TorchDynamo: Graph Capture for Dynamic PyTorch</a></li><li><a href=#torchinductor-fused-kernels-and-code-generation>TorchInductor: Fused Kernels and Code Generation</a></li><li><a href=#closing-thoughts>Closing Thoughts</a></li></ul></nav></div></details></div><div class=post-content><p>If TensorFlow was about making large-scale distributed ML programmable, PyTorch 2.0 is about making dynamic Python code compilable without losing flexibility. The TensorFlow paper walked through distributed dataflow graphs, parameter servers, checkpointing, and execution placement. In contrast, the PyTorch 2.0 paper focused on TorchDynamo and TorchInductor, showing how dynamic graphs can be captured and compiled into fused kernels that run nearly as fast as handwritten CUDA. Below are my notes on the history, design, and impact of these systems.</p><hr><h2 id=tensorflow-distributed-ml-via-dataflow-graphs>TensorFlow: Distributed ML via Dataflow Graphs<a hidden class=anchor aria-hidden=true href=#tensorflow-distributed-ml-via-dataflow-graphs>#</a></h2><p><strong>History:</strong><br>Released in 2015 (paper at OSDI 2016), TensorFlow was Google’s successor to DistBelief, but with the key difference of being open source and user-facing.</p><p><strong>Key ideas:</strong></p><ul><li><strong>Dataflow graphs</strong>: nodes are ops, edges are tensors. Easy to parallelize and reason about.</li><li><strong>Device placement</strong>: runtime decides whether ops run on CPU, GPU, or TPU, with flexibility for heterogeneous clusters.</li><li><strong>Parameter servers</strong>: split model state management from computation; workers compute gradients, parameter servers update global state.</li><li><strong>Checkpointing and recovery</strong>: periodic backups of model state for fault tolerance and restart.</li><li><strong>Mutations</strong>: even small ops like <code>+=</code> were highlighted for their efficiency in a distributed setting.</li></ul><p><strong>Impact:</strong></p><ul><li>Brought distributed ML into mainstream practice.</li><li>Made concepts like parameter servers widely known.</li><li>Inspired entire ecosystems of frameworks (MXNet, Horovod, PyTorch Distributed).</li><li>Also exposed challenges like <strong>stale gradients</strong>, a problem still not fully solved.</li></ul><hr><h2 id=torchdynamo-graph-capture-for-dynamic-pytorch>TorchDynamo: Graph Capture for Dynamic PyTorch<a hidden class=anchor aria-hidden=true href=#torchdynamo-graph-capture-for-dynamic-pytorch>#</a></h2><p><strong>History:</strong><br>Introduced with PyTorch 2.0 (2023), TorchDynamo is the front-end of the new compiler stack. It uses CPython hooks to intercept execution and turn PyTorch programs into graphs.</p><p><strong>Key ideas:</strong></p><ul><li><strong>Frame evaluation hooks</strong>: intercept Python bytecode at runtime.</li><li><strong>FX graph IR</strong>: TorchDynamo lowers programs into FX graphs, a PyTorch-native intermediate representation.</li><li><strong>Guards</strong>: attach conditions (tensor shapes, dtypes, control flow) so graphs are only reused when valid.</li><li><strong>Minimal optimization</strong>: Dynamo itself does light simplifications but mainly produces a clean graph for backends.</li></ul><p><strong>Impact:</strong></p><ul><li>First system to make dynamic Python code reliably compilable in PyTorch.</li><li>FX graphs became the “lingua franca” of PyTorch compilers.</li><li>Established the bridge between eager-mode flexibility and compiled execution.</li></ul><hr><h2 id=torchinductor-fused-kernels-and-code-generation>TorchInductor: Fused Kernels and Code Generation<a hidden class=anchor aria-hidden=true href=#torchinductor-fused-kernels-and-code-generation>#</a></h2><p><strong>History:</strong><br>Also part of PyTorch 2.0, TorchInductor is the default backend compiler that takes FX graphs and lowers them to optimized code.</p><p><strong>Key ideas:</strong></p><ul><li><strong>Operator fusion</strong>: combine sequences of ops into a single kernel to cut down launch overhead and improve locality.</li><li><strong>Portable kernel generation</strong>: emits C++/OpenMP for CPUs, Triton for GPUs.</li><li><strong>Dynamic shapes support</strong>: designs execution plans that adapt without speculative recompilation.</li><li><strong>Seamless use</strong>: users just call <code>torch.compile</code>, no rewriting needed.</li></ul><p><strong>Impact:</strong></p><ul><li>Turned PyTorch 2.0 into a “compiled PyTorch” without losing dynamic semantics.</li><li>Gave researchers nearly hand-tuned performance with no CUDA required.</li><li>Strongly boosted distributed training frameworks (DDP, FSDP, DeepSpeed) by improving single-node throughput.</li><li>Made Triton a hot topic in ML systems by showing it could serve as a production GPU kernel generator.</li></ul><hr><h2 id=closing-thoughts>Closing Thoughts<a hidden class=anchor aria-hidden=true href=#closing-thoughts>#</a></h2><p>Reading these two papers back to back felt like looking at two eras of ML systems design. TensorFlow focused on <em>distributed execution and system scalability</em>, while PyTorch 2.0 doubled down on <em>compilation and single-node performance</em>. TensorFlow brought parameter servers, checkpoints, and the dataflow model into the vocabulary of ML practitioners. PyTorch 2.0 brought graph capture, FX IR, and kernel fusion into the mainstream. Together they map the shift from &ldquo;how do we scale models across data centers?&rdquo; to &ldquo;how do we make dynamic Python code run like C++?&rdquo;</p><hr></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/notes/>Notes</a></li><li><a href=http://localhost:1313/tags/machine-learning/>Machine-Learning</a></li><li><a href=http://localhost:1313/tags/distributed-systems/>Distributed-Systems</a></li><li><a href=http://localhost:1313/tags/compilers/>Compilers</a></li><li><a href=http://localhost:1313/tags/tensorflow/>Tensorflow</a></li><li><a href=http://localhost:1313/tags/pytorch/>Pytorch</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/vming-containers/><span class=title>« Prev</span><br><span>NOTES: VMing the Containers - The Latency-Availability Tradeoff</span>
</a><a class=next href=http://localhost:1313/posts/micro-pq-crypt-for-lora/><span class=title>Next »</span><br><span>PROJECT: Post-Quantum Cryptography in Meshtastic</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>T|A</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>