<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NOTES: training the distributed scale? | T|A</title><meta name=keywords content="notes,distributed-training,large-models,data-parallel,model-parallel,memory-optimization"><meta name=description content="Summary of DDP, FlexFlow, ZeRO, FSDP, activation checkpointing and ZeRO-Infinity for GPU/ML inference and training systems."><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.3f7ba6a00d316a1658af1e52b60f5592bfd3f63e1683217d447958625c9fec2a.css integrity="sha256-P3umoA0xahZYrx5Stg9Vkr/T9j4WgyF9RHlYYlyf7Co=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/model-training-partitioning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/model-training-partitioning/"><meta property="og:site_name" content="T|A"><meta property="og:title" content="NOTES: training the distributed scale?"><meta property="og:description" content="Summary of DDP, FlexFlow, ZeRO, FSDP, activation checkpointing and ZeRO-Infinity for GPU/ML inference and training systems."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-21T04:52:54-07:00"><meta property="article:modified_time" content="2025-10-21T04:52:54-07:00"><meta property="article:tag" content="Notes"><meta property="article:tag" content="Distributed-Training"><meta property="article:tag" content="Large-Models"><meta property="article:tag" content="Data-Parallel"><meta property="article:tag" content="Model-Parallel"><meta property="article:tag" content="Memory-Optimization"><meta property="og:image" content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/papermod-cover.png"><meta name=twitter:title content="NOTES: training the distributed scale?"><meta name=twitter:description content="Summary of DDP, FlexFlow, ZeRO, FSDP, activation checkpointing and ZeRO-Infinity for GPU/ML inference and training systems."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"NOTES: training the distributed scale?","item":"http://localhost:1313/posts/model-training-partitioning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NOTES: training the distributed scale?","name":"NOTES: training the distributed scale?","description":"Summary of DDP, FlexFlow, ZeRO, FSDP, activation checkpointing and ZeRO-Infinity for GPU/ML inference and training systems.","keywords":["notes","distributed-training","large-models","data-parallel","model-parallel","memory-optimization"],"articleBody":" Key definitions Data parallelism: every GPU (worker) holds a full copy of the model and processes a different slice of the training data; after computing gradients they are combined (synchronised). Model parallelism: the model itself is split across devices (layers or parameters), so different GPUs handle different parts of the model. Parameter / optimizer state / gradient: model parameters = weights; gradients = derivatives computed in back-prop; optimizer state = extra per-parameter data (momentum, Adam’s moments). Collective communication / all-reduce: when multiple devices coordinate data exchange, e.g., an all-reduce sums up a tensor across all devices then distributes the result back. Sharding: dividing a tensor or state among workers so each stores only a part (instead of full replication). Offloading: moving data (parameters, states) to slower but larger memory (CPU RAM or SSD/NVMe) when it isn’t actively needed on GPU. 1. PyTorch Distributed Data Parallel (DDP) There is a problem of training large deep-neural-network models efficiently across many GPUs, so we used DDP, which replicates the model on each GPU, processes different samples in parallel, and then synchronises gradients via collective operations, achieving high scalability. The main takeaway / innovation / addition / insight is efficient overlap of computation and communication - it was implemented through gradient “bucketing” (grouping gradient tensors) and overlapping gradient reduction during backward pass, and enabled by the process-group abstraction (in torch.distributed) and collective backends such as NCCL. The second takeaway is using a process-group abstraction that hides details of inter-GPU communication - implemented via torch.distributed.ProcessGroup, and enabled by NCCL or MPI based libraries that manage all-reduce/ broadcast under the hood. The third insight is reducing redundant copies of model parameters and buffers across processes - rather than each process fully copying every buffer, DDP uses shared storage for model parameters in multi-process mode (via torch.multiprocessing + shared CUDA memory) so that intra-node clones reuse memory rather than duplicating it. And the last additional interesting fact is that DDP achieves near-linear scalability (up to ~256 GPUs) when configured properly (bucketing, overlapping) as shown in the PyTorch paper. (arXiv)\nClarification on the “shared memory between processes” point: In PyTorch DDP when using multi-process on one node, each process holds its own CUDA context but the CPU memory region can use torch.multiprocessing with shared memory segments, and PyTorch also uses the NCCL backend so that intra-node GPUs can share parameter memory without full duplication when possible. It’s not that Python’s GIL blocks that sharing - each process has independent Python interpreter, and low-level CUDA/NCCL primitives handle memory sharing and communication. So the insight is: instead of naively replicating full model state in each process, DDP reuses underlying memory (especially on a node) and minimizes inter-process redundant copies, thereby saving GPU/CPU memory and reducing pressure.\n2. FlexFlow There is a problem of finding the optimal parallelism strategy for DNN training (beyond simple data or model parallelism) so we built FlexFlow, which automatically searches a broader space of parallel strategies (Sample, Operator, Attribute, Parameter) and achieved up to ~3-4× speed‐up over prior systems. The main takeaway is the introduction of the SOAP search space (Sample, Operator, Attribute, Parameter) for parallelism - implemented via a simulator that predicts execution cost, and enabled by modelling both computation and communication accurately in that simulator. The second takeaway is guided randomised search (e.g., MCMC) through the SOAP space - implemented using the simulator + search algorithm, and enabled by the fact that the simulator is ~1000× faster than executing the full strategy. The third insight is adapting to heterogeneous GPU clusters and layer/operator-specific splits - implemented by allowing partitioning not just by samples or weights but also by operators and attributes (e.g., splitting internal tensor attributes), enabled by the flexible runtime that maps operations across devices. And the last additional interesting fact is that FlexFlow sometimes picks unexpected parallelisation patterns (e.g., splitting attributes inside a tensor) that outperform standard data/model parallelism by up to 3.8×. (arXiv)\nBonus: Other two dimensions besides “load model on different devices \u0026 sync gradients”: In addition to Sample‐parallel (data parallel) and Parameter/model‐parallel, FlexFlow supports:\nOperator parallelism: splitting the work of a single operator (e.g., a matrix‐multiply) across devices (the “O” in SOAP). Attribute parallelism: splitting within the tensor attributes (e.g., partitioning along channel/feature dimension) (the “A” in SOAP). So FlexFlow explores all four dimensions (S,O,A,P) to find novel configurations. 3. ZeRO (Zero Redundancy Optimizer) There is a problem of GPU memory constraints when training extremely large models (billions-to-trillions of parameters), so we used ZeRO, which shards optimizer states, gradients, and parameters across GPUs to achieve drastic memory reduction and enable much bigger models. The main takeaway is optimizer-state sharding (Stage 1) - implemented by partitioning the optimizer states so each device only stores a fraction of the momentum/Adam states, and enabled by distributed coordination of update steps and communication to fetch needed states. The second takeaway is gradient and parameter sharding (Stages 2 and 3) - implemented via splitting gradients and weights across devices so no device holds full copies, and enabled by efficient collective operations (all-gather, reduce-scatter) that aggregate results across shards. The third insight is minimizing communication overhead while scaling memory linearly with devices - implemented by designing communication primitives that only exchange minimal necessary data instead of full copies, and enabled by high-bandwidth interconnects + optimized collective algorithms. And the last additional interesting fact is that ZeRO required no changes to model code (you could plug it in to many existing PyTorch models) and has been used to train \u003e100B parameter models. (arXiv)\nAbout the “communication trick / all-reduce”: All-reduce is a collective that sums a tensor across all devices and returns the result to each device. ZeRO optimizes this by using reduce-scatter (each device receives part of the sum) then all-gather (reconstruct full tensor) rather than full all-reduce on large full copies. This dramatically reduces communication volume and overlap time. (Engineering at Meta)\n4. PyTorch Fully Sharded Data Parallel (FSDP) There is a problem of memory inefficiency and scalability limits in standard DDP (full replication of parameters/gradients/optimizer states) when training very large models, so we use FSDP, which shards all model parameters, gradients, and optimizer states across GPUs and overlaps communication intelligently, achieving large-model training with much lower per-GPU memory. The main takeaway is “flat parameter” sharding plus on-the-fly load/unload of parameter shards - implemented by each FSDP unit wrapping a module, creating a FlatParameter from its weights, dynamically all-gathering when needed on forward/backward and then freeing immediately, enabled by PyTorch’s tensor view mechanics and caching allocator. The second takeaway is overlap of communication with forward/backward - implemented by decomposing all-reduce into reduce‐scatter + all‐gather and scheduling that to run concurrently with computation, enabled by the CUDA memory caching allocator and asynchronous streams by PyTorch. The third insight is nestable/sharded wrapping of submodules for fine granularity - implemented by allowing nested FSDP units per layer or group, and enabled by PyTorch’s modular nn.Module API and dispatcher system for parameter handling. And the last additional interesting fact is that FSDP provides a drop-in replacement for DDP in PyTorch and is now used in production at large scale. (Engineering at Meta)\n5. Activation Checkpointing There is a problem of GPU memory being consumed by storing every activation (intermediate layer output) for back-propagation in deep networks, so we use activation checkpointing, which drops or only stores some activations and recomputes them during the backward pass, achieving substantial memory savings (at the cost of some extra compute). The main takeaway is memory-vs-compute trade-off - implemented by marking certain layer outputs as “checkpoint” so they aren’t saved, and recomputing them in backward pass, enabled by frameworks (e.g., PyTorch) that support re-running forward subgraphs and backward hooks. The second takeaway is scalability for very deep models - implemented through selective checkpointing of expensive layers (transformers, CNN stacks) and enabled by custom backward scheduling logic. The third insight is compatibility with sharded/parallel training - implemented by combining checkpointing with e.g., ZeRO or FSDP to further reduce memory, and enabled by runtime memory planners that coordinate recompute with communications. And the last additional interesting fact is that checkpointing can reduce activation memory by ~80% in some deep transformer stacks, albeit with ~10-30% compute overhead.\n6. ZeRO-Infinity There is a problem of training trillion+ parameter models on limited GPU memory, so we use ZeRO-Infinity, which extends ZeRO by offloading optimizer states, gradients and parameters to CPU or NVMe storage tiers when idle, achieving virtually unlimited model size while still using GPU compute efficiently. The main takeaway is hierarchical memory management with offload tiers - implemented by keeping hot data on GPU, warm data on CPU, and cold data on NVMe, and enabled by asynchronous data movement overlapped with computation. The second takeaway is communication and bandwidth optimisation across tiers - implemented by smart prefetching, compression, and asynchronous overlap, and enabled by the runtime scheduler of DeepSpeed. The third insight is scalable training from few GPUs through large clusters using mixed offload - implemented by hybrid parallelism (data + tensor + pipeline) plus offload, and enabled by ZeRO-Infinity’s integration into DeepSpeed. And the last additional interesting fact is that ZeRO-Infinity has been used to train \u003e100 billion-parameter models on limited GPU counts by leveraging CPU + NVMe offloading.\nKey Takeaways We can offload data and model parts from GPU to CPU or NVMe to save memory. We can use checkpoints to save progress or skip them and recompute when needed. We can sync only parameters that were updated, reducing unnecessary communication. We can shard models by weights, gradients, and optimizer states to save memory and balance load across the network. We can automatically find the best parallelization strategy using a simulator. We can overlap computation and communication so GPUs stay busy while syncing. ","wordCount":"1618","inLanguage":"en","image":"http://localhost:1313/images/papermod-cover.png","datePublished":"2025-10-21T04:52:54-07:00","dateModified":"2025-10-21T04:52:54-07:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/model-training-partitioning/"},"publisher":{"@type":"Organization","name":"T|A","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="T|A (Alt + H)">T|A</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about title=About><span>About</span></a></li><li><a href=http://localhost:1313/chronicles title=Chronicles><span>Chronicles</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://github.com/t-avil title=Github><span>Github</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">NOTES: training the distributed scale?</h1><div class=post-description>Summary of DDP, FlexFlow, ZeRO, FSDP, activation checkpointing and ZeRO-Infinity for GPU/ML inference and training systems.</div><div class=post-meta><span title='2025-10-21 04:52:54 -0700 PDT'>October 21, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1618 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/t-avil/blog/tree/main/content/posts/model-training-partitioning.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#key-definitions>Key definitions</a></li></ul></li><li><a href=#1-pytorch-distributed-data-parallel-ddp>1. <a href="https://arxiv.org/abs/2006.15704?utm_source=chatgpt.com">PyTorch Distributed Data Parallel (DDP)</a></a></li><li><a href=#2-flexflow>2. <a href="https://arxiv.org/abs/1807.05358?utm_source=chatgpt.com">FlexFlow</a></a></li><li><a href=#3-zero-zero-redundancy-optimizer>3. <a href="https://arxiv.org/abs/1910.02054?utm_source=chatgpt.com">ZeRO (Zero Redundancy Optimizer)</a></a></li><li><a href=#4-pytorch-fully-sharded-data-parallel-fsdp>4. <a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/?utm_source=chatgpt.com">PyTorch Fully Sharded Data Parallel (FSDP)</a></a></li><li><a href=#5-activation-checkpointing>5. <a href="https://pytorch.org/docs/stable/checkpoint.html?utm_source=chatgpt.com">Activation Checkpointing</a></a></li><li><a href=#6-zero-infinity>6. <a href="https://arxiv.org/abs/2104.07857?utm_source=chatgpt.com">ZeRO-Infinity</a></a><ul><li><a href=#key-takeaways>Key Takeaways</a></li></ul></li></ul></nav></div></details></div><div class=post-content><hr><h3 id=key-definitions>Key definitions<a hidden class=anchor aria-hidden=true href=#key-definitions>#</a></h3><ul><li><strong>Data parallelism</strong>: every GPU (worker) holds a full copy of the model and processes a different slice of the training data; after computing gradients they are combined (synchronised).</li><li><strong>Model parallelism</strong>: the model itself is split across devices (layers or parameters), so different GPUs handle different parts of the model.</li><li><strong>Parameter / optimizer state / gradient</strong>: model <em>parameters</em> = weights; <em>gradients</em> = derivatives computed in back-prop; <em>optimizer state</em> = extra per-parameter data (momentum, Adam’s moments).</li><li><strong>Collective communication / all-reduce</strong>: when multiple devices coordinate data exchange, e.g., an <em>all-reduce</em> sums up a tensor across all devices then distributes the result back.</li><li><strong>Sharding</strong>: dividing a tensor or state among workers so each stores only a part (instead of full replication).</li><li><strong>Offloading</strong>: moving data (parameters, states) to slower but larger memory (CPU RAM or SSD/NVMe) when it isn’t actively needed on GPU.</li></ul><hr><h2 id=1-pytorch-distributed-data-parallel-ddp>1. <a href="https://arxiv.org/abs/2006.15704?utm_source=chatgpt.com">PyTorch Distributed Data Parallel (DDP)</a><a hidden class=anchor aria-hidden=true href=#1-pytorch-distributed-data-parallel-ddp>#</a></h2><p>There is a problem of training large deep-neural-network models efficiently across many GPUs, so we used DDP, which replicates the model on each GPU, processes different samples in parallel, and then synchronises gradients via collective operations, achieving high scalability. The main takeaway / innovation / addition / insight is <strong>efficient overlap of computation and communication</strong> - it was implemented through gradient “bucketing” (grouping gradient tensors) and overlapping gradient reduction during backward pass, and enabled by the process-group abstraction (in <code>torch.distributed</code>) and collective backends such as NCCL. The second takeaway is <strong>using a process-group abstraction that hides details of inter-GPU communication</strong> - implemented via <code>torch.distributed.ProcessGroup</code>, and enabled by NCCL or MPI based libraries that manage all-reduce/ broadcast under the hood. The third insight is <strong>reducing redundant copies of model parameters and buffers across processes</strong> - rather than each process fully copying every buffer, DDP uses shared storage for model parameters in multi-process mode (via <code>torch.multiprocessing</code> + shared CUDA memory) so that intra-node clones reuse memory rather than duplicating it. And the last additional interesting fact is that DDP achieves near-linear scalability (up to ~256 GPUs) when configured properly (bucketing, overlapping) as shown in the PyTorch paper. (<a href="https://arxiv.org/abs/2006.15704?utm_source=chatgpt.com" title="PyTorch Distributed: Experiences on Accelerating Data Parallel ...">arXiv</a>)</p><p><strong>Clarification on the “shared memory between processes” point</strong>: In PyTorch DDP when using multi-process on one node, each process holds its own CUDA context but the CPU memory region can use <code>torch.multiprocessing</code> with shared memory segments, and PyTorch also uses the NCCL backend so that intra-node GPUs can share parameter memory without full duplication when possible. It’s not that Python’s GIL blocks that sharing - each process has independent Python interpreter, and low-level CUDA/NCCL primitives handle memory sharing and communication. So the insight is: instead of naively replicating full model state in each process, DDP reuses underlying memory (especially on a node) and minimizes inter-process redundant copies, thereby saving GPU/CPU memory and reducing pressure.</p><hr><h2 id=2-flexflow>2. <a href="https://arxiv.org/abs/1807.05358?utm_source=chatgpt.com">FlexFlow</a><a hidden class=anchor aria-hidden=true href=#2-flexflow>#</a></h2><p>There is a problem of finding the optimal parallelism strategy for DNN training (beyond simple data or model parallelism) so we built FlexFlow, which automatically searches a broader space of parallel strategies (Sample, Operator, Attribute, Parameter) and achieved up to ~3-4× speed‐up over prior systems. The main takeaway is the introduction of the <strong>SOAP search space</strong> (Sample, Operator, Attribute, Parameter) for parallelism - implemented via a simulator that predicts execution cost, and enabled by modelling both computation and communication accurately in that simulator. The second takeaway is <strong>guided randomised search (e.g., MCMC) through the SOAP space</strong> - implemented using the simulator + search algorithm, and enabled by the fact that the simulator is ~1000× faster than executing the full strategy. The third insight is <strong>adapting to heterogeneous GPU clusters and layer/operator-specific splits</strong> - implemented by allowing partitioning not just by samples or weights but also by operators and attributes (e.g., splitting internal tensor attributes), enabled by the flexible runtime that maps operations across devices. And the last additional interesting fact is that FlexFlow sometimes picks unexpected parallelisation patterns (e.g., splitting attributes inside a tensor) that outperform standard data/model parallelism by up to 3.8×. (<a href="https://arxiv.org/abs/1807.05358?utm_source=chatgpt.com" title="Beyond Data and Model Parallelism for Deep Neural Networks">arXiv</a>)</p><p><strong>Bonus: Other two dimensions besides “load model on different devices & sync gradients”</strong>: In addition to Sample‐parallel (data parallel) and Parameter/model‐parallel, FlexFlow supports:</p><ul><li>Operator parallelism: splitting the work of a single operator (e.g., a matrix‐multiply) across devices (the “O” in SOAP).</li><li>Attribute parallelism: splitting within the tensor attributes (e.g., partitioning along channel/feature dimension) (the “A” in SOAP).
So FlexFlow explores all four dimensions (S,O,A,P) to find novel configurations.</li></ul><hr><h2 id=3-zero-zero-redundancy-optimizer>3. <a href="https://arxiv.org/abs/1910.02054?utm_source=chatgpt.com">ZeRO (Zero Redundancy Optimizer)</a><a hidden class=anchor aria-hidden=true href=#3-zero-zero-redundancy-optimizer>#</a></h2><p>There is a problem of GPU memory constraints when training extremely large models (billions-to-trillions of parameters), so we used ZeRO, which shards optimizer states, gradients, and parameters across GPUs to achieve drastic memory reduction and enable much bigger models. The main takeaway is <strong>optimizer-state sharding</strong> (Stage 1) - implemented by partitioning the optimizer states so each device only stores a fraction of the momentum/Adam states, and enabled by distributed coordination of update steps and communication to fetch needed states. The second takeaway is <strong>gradient and parameter sharding</strong> (Stages 2 and 3) - implemented via splitting gradients and weights across devices so no device holds full copies, and enabled by efficient collective operations (all-gather, reduce-scatter) that aggregate results across shards. The third insight is <strong>minimizing communication overhead while scaling memory linearly with devices</strong> - implemented by designing communication primitives that only exchange minimal necessary data instead of full copies, and enabled by high-bandwidth interconnects + optimized collective algorithms. And the last additional interesting fact is that ZeRO required <em>no changes to model code</em> (you could plug it in to many existing PyTorch models) and has been used to train >100B parameter models. (<a href="https://arxiv.org/abs/1910.02054?utm_source=chatgpt.com" title="ZeRO: Memory Optimizations Toward Training Trillion Parameter Models">arXiv</a>)</p><p><strong>About the “communication trick / all-reduce”</strong>: All-reduce is a collective that sums a tensor across all devices and returns the result to each device. ZeRO optimizes this by using <em>reduce-scatter</em> (each device receives part of the sum) then <em>all-gather</em> (reconstruct full tensor) rather than full all-reduce on large full copies. This dramatically reduces communication volume and overlap time. (<a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/?utm_source=chatgpt.com" title="Fully Sharded Data Parallel: faster AI training with fewer GPUs">Engineering at Meta</a>)</p><hr><h2 id=4-pytorch-fully-sharded-data-parallel-fsdp>4. <a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/?utm_source=chatgpt.com">PyTorch Fully Sharded Data Parallel (FSDP)</a><a hidden class=anchor aria-hidden=true href=#4-pytorch-fully-sharded-data-parallel-fsdp>#</a></h2><p>There is a problem of memory inefficiency and scalability limits in standard DDP (full replication of parameters/gradients/optimizer states) when training very large models, so we use FSDP, which shards all model parameters, gradients, and optimizer states across GPUs and overlaps communication intelligently, achieving large-model training with much lower per-GPU memory. The main takeaway is <strong>“flat parameter” sharding plus on-the-fly load/unload of parameter shards</strong> - implemented by each FSDP unit wrapping a module, creating a FlatParameter from its weights, dynamically all-gathering when needed on forward/backward and then freeing immediately, enabled by PyTorch’s tensor view mechanics and caching allocator. The second takeaway is <strong>overlap of communication with forward/backward</strong> - implemented by decomposing all-reduce into reduce‐scatter + all‐gather and scheduling that to run concurrently with computation, enabled by the CUDA memory caching allocator and asynchronous streams by PyTorch. The third insight is <strong>nestable/sharded wrapping of submodules for fine granularity</strong> - implemented by allowing nested FSDP units per layer or group, and enabled by PyTorch’s modular <code>nn.Module</code> API and dispatcher system for parameter handling. And the last additional interesting fact is that FSDP provides a drop-in replacement for DDP in PyTorch and is now used in production at large scale. (<a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/?utm_source=chatgpt.com" title="Fully Sharded Data Parallel: faster AI training with fewer GPUs">Engineering at Meta</a>)</p><hr><h2 id=5-activation-checkpointing>5. <a href="https://pytorch.org/docs/stable/checkpoint.html?utm_source=chatgpt.com">Activation Checkpointing</a><a hidden class=anchor aria-hidden=true href=#5-activation-checkpointing>#</a></h2><p>There is a problem of GPU memory being consumed by storing every activation (intermediate layer output) for back-propagation in deep networks, so we use activation checkpointing, which drops or only stores some activations and recomputes them during the backward pass, achieving substantial memory savings (at the cost of some extra compute). The main takeaway is <strong>memory-vs-compute trade-off</strong> - implemented by marking certain layer outputs as “checkpoint” so they aren’t saved, and recomputing them in backward pass, enabled by frameworks (e.g., PyTorch) that support re-running forward subgraphs and backward hooks. The second takeaway is <strong>scalability for very deep models</strong> - implemented through selective checkpointing of expensive layers (transformers, CNN stacks) and enabled by custom backward scheduling logic. The third insight is <strong>compatibility with sharded/parallel training</strong> - implemented by combining checkpointing with e.g., ZeRO or FSDP to further reduce memory, and enabled by runtime memory planners that coordinate recompute with communications. And the last additional interesting fact is that checkpointing can reduce activation memory by ~80% in some deep transformer stacks, albeit with ~10-30% compute overhead.</p><hr><h2 id=6-zero-infinity>6. <a href="https://arxiv.org/abs/2104.07857?utm_source=chatgpt.com">ZeRO-Infinity</a><a hidden class=anchor aria-hidden=true href=#6-zero-infinity>#</a></h2><p>There is a problem of training trillion+ parameter models on limited GPU memory, so we use ZeRO-Infinity, which extends ZeRO by offloading optimizer states, gradients and parameters to CPU or NVMe storage tiers when idle, achieving virtually unlimited model size while still using GPU compute efficiently. The main takeaway is <strong>hierarchical memory management with offload tiers</strong> - implemented by keeping hot data on GPU, warm data on CPU, and cold data on NVMe, and enabled by asynchronous data movement overlapped with computation. The second takeaway is <strong>communication and bandwidth optimisation across tiers</strong> - implemented by smart prefetching, compression, and asynchronous overlap, and enabled by the runtime scheduler of DeepSpeed. The third insight is <strong>scalable training from few GPUs through large clusters using mixed offload</strong> - implemented by hybrid parallelism (data + tensor + pipeline) plus offload, and enabled by ZeRO-Infinity’s integration into DeepSpeed. And the last additional interesting fact is that ZeRO-Infinity has been used to train >100 billion-parameter models on limited GPU counts by leveraging CPU + NVMe offloading.</p><hr><h3 id=key-takeaways>Key Takeaways<a hidden class=anchor aria-hidden=true href=#key-takeaways>#</a></h3><ul><li>We can offload data and model parts from GPU to CPU or NVMe to save memory.</li><li>We can use checkpoints to save progress or skip them and recompute when needed.</li><li>We can sync only parameters that were updated, reducing unnecessary communication.</li><li>We can shard models by weights, gradients, and optimizer states to save memory and balance load across the network.</li><li>We can automatically find the best parallelization strategy using a simulator.</li><li>We can overlap computation and communication so GPUs stay busy while syncing.</li></ul><hr></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/notes/>Notes</a></li><li><a href=http://localhost:1313/tags/distributed-training/>Distributed-Training</a></li><li><a href=http://localhost:1313/tags/large-models/>Large-Models</a></li><li><a href=http://localhost:1313/tags/data-parallel/>Data-Parallel</a></li><li><a href=http://localhost:1313/tags/model-parallel/>Model-Parallel</a></li><li><a href=http://localhost:1313/tags/memory-optimization/>Memory-Optimization</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/virtual-spiders/><span class=title>Next »</span><br><span>NOTES: vSpiders or how to virtualize a network</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>T|A</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>