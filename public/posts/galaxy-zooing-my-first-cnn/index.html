<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PROJECT: Galaxy Zooing my first CNN | T|A</title><meta name=keywords content="notes,deep-learning,computer-vision,pytorch,cnn,astronomy,galaxy-zoo,first-project,vgg,kaggle"><meta name=description content="VGG architectures, galaxy morphology classification, cross-validation, and way too much time spent on data augmentation"><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.3f7ba6a00d316a1658af1e52b60f5592bfd3f63e1683217d447958625c9fec2a.css integrity="sha256-P3umoA0xahZYrx5Stg9Vkr/T9j4WgyF9RHlYYlyf7Co=" rel="preload stylesheet" as=style><link rel=icon href=https://timavilov.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://timavilov.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://timavilov.com/favicon-32x32.png><link rel=apple-touch-icon href=https://timavilov.com/apple-touch-icon.png><link rel=mask-icon href=https://timavilov.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://timavilov.com/posts/galaxy-zooing-my-first-cnn/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://timavilov.com/posts/galaxy-zooing-my-first-cnn/"><meta property="og:site_name" content="T|A"><meta property="og:title" content="PROJECT: Galaxy Zooing my first CNN"><meta property="og:description" content="VGG architectures, galaxy morphology classification, cross-validation, and way too much time spent on data augmentation"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-28T08:15:04-03:00"><meta property="article:modified_time" content="2025-05-28T08:15:04-03:00"><meta property="article:tag" content="Notes"><meta property="article:tag" content="Deep-Learning"><meta property="article:tag" content="Computer-Vision"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Cnn"><meta property="article:tag" content="Astronomy"><meta property="og:image" content="https://timavilov.com/images/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://timavilov.com/images/papermod-cover.png"><meta name=twitter:title content="PROJECT: Galaxy Zooing my first CNN"><meta name=twitter:description content="VGG architectures, galaxy morphology classification, cross-validation, and way too much time spent on data augmentation"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://timavilov.com/posts/"},{"@type":"ListItem","position":2,"name":"PROJECT: Galaxy Zooing my first CNN","item":"https://timavilov.com/posts/galaxy-zooing-my-first-cnn/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PROJECT: Galaxy Zooing my first CNN","name":"PROJECT: Galaxy Zooing my first CNN","description":"VGG architectures, galaxy morphology classification, cross-validation, and way too much time spent on data augmentation","keywords":["notes","deep-learning","computer-vision","pytorch","cnn","astronomy","galaxy-zoo","first-project","vgg","kaggle"],"articleBody":" Project poster for Applied Astrophysics Symposium at University of Washington\nI learned that building your first serious CNN is less about the architecture and more about understanding why galaxies don’t have a canonical “up” direction, and why that makes data augmentation both critical and surprisingly fun.\nFirst Time Building Something That Actually Works So this was my first real dive into CNNs beyond MNIST and cat/dog classification, and honestly? I picked probably the most complicated dataset I could find. Galaxy Zoo isn’t your typical ImageNet problem - instead of “this is a cat,” you’re trying to predict 37 different probability values that represent how humans answered morphological questions about each galaxy. Because apparently nothing in astronomy can ever be simple.\nThe whole thing started when I realized that most computer vision projects I’d done were basically following tutorials step-by-step. I wanted to build something from scratch that actually required thinking about the problem domain, not just plugging in a pre-trained ResNet and calling it a day.\nThe Dataset: 61,578 Ways to Overthink Image Classification The Galaxy Zoo dataset gives you 61,578 galaxy images (424x424 RGB) for training, each labeled with a 37-dimensional probability vector. These aren’t one-hot encoded classes - they’re soft targets derived from aggregating multiple human classifications. So instead of “this galaxy is spiral,” you get something like “73% of people said it has spiral arms, 12% said it has a prominent bulge, 45% said it has a bar…” and so on for 37 different morphological features.\nThe test set has 79,975 unlabeled images where you need to predict these probability distributions. The evaluation metric is RMSE across all 37 dimensions, which immediately made me realize this was going to be more like a regression problem than standard classification.\nThey also provided some baselines: naive all-ones/all-zeros predictions and a “central pixel benchmark” that uses k-means clustering on the center pixel to assign labels (RMSE: 0.16194). The central pixel thing actually kind of works because galaxy color correlates with morphology, which is both clever and slightly depressing.\nStealing Ideas from the Winners (Academic Integrity Approved) The winning solution by Sander Dieleman achieved 0.07941 RMSE, and his approach became my roadmap. He used several techniques that I shamelessly borrowed:\nRotational Symmetry Augmentation: Galaxies have no canonical “up” direction, so you can rotate them by any angle. I implemented full 360-degree random rotations during training, plus horizontal and vertical flips. This effectively gave me infinite training data variations, which felt like cheating but was totally legal.\nSmart Cropping with Buffer: Instead of just random cropping, I resized images to 160x160 then randomly cropped to 128x128. This buffer zone meant I could crop without losing important galaxy features at the edges. Morphological details matter in astronomy, so you can’t just YOLO crop like you might with natural images.\nControlled Color Jittering: This was tricky because galaxy colors have physical meaning - you can’t just randomly shift hues like you would for cats and dogs. I kept brightness, contrast, and saturation changes minimal (0.1 max) and hue shifts tiny (0.05) to preserve the astrophysical information while still adding some robustness.\nThe Loss Function Nightmare (And Why I Gave Up on Multi-Head) Here’s where things got academically interesting and practically frustrating. The Galaxy Zoo decision tree structure suggests you should use a multi-head architecture - different output heads for different question branches, with custom loss functions that respect the logical dependencies between questions.\nBut after spending way too many hours trying to implement this properly, I realized the computational complexity was getting ridiculous, and honestly, my linear algebra wasn’t quite up to designing the constraint matrices. So I took the pragmatic route: treat it as a simple 37-dimensional regression problem with MSE loss and sigmoid outputs.\nThis definitely wasn’t optimal - you end up with predictions that might not respect the logical structure (like predicting high probability for both “smooth galaxy” and “has spiral arms”). But it was tractable, and sometimes tractable beats theoretically perfect when you’re learning.\nPlatform Hopping: A Comedy of GPU Allocation Training was conducted across multiple platforms because, as a student, you take whatever compute you can get. Started development on my M1 MacBook (surprisingly decent for prototyping), moved to Google Colab’s T4 GPU for initial training runs, then got access to the HYAK cluster for hyperparameter sweeps, and finally used Colab Pro’s L4 GPU for the final training runs.\nThe modeling pipeline followed an iterative refinement process that probably looked more systematic than it actually was: VGG11 (too shallow), VGG16 (better but still underfitting), ResNet50 (overly complex for this dataset size), and ultimately a custom 7-block VGG-inspired CNN that I convinced myself was “tailored for the task” but was probably just the result of too much hyperparameter tuning.\nEach platform switch required refactoring the data loading code, which taught me way more about file I/O and path management than I expected. Nothing like debugging CUDA out-of-memory errors at 2 AM to really understand batch size tradeoffs.\nImplementation Details (The Stuff That Actually Mattered) My final architecture was a VGG-style CNN with 5 convolutional blocks (3→64→128→256→512→512 channels) followed by a custom fully connected tail with dropout regularization. The key insight was using adaptive average pooling before the classifier, which made the network more robust to input size variations and reduced the parameter count significantly.\nI implemented mixed precision training with PyTorch’s autocast, which gave me about 30% speedup and let me use larger batch sizes. Cross-validation with 5 folds helped validate that the model was actually learning generalizable features rather than just memorizing training quirks.\nThe data loading pipeline included per-batch memory cleanup (probably overkill but CUDA OOM errors are traumatic), and I spent an embarrassing amount of time optimizing the transforms pipeline to avoid CPU bottlenecks.\nResults: Better Than Baselines, Learning More Important Than Leaderboards Final validation RMSE: 0.1861 ± 0.0056 across cross-validation folds. This beat the central pixel benchmark (0.16194) but was still far from the winning solution (0.07941). Still, for a first serious CNN project, I was pretty happy with the engineering aspects even if the performance wasn’t record-breaking.\nThe model showed reasonable behavior - conservative confidence levels, sensible class distributions, and stable training curves. The predictions looked astronomically plausible, which felt like a small victory given how easy it would be to produce complete nonsense.\nAn iterative deep learning approach using progressively more complex architectures allowed for improved galaxy morphology predictions. Leveraging varied compute resources and continuous model tuning was critical in building a robust classification pipeline, even if “surpassing the record RMSE” remains an aspirational goal rather than an achieved one.\nWhat I Actually Learned Beyond the technical skills (PyTorch proficiency, CNN architectures, hyperparameter tuning), this project taught me that domain knowledge matters way more than I expected. Understanding why galaxies look the way they do informed every design decision, from augmentation strategies to loss function choices.\nI also learned that sometimes the theoretically optimal approach isn’t practical for a student project timeline. The multi-head architecture would have been more elegant, but the simple regression approach actually worked and let me focus on other important aspects like proper evaluation and reproducible training.\nMost importantly: when your validation loss curves look good and your model predicts reasonable probability distributions, that’s already a success for a learning project, even if you’re not beating Kaggle leaderboards.\nFull code available on GitHub Gist, and way too many comments explaining design decisions. Because if you’re going to overthink a project, you might as well document the overthinking process.\nBut also a preview is visible here ","wordCount":"1255","inLanguage":"en","image":"https://timavilov.com/images/papermod-cover.png","datePublished":"2025-05-28T08:15:04-03:00","dateModified":"2025-05-28T08:15:04-03:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://timavilov.com/posts/galaxy-zooing-my-first-cnn/"},"publisher":{"@type":"Organization","name":"T|A","logo":{"@type":"ImageObject","url":"https://timavilov.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://timavilov.com/ accesskey=h title="T|A (Alt + H)">T|A</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://timavilov.com/about title=About><span>About</span></a></li><li><a href=https://timavilov.com/chronicles title=Chronicles><span>Chronicles</span></a></li><li><a href=https://timavilov.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://timavilov.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://github.com/CrustularumAmator title=Github><span>Github</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://timavilov.com/>Home</a>&nbsp;»&nbsp;<a href=https://timavilov.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">PROJECT: Galaxy Zooing my first CNN</h1><div class=post-description>VGG architectures, galaxy morphology classification, cross-validation, and way too much time spent on data augmentation</div><div class=post-meta><span title='2025-05-28 08:15:04 -0300 -0300'>May 28, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1255 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/CrustularumAmator/blog/tree/main/content/posts/galaxy-zooing-my-first-cnn.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#first-time-building-something-that-actually-works>First Time Building Something That Actually Works</a></li><li><a href=#the-dataset-61578-ways-to-overthink-image-classification>The Dataset: 61,578 Ways to Overthink Image Classification</a></li><li><a href=#stealing-ideas-from-the-winners-academic-integrity-approved>Stealing Ideas from the Winners (Academic Integrity Approved)</a></li><li><a href=#the-loss-function-nightmare-and-why-i-gave-up-on-multi-head>The Loss Function Nightmare (And Why I Gave Up on Multi-Head)</a></li><li><a href=#platform-hopping-a-comedy-of-gpu-allocation>Platform Hopping: A Comedy of GPU Allocation</a></li><li><a href=#implementation-details-the-stuff-that-actually-mattered>Implementation Details (The Stuff That Actually Mattered)</a></li><li><a href=#results-better-than-baselines-learning-more-important-than-leaderboards>Results: Better Than Baselines, Learning More Important Than Leaderboards</a></li><li><a href=#what-i-actually-learned>What I Actually Learned</a></li></ul></nav></div></details></div><div class=post-content><figure><img loading=lazy src=/images/glaxy_zoo_clasifier.jpg><figcaption><p>Project poster for Applied Astrophysics Symposium at University of Washington</p></figcaption></figure><p>I learned that building your first serious CNN is less about the architecture and more about understanding why galaxies don&rsquo;t have a canonical &ldquo;up&rdquo; direction, and why that makes data augmentation both critical and surprisingly fun.</p><h2 id=first-time-building-something-that-actually-works>First Time Building Something That Actually Works<a hidden class=anchor aria-hidden=true href=#first-time-building-something-that-actually-works>#</a></h2><p>So this was my first real dive into CNNs beyond MNIST and cat/dog classification, and honestly? I picked probably the most complicated dataset I could find. Galaxy Zoo isn&rsquo;t your typical ImageNet problem - instead of &ldquo;this is a cat,&rdquo; you&rsquo;re trying to predict 37 different probability values that represent how humans answered morphological questions about each galaxy. Because apparently nothing in astronomy can ever be simple.</p><p>The whole thing started when I realized that most computer vision projects I&rsquo;d done were basically following tutorials step-by-step. I wanted to build something from scratch that actually required thinking about the problem domain, not just plugging in a pre-trained ResNet and calling it a day.</p><h2 id=the-dataset-61578-ways-to-overthink-image-classification>The Dataset: 61,578 Ways to Overthink Image Classification<a hidden class=anchor aria-hidden=true href=#the-dataset-61578-ways-to-overthink-image-classification>#</a></h2><p>The Galaxy Zoo dataset gives you 61,578 galaxy images (424x424 RGB) for training, each labeled with a 37-dimensional probability vector. These aren&rsquo;t one-hot encoded classes - they&rsquo;re soft targets derived from aggregating multiple human classifications. So instead of &ldquo;this galaxy is spiral,&rdquo; you get something like &ldquo;73% of people said it has spiral arms, 12% said it has a prominent bulge, 45% said it has a bar&mldr;&rdquo; and so on for 37 different morphological features.</p><p>The test set has 79,975 unlabeled images where you need to predict these probability distributions. The evaluation metric is RMSE across all 37 dimensions, which immediately made me realize this was going to be more like a regression problem than standard classification.</p><p>They also provided some baselines: naive all-ones/all-zeros predictions and a &ldquo;central pixel benchmark&rdquo; that uses k-means clustering on the center pixel to assign labels (RMSE: 0.16194). The central pixel thing actually kind of works because galaxy color correlates with morphology, which is both clever and slightly depressing.</p><h2 id=stealing-ideas-from-the-winners-academic-integrity-approved>Stealing Ideas from the Winners (Academic Integrity Approved)<a hidden class=anchor aria-hidden=true href=#stealing-ideas-from-the-winners-academic-integrity-approved>#</a></h2><p>The winning solution by Sander Dieleman achieved 0.07941 RMSE, and his approach became my roadmap. He used several techniques that I shamelessly borrowed:</p><p><strong>Rotational Symmetry Augmentation</strong>: Galaxies have no canonical &ldquo;up&rdquo; direction, so you can rotate them by any angle. I implemented full 360-degree random rotations during training, plus horizontal and vertical flips. This effectively gave me infinite training data variations, which felt like cheating but was totally legal.</p><p><strong>Smart Cropping with Buffer</strong>: Instead of just random cropping, I resized images to 160x160 then randomly cropped to 128x128. This buffer zone meant I could crop without losing important galaxy features at the edges. Morphological details matter in astronomy, so you can&rsquo;t just YOLO crop like you might with natural images.</p><p><strong>Controlled Color Jittering</strong>: This was tricky because galaxy colors have physical meaning - you can&rsquo;t just randomly shift hues like you would for cats and dogs. I kept brightness, contrast, and saturation changes minimal (0.1 max) and hue shifts tiny (0.05) to preserve the astrophysical information while still adding some robustness.</p><h2 id=the-loss-function-nightmare-and-why-i-gave-up-on-multi-head>The Loss Function Nightmare (And Why I Gave Up on Multi-Head)<a hidden class=anchor aria-hidden=true href=#the-loss-function-nightmare-and-why-i-gave-up-on-multi-head>#</a></h2><p>Here&rsquo;s where things got academically interesting and practically frustrating. The Galaxy Zoo decision tree structure suggests you should use a multi-head architecture - different output heads for different question branches, with custom loss functions that respect the logical dependencies between questions.</p><p>But after spending way too many hours trying to implement this properly, I realized the computational complexity was getting ridiculous, and honestly, my linear algebra wasn&rsquo;t quite up to designing the constraint matrices. So I took the pragmatic route: treat it as a simple 37-dimensional regression problem with MSE loss and sigmoid outputs.</p><p>This definitely wasn&rsquo;t optimal - you end up with predictions that might not respect the logical structure (like predicting high probability for both &ldquo;smooth galaxy&rdquo; and &ldquo;has spiral arms&rdquo;). But it was tractable, and sometimes tractable beats theoretically perfect when you&rsquo;re learning.</p><h2 id=platform-hopping-a-comedy-of-gpu-allocation>Platform Hopping: A Comedy of GPU Allocation<a hidden class=anchor aria-hidden=true href=#platform-hopping-a-comedy-of-gpu-allocation>#</a></h2><p>Training was conducted across multiple platforms because, as a student, you take whatever compute you can get. Started development on my M1 MacBook (surprisingly decent for prototyping), moved to Google Colab&rsquo;s T4 GPU for initial training runs, then got access to the HYAK cluster for hyperparameter sweeps, and finally used Colab Pro&rsquo;s L4 GPU for the final training runs.</p><p>The modeling pipeline followed an iterative refinement process that probably looked more systematic than it actually was: VGG11 (too shallow), VGG16 (better but still underfitting), ResNet50 (overly complex for this dataset size), and ultimately a custom 7-block VGG-inspired CNN that I convinced myself was &ldquo;tailored for the task&rdquo; but was probably just the result of too much hyperparameter tuning.</p><p>Each platform switch required refactoring the data loading code, which taught me way more about file I/O and path management than I expected. Nothing like debugging CUDA out-of-memory errors at 2 AM to really understand batch size tradeoffs.</p><h2 id=implementation-details-the-stuff-that-actually-mattered>Implementation Details (The Stuff That Actually Mattered)<a hidden class=anchor aria-hidden=true href=#implementation-details-the-stuff-that-actually-mattered>#</a></h2><p>My final architecture was a VGG-style CNN with 5 convolutional blocks (3→64→128→256→512→512 channels) followed by a custom fully connected tail with dropout regularization. The key insight was using adaptive average pooling before the classifier, which made the network more robust to input size variations and reduced the parameter count significantly.</p><p>I implemented mixed precision training with PyTorch&rsquo;s autocast, which gave me about 30% speedup and let me use larger batch sizes. Cross-validation with 5 folds helped validate that the model was actually learning generalizable features rather than just memorizing training quirks.</p><p>The data loading pipeline included per-batch memory cleanup (probably overkill but CUDA OOM errors are traumatic), and I spent an embarrassing amount of time optimizing the transforms pipeline to avoid CPU bottlenecks.</p><h2 id=results-better-than-baselines-learning-more-important-than-leaderboards>Results: Better Than Baselines, Learning More Important Than Leaderboards<a hidden class=anchor aria-hidden=true href=#results-better-than-baselines-learning-more-important-than-leaderboards>#</a></h2><p>Final validation RMSE: 0.1861 ± 0.0056 across cross-validation folds. This beat the central pixel benchmark (0.16194) but was still far from the winning solution (0.07941). Still, for a first serious CNN project, I was pretty happy with the engineering aspects even if the performance wasn&rsquo;t record-breaking.</p><p>The model showed reasonable behavior - conservative confidence levels, sensible class distributions, and stable training curves. The predictions looked astronomically plausible, which felt like a small victory given how easy it would be to produce complete nonsense.</p><p>An iterative deep learning approach using progressively more complex architectures allowed for improved galaxy morphology predictions. Leveraging varied compute resources and continuous model tuning was critical in building a robust classification pipeline, even if &ldquo;surpassing the record RMSE&rdquo; remains an aspirational goal rather than an achieved one.</p><h2 id=what-i-actually-learned>What I Actually Learned<a hidden class=anchor aria-hidden=true href=#what-i-actually-learned>#</a></h2><p>Beyond the technical skills (PyTorch proficiency, CNN architectures, hyperparameter tuning), this project taught me that domain knowledge matters way more than I expected. Understanding why galaxies look the way they do informed every design decision, from augmentation strategies to loss function choices.</p><p>I also learned that sometimes the theoretically optimal approach isn&rsquo;t practical for a student project timeline. The multi-head architecture would have been more elegant, but the simple regression approach actually worked and let me focus on other important aspects like proper evaluation and reproducible training.</p><p>Most importantly: when your validation loss curves look good and your model predicts reasonable probability distributions, that&rsquo;s already a success for a learning project, even if you&rsquo;re not beating Kaggle leaderboards.</p><hr><p><em>Full code available on <a href=https://gist.github.com/CrustularumAmator/900505afe938f2af9ae31af57ee20d52>GitHub Gist</a>, and way too many comments explaining design decisions. Because if you&rsquo;re going to overthink a project, you might as well document the overthinking process.</em></p><p><em>But also a preview is visible here</em>
<script src=https://gist.github.com/CrustularumAmator/900505afe938f2af9ae31af57ee20d52.js></script></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://timavilov.com/tags/notes/>Notes</a></li><li><a href=https://timavilov.com/tags/deep-learning/>Deep-Learning</a></li><li><a href=https://timavilov.com/tags/computer-vision/>Computer-Vision</a></li><li><a href=https://timavilov.com/tags/pytorch/>Pytorch</a></li><li><a href=https://timavilov.com/tags/cnn/>Cnn</a></li><li><a href=https://timavilov.com/tags/astronomy/>Astronomy</a></li><li><a href=https://timavilov.com/tags/galaxy-zoo/>Galaxy-Zoo</a></li><li><a href=https://timavilov.com/tags/first-project/>First-Project</a></li><li><a href=https://timavilov.com/tags/vgg/>Vgg</a></li><li><a href=https://timavilov.com/tags/kaggle/>Kaggle</a></li></ul><nav class=paginav><a class=next href=https://timavilov.com/posts/mmorpgs-from-be-eng-perspective/><span class=title>Next »</span><br><span>NOTES: MMORPGs From BE Eng Perspective</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://timavilov.com/>T|A</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>